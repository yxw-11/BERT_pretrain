{"index": 0, "text": "A framework for recovering high-resolution information from a sequence of sub-sampled and compressed observations is presented. Compression schemes that describe a video sequence through a combination of motion vectors and transform coefficients are the focus (e.g. the MPEG and ITU family of standards), and we consider the influence of both the motion vectors and transform coefficients within the reconstruction algorithm. A Bayesian approach is utilized to incorporate the information, and results show a discemable improvement in resolution, as compared to standard interpolation methods.", "paperid": 2075697838, "normalizedname_level1": "artificial intelligence"}
{"index": 1, "text": "A novel strategy for the visual tracking and the information extracted problem is proposed which is for the case of maneuvering target. The strategy contains two methods. One is used to tracking, and the other one is used to extract the information of the vehicle. A non-linear estimation method using the particle filter to track objects is presented. During the tracking, a great deal of vehicle information is extracted at the same time, such as the type and velocity of vehicle. An improved Fisher Linear Discriminate is proposed to extract the license plate. By experiments, this strategy is proved much effectively, and the accuracy of tracking is improved greatly.", "paperid": 2158941088, "normalizedname_level1": "artificial intelligence"}
{"index": 2, "text": "In this paper, we describe a watermarking scheme using block compressive sensing for tampered image detection. The image is divided into several sub blocks whose size can be tuned by the data quantity of watermarking and tamper localization accuracy. The sub blocks were observed by compressive sensing using a Fourier matrix. The observing results were assembled to form the watermarking which will be embedded to the whole image. We save the watermarking and seed for image tampering detection. Test results proved higher security and scalability of the proposed scheme over conventional domain watermarking scheme.", "paperid": 2068239979, "normalizedname_level1": "artificial intelligence"}
{"index": 3, "text": "In computer vision and image processing, when dealing with image segmentation, edge detection, shape detection and particularly for pattern recognition, it is sometimes interesting and even necessary to know whether a given point is inside or outside a closed polygon. This will enable us to detect objects (or shapes) that are enclosed within a given closed contour. But this is not a trivial task and since we are, generally, concerned with discrete contours, the solution and its performance are dependant on the connectness of the vertices of the contour and on their number. In this paper, we present two new methods for interior point query: the first one is based on Cauchy's theorem of closed curves in the complex plan. The second proposed method, the “point insertion” method, is a heuristic approach. The proposed solutions are evaluated based on practical applications like polygon filling, polygons intersection detection and freeform cropping of images. Experimental results indicate that the proposed methods yield interesting performance. For low mean distance between consecutive vertices, both methods are equivalent in terms of polygon filling performance but the method of “point insertion” is faster. When this distance increases, the method based on Cauchy's theorem outperforms with almost zero error.", "paperid": 2086734243, "normalizedname_level1": "artificial intelligence"}
{"index": 4, "text": "A novel shape descriptor, which can be extracted from the major object edges automatically and used for the multimedia content-based retrieval in multimedia databases, is presented. By adopting a multiscale approach over the edge field where the scale represents the amount of simplification, the most relevant edge segments, referred to as subsegments, which eventually represent the major object boundaries, are extracted from a scale-map. Similar to the process of a walking ant with a limited line of sight over the boundary of a particular object, we traverse through each subsegment and describe a certain line of sight, whether it is a continuous branch or a corner, using individual 2-D histograms. Furthermore, the proposed method can also be tuned to be an efficient texture descriptor, which achieves a superior performance especially for directional textures. Finally, integrating the whole process as feature extraction module into MUVIS framework allows us to test the mutual performance of the proposed shape descriptor in the context of multimedia indexing and retrieval.", "paperid": 2156203145, "normalizedname_level1": "artificial intelligence"}
{"index": 5, "text": "Genetic fuzzy rule selection has been successfully used to design accurate and interpretable fuzzy classifiers from numerical data. In our former study, we proposed its parallel distributed implementation which can drastically decrease the computational time by dividing both a population and a training data set into sub-groups. In this paper, we examine the effect of data reduction on the generalization ability of fuzzy rule-based classifiers designed by our parallel distributed approach. Through computational experiments, we show that data reduction can be realized without severe deterioration in the generalization ability of the designed fuzzy classifiers.", "paperid": 2116047233, "normalizedname_level1": "artificial intelligence"}
{"index": 6, "text": "The fast and steady feature extraction and key point matching are key factors to SAR (synthetic aperture radar) image automatic registration algorithm. The real-time processing of the algorithm in SAR is one of the challenges that need to be solved urgently. This paper aims at improving the efficiency of the description of key point in SIFT algorithm. We implemented the rapid calculation of the feature extraction section in SIFT (scale invariant feature transform) based on embedded NVIDIA GPU-Tegra K1 by fully taking the advantages of multi-cores of GPU in parallel computing, floating point calculation, memory management, etc. And the speed ratio of the accelerated calculation of the SIFT feature reaches 5.5.", "paperid": 2335145283, "normalizedname_level1": "artificial intelligence"}
{"index": 7, "text": "Acquiring new customers in any business is much more expensive than trying to keep the existing ones. Thus many prediction models are presented to detect churning customers. The objective of this paper was to improve the predictive accuracy and interpretability of churn detection. For this purpose, the application of the locally linear model tree (LOLIMOT) algorithm, which integrates the advantage of neural networks, tree model and fuzzy modeling, was experimented. Applied to the data of a major telecommunication company, the method is found to improve prediction accuracy significantly compared to other algorithms, such as artificial neural networks, decision trees, and logistic regression. The results also indicate that LOLIMOT can have accurate outcome in extremely unbalanced datasets.", "paperid": 2107691272, "normalizedname_level1": "artificial intelligence"}
{"index": 8, "text": "We show how a common language of variability can be used to enhance the expressiveness of a domain specific language (DSL). DSLs have been proposed as a mechanism for expressing variability. Variability between models in a given domain or of a family of systems is captured by language constructs, implying that all possible models in this language are the allowed variations. We explore the possibility of expressing variability in a language independently of the base modeling language. We explore how this works for small DSLs as well as for general purpose languages like UML. Implications of this approach are that the variability language can be standardized, and that DSLs do not have to include variability mechanisms.", "paperid": 2105035644, "normalizedname_level1": "artificial intelligence"}
{"index": 9, "text": "Many clinical applications, such as surgical planning, require volumetric models of anatomical structures represented as a set of tetrahedra. A method of constructing anatomical models from medical images is presented. The method starts with a set of contours segmented from the medical images by a clinician and produces a model that has high fidelity with the contours. Unlike most modeling methods, the contours are not restricted to lie on parallel planes. The main steps are a 3D Delaunay tetrahedralization, culling of non-object tetrahedra, and refinement of the tetrahedral mesh. The result is a high-quality set of tetrahedra whose surface points are guaranteed to match the original contours. The key is to use the distance-map and bit-volume structures that were created along with the contours. The method is demonstrated on both computed tomography and 3D ultrasound data. Models of 170,000 tetrahedra are constructed on a standard workstation in approximately ten seconds.", "paperid": 2544622845, "normalizedname_level1": "artificial intelligence"}
{"index": 10, "text": "In this paper, a double-regularization (DR) approach is proposed to perform blind multichannel imagery restoration. The proposed DR approach effectively utilizes the smoothness property of image planes and blur operators. The correlations between different image planes are considered in the blind restoration process. The proposed approach minimizes a cost function that consists of a restoration error measure about each image plane and two regularization terms. The conjugate gradient type alternative-minimization (AM) strategy is derived to minimize the cost function. The developed AM strategy is simple and effective. Excellent results were obtained. The proposed approach provides a new dimension for the blind restoration of the multichannel imagery.", "paperid": 2104251783, "normalizedname_level1": "artificial intelligence"}
{"index": 11, "text": "The paper presents two useful extensions of the incremental SVM in the context of online learning. An online support vector data description algorithm enables application of the online paradigm to unsupervised learning. Furthermore, online learning can be used in the large-scale classification problems to limit the memory requirements for storage of the kernel matrix. The proposed algorithms are evaluated on the task of online monitoring of EEG data, and on the classification task of learning the USPS dataset with a-priori chosen working set size.", "paperid": 2534230712, "normalizedname_level1": "artificial intelligence"}
{"index": 12, "text": "A significant problem in Synthetic Aperture Sonar (SAS) imaging is compensating for unknown errors in the sonar path trajectory. Unknown deviations from the ideal sonar trajectory have the effect of blurring and smearing the sea-floor image. In typical operating conditions, the blurring can completely obscure details in the imaged scene. Techniques for estimating path deviations from the recorded data have been developed. Once estimated, the effects of deviations are compensated for and the blurring reduced. The process of estimating path-deviations and removing image blurring is called autofocus. We present a broad-band scheme for a bulk-motion estimation in the absence of inertial navigation system (INS) data. Once the effect of bulk-motion error is removed, alternate autofocus methods may be used to yield diffraction-limited imagery. In addition, the bulk autofocus allows a check on the validity of the motion measured by any inertial systems installed on the sonar tow-fish. The noncoherent autofocus operates by exploiting the envelope correlation between adjacent sonar echos to provide path deviation information. This puts the method into the same class of algorithm as the well-known shear-average autofocus, which exploits ping-to-ping phase correlation for its operation. The algorithm differs from shear-average by using only the base-band envelope of the echo data and not phase information. The algorithm has been tested on both real and simulated data and has some promise as a first step in any autofocusing system. In particular, the system should provide good starting information for any image quality based autofocus methods. The accuracy of the method is limited by biasing caused by strong target scatterers. Noncoherent autofocus also performs better than coherent shear-average autofocus for the very large path perturbations investigated.", "paperid": 2118927507, "normalizedname_level1": "artificial intelligence"}
{"index": 13, "text": "Understanding and predicting human's behavior by a robot is necessary for smooth interaction with humans. Humans often achieve them by guidance of other's behavior, especially controlling its attention. Magicians can effectively manipulate spectators' attention. In this study, we hypothesized that the relationship between magician's gaze and hands plays an important role for controlling spectators' attention. To test the hypothesis, we carried out an experiment to measure gaze points of spectators who were watching magic videos and analyzed the result of the experiment. As a result, we obtained the result that when the magician's gaze coincided with his hands that were manipulating objects, the degree of attention drawing was higher than otherwise.", "paperid": 2087614050, "normalizedname_level1": "artificial intelligence"}
{"index": 14, "text": "We present a new algorithm for linear spectral mixture analysis, which is capable of supervised unmixing of hyperspectral data while respecting the constraints on the abundance coefficients. This simplex-projection unmixing algorithm is based upon the equivalence of the fully constrained least squares problem and the problem of projecting a point onto a simplex. We introduce several geometrical properties of high-dimensional simplices and combine them to yield a recursive algorithm for solving the simplex-projection problem. A concrete implementation of the algorithm for large data sets is provided, and the algorithm is benchmarked against well-known fully constrained least squares unmixing (FCLSU) techniques, on both artificial data sets and real hyperspectral data collected over the Cuprite mining region. Unlike previous algorithms for FCLSU, the presented algorithm possesses no optimization steps and is completely analytical, severely reducing the required processing power.", "paperid": 2113753426, "normalizedname_level1": "artificial intelligence"}
{"index": 15, "text": "The present natural language processing (NLP) methods mainly employ bottom-up language analysis ways, which only have limited semantic analysis capabilities. Therefore it is impossible to carry out semantic processing for large amounts of real texts. This paper presents a semantic analysis approach and corresponding algorithms based on term connection. The approach has rooted in semantics based on term connection and taken a major sentence cut-in, bottom-up semantic analyzing process. The main algorithm is to find the optimal sentence tree based on the semantic meaning conformity of term connection. So far, this approach has been applied in project CAPC (computer aided poetry composing) funded by the Chinese Natural Science Foundation.", "paperid": 2533755844, "normalizedname_level1": "artificial intelligence"}
{"index": 16, "text": "The problem of image based visual servoing for robots working in a cluttered dynamic environment is addressed in this paper. It is assumed that the environment is observed by depth sensors which allow to measure the distance between any moving obstacle and the robot. Also an eye-in-hand camera is used to extract image features. The main idea is to control suitable image moments and to relax a certain number of robot's degrees of freedom during the interaction phase. If an obstacle approaches the robot, the main visual servoing task is relaxed partially or completely, while the image features are kept in the camera field of view by controlling the image moments. Fuzzy rules are used to set the desired values of the image moments. Beside that, the relaxed redundancy of the robot is exploited to avoid collisions. After removing the risk of collision, the main visual servoing task is resumed. The effectiveness of the algorithm is shown by several case studies on a KUKA LWR 4 robot arm.", "paperid": 2221028717, "normalizedname_level1": "artificial intelligence"}
{"index": 17, "text": "This paper investigates the use of parallel corpus for the annotation of temporal objects -- EVENT-type and TIMEX3-type elements in Chinese text. We propose a TRR engine based on parallel corpus in Chinese text employing existing tools for the temporal recognition and normalization of English to annotate the English portion of English-Chinese bitexts, and automatically project these annotations to the Chinese text, guided by word alignment. Through the method, Chinese Time Bank has been constructed, which lays the foundation for the further TRR research based on Time ML in Chinese text.", "paperid": 2067507892, "normalizedname_level1": "artificial intelligence"}
{"index": 18, "text": "This paper introduces a highly integrated system providing very accurate object detection with RGB-D sensor. To solve the problem that there are always insufficient training sets for object detection in real world, we present an online learning architecture to learn templates and to detect objects real-time. The proposed novel concept skips the training phase required in previous recognition works, and it comprises independent tracking and detection function, which collaborates with each other to make the detection more precise. We furthermore illustrate four strategies for online learning and compare the efficiency. With depth information, the experiment results perform remarkable in challenging scenarios.", "paperid": 2093470487, "normalizedname_level1": "artificial intelligence"}
{"index": 19, "text": "Recently, spatial principal component analysis of census transform histograms (PACT) was proposed to recognize instance and categories of places or scenes in an image. When combining PACT with Local difference Magnitude Binary Pattern (LMBP), a new representation called Local Difference Binary Pattern (LDBP) was proposed and performed better. LDBP is based on the comparisons between center pixel and its neighboring pixels. However, the relationship among neighbor pixels is not considered. In this paper we proposed Local Neighbor Binary Pattern (LNBP) to utilize the relationship among neighboring pixels. LNBP provides complementary information regarding neighboring pixels for LDBP. We propose to combine LDBP with LNBP, and used a spatial representation for scene recognition. Experiments on two widely used dataset demonstrate the proposed method can improve the performance of recognition.", "paperid": 2089145864, "normalizedname_level1": "artificial intelligence"}
{"index": 20, "text": "The motor vehicle is one of great inventions in recent century. It although brings more convenient life for us, it also makes more troubles at the same time. The number of traffic accidents that caused casualties in the past two years. It has 160,000 accidents in Taiwan, 1,800,000 in America and 7,000 in Singapore; about 90% of these accidents are man-made. Among them, over ninety percent of accidents are caused due to the careless of drivers. Therefore, the study of driver status detection is increase gradually. The development platform in the paper is based on LabVIEW, NIIMAQ Vision,and the hardware which are the CCD and the NI-PCI 1411 image acquisition card to realize an eye's state recognition system. In the paper, we designed the system that included the face detection, the face tracking, the eye's image locating and extracting, the eye's opening and closing state recognition. Finally, in order to avoid the situation of the tilted head will interfere with the system's operation, we realized no matter face tilts or not, the system can still recognize the eye's opening and closing. By the study effort of above, we hope to provide some contributions to the developer in the area of face detection and feature recognition, and contribute my mental and physical efforts to the study of driver status detection analysis to reduce the traffic accidents.", "paperid": 2138974580, "normalizedname_level1": "artificial intelligence"}
{"index": 21, "text": "We propose a face recognition scheme based on an auto-associative memory (AM) model. Two kinds of AM models are compared, namely, pseudo-inverse memory and radial basis function (RBF) network, and we found that RBF based associative memory is much more efficient. To capture substantial facial features and reduce computational complexity, we use a wavelet transform (WT) to decompose face images and choose the lowest resolution subband coefficients for face representation. Results indicate that the modular scheme yields accurate recognition on the widely used XM2VTS face database and Olivetti Research Laboratory (ORL) face database.", "paperid": 2100924272, "normalizedname_level1": "artificial intelligence"}
{"index": 22, "text": "This paper presents a novel unequal error protection scheme for H.264/AVC based on region of interest (ROI) with improved error resilience and quality of ROI region in video communication. With the help of Flexible Macro-Block Ordering (FMO) in H.264/AVC, the proposed scheme can not only encode the ROI region and the Non-ROI region in separate slice groups (SGs), but also further classify the ROI region into two slice groups with motion information from macro-blocks (MB). After employing Rate-compatible punctured convolutional (RCPC) codes for unequal error protection transmission strategy, experimental results show that the proposed algorithm obtained a better subjective quality of the reconstructed video than the traditional algorithm in ROI at the decoder and the peak-signal-noise-ratio (PSNR) has 0.87dB outperforming in average.", "paperid": 2021683308, "normalizedname_level1": "artificial intelligence"}
{"index": 23, "text": "Diffusion filtering techniques are mostly used to enhance the ridge structure of a noisy fingerprint image. In these filtering techniques the measurement of local orientation is needed. The diffusion tensor used in these techniques reflects the local image structure, as in a structure tensor same set of eigenvectors are used. To control the diffusion along the direction of high coherence special Eigenvalues are chosen. It works well in enhancing the ridges but, it takes orientation angles implicitly by using local image structure (derivatives). As we know that the derivatives have undesirable property of enhancing noise which makes the process of finding the correct orientation more difficult. This gives a further motivation for the improved orientation field calculated by some more reliable mean, which can overcome such difficulties. Therefore, in this work a Multi-Scale DDFB is used which adaptively change the local neighborhood size with the image local contrast and feature width. Experimental results show that the proposed algorithm is noise robust and is more suitable for feature localization as compare to other coherence enhancement diffusion algorithms.", "paperid": 1982877948, "normalizedname_level1": "artificial intelligence"}
{"index": 24, "text": "Facing all kinds of attacks and destruction of campus network, we urgently need agood detection model to detecting all kinds of campus network attack with high detection rate and low false positive rate, which possesses the ability of recognizing unknown abnormal activities. With considering the characteristic of network data source, a new intrusion detection application model is proposed. With the adaptive learning ability,this application model can fast recognize normal or abnormal activities of the campus network and possess the basic ability of recognizing new and unknown abnormal activities.", "paperid": 1991702471, "normalizedname_level1": "artificial intelligence"}
{"index": 25, "text": "Recently, open-domain question answering (ODQA) systems that extract an exact answer from large text corpora based on text input are intensively being investigated. However, the information in the first question input by a user is not usually enough to yield the desired answer. Interactions for collecting additional information to accomplish QA is needed. This paper proposes an interactive approach for spoken interactive ODQA systems. When the reliabilities for answer hypotheses obtained by an ODQA system are low, the system automatically derives disambiguous queries (DQ) that draw out additional information. The additional information based on the DQ should contribute to distinguishing effectively an exact answer and to supplementing a lack of information by recognition errors. In our spoken interactive ODQA system, SPIQA, spoken questions are recognized by an ASR system, and DQ are automatically generated to disambiguate the transcribed questions. We confirmed the appropriateness of the derived DQ by comparing them with manually prepared ones.", "paperid": 2128232273, "normalizedname_level1": "artificial intelligence"}
{"index": 26, "text": "In this article, we present a new wearable haptic interface developed at CEA-LIST for precise finger interactions within virtual reality applications in large environments. The hand movements are tracked using a stereoscopic visual tracking system, allowing large movements in free space. Moreover, the device integrates two three degrees of freedom with force feedback robots associated with index and thumb fingers, allowing virtual objects fine manipulation. Finally, a two degrees of freedom tactile actuator is integrated under the pulp of each fingertip in order to improve the high frequency response of the haptic interface and to provide information on the texture and the shape of the virtual objects manipulated.", "paperid": 2108195580, "normalizedname_level1": "artificial intelligence"}
{"index": 27, "text": "Many researchers have shown the ability of X-ray computer tomography (CT) of acquiring knowledge about the internal structure of logs. Contrarily to medical CT scanners, industrial CT scanners should satisfy more extreme constraints (in terms of speed, duty cycle, radiation safety, reconstruction circle and aperture size) in order to be used in a sawmill environment. All these factors lower the quality of industrial CT images while automatic methods for analyzing data are required. To ensure good performances and robustness, an automatic system should adapt to the quality of the input images. The paper focuses on an algorithm which can adaptively improve the contrast of the ridge and valley structure of annual rings in CT images of wooden logs. We also show that this algorithm does not only help in segmenting annual rings but also gives with little adaptation a good approximation of the localization of the pith.", "paperid": 2163701778, "normalizedname_level1": "artificial intelligence"}
{"index": 28, "text": "In this paper we present a fast, interactive method for collecting structural primitives from objects of interest contained within manually selected image regions. The input image is projected onto a Max-Tree and Min-Tree structure from which a pixel-to-node mapper marks the nodes of each tree that correspond to peak components explicitly contained within the selected window. In a pass through the selected nodes, an attribute vector is constructed from the pool of auxiliary data associated with each node separately. The set of all attribute vectors is mapped into a pre-computed multidimensional feature space from which a binary criterion is constructed to accept or reject the remaining image objects. The method is demonstrated in a real application on information extraction from very high resolution satellite imagery.", "paperid": 2054188650, "normalizedname_level1": "artificial intelligence"}
{"index": 29, "text": "This talk introduces several kinds of neural-network-based adaptive control systems. These control systems combine the advantages of neural network identification, adaptive control and robust control techniques. These neural networks include recurrent neural network (RNN), recurrent fuzzy neural network (RFNN), cerebellar model articulation controller (CMAC) and recurrent cerebellar model articulation controller (RCMAC). Moreover, their applications in control problems are demonstrated to illustrate the effectives of these control systems.", "paperid": 2166334155, "normalizedname_level1": "artificial intelligence"}
{"index": 30, "text": "With the ever-increasing number of digital documents, the ability to automatically classify those documents both quickly and accurately is becoming more critical and difficult. A text classification system for Chinese documents is developed in this paper. A HTF-WDF algorithm is proposed for feature selection. Different from other feature selection algorithms, this method considers the effect of term frequency. Using the idea of fuzzy feature, the terms with high term frequency (HTF) are distinguished and appended to the feature list. The features which can represent the topic of the documents are picked out according to the weighted document frequencies (WDF), which can avoid the problems of the traditional document frequency (DF) method. Then the Support Vector Machine (SVM) is used to training the classifier. The proposed algorithm is verified by representative Chinese documents. The experiment results manifest the superiority of the proposed algorithm to the traditional DF algorithm.", "paperid": 2065823460, "normalizedname_level1": "artificial intelligence"}
{"index": 31, "text": "One challenging aspect of automated surveillance for real environments is the occurrences of various difficult scenarios brought about by practical unconstrained settings. We address foreground detection for automated surveillance under the following challenging situations: i) foregrounds being partially hidden due to close similarities to the background, and ii) foregrounds representing multiple objects being inseparable, forming a large contiguous blob due to occlusion. To build a robust system, we present a new foreground detection framework based on Bayesian formulation, comprising both bottom-up and top-down approaches. We first propose a region-based background subtraction and a localized spatial segmentation scheme as the bottom-up steps for foreground detection. We then incorporate a human shape model as the top-down step for foreground validation and occlusion handling. Segmentation is obtained when a maximum posteriori value is found, corresponding to the best description about foregrounds given by the approach. Such integration of bottom-up and top-down approaches leads directly to more robust performance in handling challenging situations within hostile real environments. Promising results are obtained when the algorithm is tested on real video sequences captured from a live surveillance system that operates at a public outdoor swimming pool.", "paperid": 2078233551, "normalizedname_level1": "artificial intelligence"}
{"index": 32, "text": "This paper is concerned with real time pattern matching using projection kernels. We derive an analytical threshold based on statistical properties of random noise and characteristics of the projection kernels. The proposed threshold decision scheme provides a mean to perform automatic pattern matching without human intervention. Based on the required successful rate, the analytical threshold can reliably reject mismatch and keep the target pattern irrespective of the assumption of noise model. Experimental results show that the proposed threshold follows the ground truth threshold tightly, and the false rejection rate is less than 1% even the image is very noisy.", "paperid": 2536552590, "normalizedname_level1": "artificial intelligence"}
{"index": 33, "text": "In an autonomous driving system, a precise reference map with static environment information is necessary to perform accurate localization and path planning. To build a static reference map, only the background parts in the scene must be extracted by removing dynamic objects in the scanned data. However, filtering dynamic objects out from the scene is currently an extremely difficult task. To solve this problem, we propose a ray binding method to detect precise free space and a background extraction method based on the free space information. The ray binding method is a novel approach to detect the free space of environments precisely and reliably by constructing a spherical triangular pyramid set from adjacent laser beams. This precise free space information can be utilized in the process of map building to remove dynamic objects automatically, enabling us to extract residual background points reliably. We experimentally verify that the proposed background extraction method using the free space information operates well in diverse real scenarios.", "paperid": 1665967044, "normalizedname_level1": "artificial intelligence"}
{"index": 34, "text": "A comparative study is commonly performed by means of pre-defined or expert selected region of interest (ROI)-analysis or voxel based analysis (VBA). In contrast to these methods, correlations within the data can be modeled by using principal component analysis (PCA) and linear discriminant analysis (LDA). The mapping computed by PCA/LDA is displayed to identify the discriminative regions. A technique called 'pruning' is introduced to iteratively discard misclassified subjects from the cohort. These subjects reside in the region in feature space where the classes are overlapping. As the exact cause of this overlapping is unknown, it is preferable to base the mapping merely on representative prototypes, residing in the nonoverlapping parts of the feature space. After pruning the PCA/LDA mapping, a more pronounced decrease in FA in larger parts of the corpus callosum was observed, compared to conventional VBA", "paperid": 2127266706, "normalizedname_level1": "artificial intelligence"}
{"index": 35, "text": "One of the threatening trends of human health in recent years has been metabolic syndrome. Metabolic syndrome is a cluster of conditions that occur together resulting in simultaneous disorders related to ones metabolism. This paper analyses the effect age clustering has on the syndrome trends using SOM. It gives an analysis and visualization of the contributing parameter(s) to the syndrome in each cluster and then projects the overall effect the clustered SOM analysis has on the entire group of examinees. Inter-relation of the input parameters and the severity of their contribution to the syndrome risks are investigated.", "paperid": 2138575662, "normalizedname_level1": "artificial intelligence"}
{"index": 36, "text": "The representation of object features is an important task in the researches such as pattern recognition, information retrieval, interactive query, etc. This paper addresses how to integrate computational linguistics and fuzzy set techniques to automatically establish the knowledge base for semi-structural domain expertise. The proposed task includes document pre-processing, lexical vector encoding, fuzzy vector shrinking and fuzzy membership encoding. The content of a wild bird illustrated book was used as the training corpus and the domain expertise was established. Queries from another book, an expert and a naive were used as testing corpus. The preliminary results showed that the proposed approach is suitable for representing domain expertise. The top-N scores an average of 79 query for a specific object.", "paperid": 2101608358, "normalizedname_level1": "artificial intelligence"}
{"index": 37, "text": "Muscle movement inside the face generates the real facial expressions of a human. This paper proposes a method that generates facial expressions based on estimating muscle movement, i.e.. muscular contraction parameters using detected facial feature points. First, the facial feature points of a facial image are detected using image processing methods. Then, the displacements of facial feature points, which correspond to the muscle-based facial model that Waters proposed, are calculated. Finally, we estimate the muscular contraction parameters of a facial model to obtain the vertex displacements. Experimental results reveal that our approach can generate a facial expression of the facial model, which corresponds to the facial expression of the actual facial image", "paperid": 2161827013, "normalizedname_level1": "artificial intelligence"}
{"index": 38, "text": "A Dual Swath BuckEye EO imaging system has been developed and flown for emergency response to natural disasters. The Dual-Swath BuckEye EO sensor head is comprised of a pair of nadir pointing 39 Megapixel or 60.5 Megapixel CCD cameras for simultaneously acquiring twin adjacent aerial digital images to double imaging swathwidth for a panoramic field of view (14, 200 × 5, 400 pixels or 18, 800 × 6, 700 pixels) and obtaining wide ground-track coverage. The Dual-Swath BuckEye EO imaging system fully integrates a sensor head, embedded computers, and a precision GPS/IMU device into a 25lb package. The system is implemented with fast processing capabilities for direct aerial image geo-reference, orthorectification, and fast image mosaic. High resolution fast corridor mapping samples are presented. These include 11cm resolution and one (1) mile wide corridor mosaics over the flooded Upper Mississippi river in Iowa and a 4cm resolution single-pass photomap for a tornado damaged corridor in New Hampshire.", "paperid": 2150824845, "normalizedname_level1": "artificial intelligence"}
{"index": 39, "text": "Multispectral images present complimentary information, which enables night vision (NV). Specifically, night vision colorization using multispectral image increases the reliability of interpretation, and thus they are good for visual analysis (human vision). The purpose of NV colorization is to resemble a natural scene in colors, which differs from false coloring. This paper gives an overview of NV colorization techniques proposed in past decade. Two categories of coloring methods, color fusion and color mapping, are discussed and compared in this paper. Color fusion directly combines multispectral NV images into a color-version image by mixing pixel intensities. A channel-based color fusion method will be reviewed. Color mapping usually maps the color properties of a false-colored NV image (source) onto that of a true-color daylight picture (target). Four coloring mapping methods, statistical matching, histogram matching, joint histogram matching, and lookup table (LUT) will be presented and compared. The joint histogram matching is newly introduced in this paper. The experimental NV imagery includes visible (RGB), image intensified, near infrared, long wave infrared. From the experimental results, the following conclusions can be made: (i) The segmentation-based color mapping method produces the most impressive and realistic colors but it requires heavy computations; (ii) Color fusion and LUT-based methods run very fast but their results are less realistic; (iii) The statistical matching method always provides acceptable results (i.e., never fails); and (iv) Histogram matching and joint-histogram matching can generate more impressive colors when the color distributions between source and target are similar.", "paperid": 2065195970, "normalizedname_level1": "artificial intelligence"}
{"index": 40, "text": "For quantization of line spectral frequency (LSF), Gaussian mixture model (GMM) based switched split vector quantization (SSVQ) has been reported as the best performing intra-frame coding method. However, GMM-SSVQ partly recovers correlations between the subvectors of split vector quantization (SVQ). In the proposed GMM-SSVQ with the Karhunen-Loeve Transform (KLT), KLT-domain quantization for each mixture with a novel region-clustering algorithm is applied to GMM-SSVQ. Compared with SVQ and GMM-SSVQ, it provides 4 and 1 bit higher performance in terms of average spectral distortion and outliers, respectively. Computational complexity and memory requirements are similar to GMM-SSVQ.", "paperid": 2157422526, "normalizedname_level1": "artificial intelligence"}
{"index": 41, "text": "Normal mapping is a variant of bump mapping that is commonly used in computer games. It models complex surface variations by explicitly storing a surface normal in a texture map. However, it has not been used with precomputed radiance transfer (PRT), a technique for modeling an object's response to a parameterized model of lighting, which enables interactive rendering of complex global illumination effects such as soft shadows and interreflections. This paper presents several techniques that effectively combine normal mapping and precomputed radiance transfer for rigid objects. In particular, it investigates representing the convolved radiance function in various bases and borrowing concepts from the separable decomposition of BRDF's.", "paperid": 2052989718, "normalizedname_level1": "artificial intelligence"}
{"index": 42, "text": "RIF (Rule Interchange Format) has become W3C's candidate recommendation on rule interchange. To let RIF represent and interchange fuzzy knowledge in the Semantic Web, we propose RIF-FRD (RIF Fuzzy Rule Dialect), a rule interchange format based on fuzzy sets, defining its XML syntax and metamodel. Based on the metamodel, we propose a fuzzy RIF framework—f-RIA (fuzzy Rule Interchange Architecture), which supports rule interchange between RIF-FRD and some important fuzzy rule languages.", "paperid": 2012307671, "normalizedname_level1": "artificial intelligence"}
{"index": 43, "text": "The relative strengths of individual units, described as attributes, determine the outcome during unit conflicts involving units in the armies of opponents in large strategy games. We describe a process for developing predictive models of the outcome for such conflicts where combat involves single unit adversaries where each unit is defined in terms of 4 fundamental attributes. These attributes: range, speed, health and damage are common to unit descriptions in many successful real-time strategy games. Our analysis process identifies three phases of game play with invariant properties that hold throughout each phase. We demonstrate the utility of this analysis by creating a model that predicts game outcomes. Predictions are validated against the game simulation. The game outcomes are explicitly related to the attributes of the two units involved, highlighting the significance and role of each attribute.", "paperid": 2014429413, "normalizedname_level1": "artificial intelligence"}
{"index": 44, "text": "This paper describes a method for object detection and recognition based on appearance based approach. We introduce a probabilistic model to describe the wide variation of object appearance in images. In our method, objects are modeled as probabilistic features of silhouette and edge. These features are extracted from the object images viewed from various distance and orientation, and form the training data set for template modeling. A Non linear template model is build by the combination of Principal Component Analysis (PCA) and Kernel Ridge Regression (KRR). Finally, the problem of object detection is formulated as maximum a posteriori (MAP) estimation using above model. Experiments are conducted on road surveillance, where our method is applied to a certain car type recognition.", "paperid": 1747102232, "normalizedname_level1": "artificial intelligence"}
{"index": 45, "text": "This paper proposes a fast Adaptive Modified Hexagon Based Search (AMHexBS) Motion Estimation (ME) algorithm. The proposed algorithm predicts the direction of motion by using rood shaped pattern and then Hexagonal based Search (HexBS) is applied for refinement of search process. The size of rood shaped pattern is dynamically determined for each Macro Block (MB). An early termination scheme is incorporated to speed-up the ME process, which is mainly advantageous for the video sequences having low motion activities. The simulation results show that the proposed algorithm performs better according to Speed Improvement Rate (SIR) and number of search points for low and moderate motion video sequences.", "paperid": 1985786817, "normalizedname_level1": "artificial intelligence"}
{"index": 46, "text": "This study presents a system for automatically producing puzzles for use in game design. The system incorporates an evolutionary algorithm that optimizes the puzzle to a specified level of difficulty. The fitness function uses dynamic programming to compute the minimum number of moves required to solve a puzzle. Two types of puzzle are explored, one is a maze based on chess pieces and the other uses colors as related by the color wheel to create an implicit maze. The evolutionary algorithm is able to produce a wide variety of puzzles at specified levels of difficulty. The algorithm can thus be used to provides a library for game design or for variable game content. The technique is flexible and can be generalized to puzzles of remarkable complexity by simply upgrading the dynamic programming algorithm used in the fitness function. It is found that puzzles requiring a maximum number of moves to solve are, potentially, less difficult because such puzzles present the player with few choices. This problem is addressed by modifying the algorithm to search for puzzles with a smaller minimum number of moves required, leaving more room in the puzzle for choice and its attendant confusion.", "paperid": 2051967127, "normalizedname_level1": "artificial intelligence"}
{"index": 47, "text": "This article presents results of the recognition process of acoustic fingerprints from a noise source using spectral characteristics of the signal. Principal Components Analysis (PCA) is applied to reduce the dimensionality of extracted features and then a classifier is implemented using the method of the k-nearest neighbors (KNN) to identify the pattern of the audio signal. This classifier is compared with an Artificial Neural Network (ANN) implementation. It is necessary to implement a filtering system to the acquired signals for 60Hz noise reduction generated by imperfections in the acquisition system. The methods described in this paper were used for vessel recognition.", "paperid": 2549050591, "normalizedname_level1": "artificial intelligence"}
{"index": 48, "text": "In this paper, we present a general color correction function (CCF) in tone mapping (TM), which combines luminance compression and color constraint at locally prespecified luminance level. The proposed CCF is derived from the luminance compression function with the color constraint under which the color ratios between the three color channels are preserved and color saturation is controlled. The proposed CCF is developed to locally perform the luminance compression and color saturation control in local TM. Computer simulations with various sets of real, low dynamic range images show the effectiveness of the proposed TM algorithm in terms of the visual quality as well the local contrast.", "paperid": 2079866300, "normalizedname_level1": "artificial intelligence"}
{"index": 49, "text": "In robotics and augmented reality applications, model-based 3-D tracking of rigid objects is generally required. With the help of accurate pose estimates, it is required to increase reliability and decrease jitter in total. Among many solutions of pose estimation in the literature, pure vision-based 3-D trackers require either manual initializations or offline training stages. On the other hand, trackers relying on pure depth sensors are not suitable for AR applications. An automated 3-D tracking algorithm, which is based on fusion of vision and depth sensors via extended Kalman filter, is proposed in this paper. A novel measurement-tracking scheme, which is based on estimation of optical flow using intensity and shape index map data of 3-D point cloud, increases 2-D, as well as 3-D, tracking performance significantly. The proposed method requires neither manual initialization of pose nor offline training, while enabling highly accurate 3-D tracking. The accuracy of the proposed method is tested against a number of conventional techniques, and a superior performance is clearly observed in terms of both objectively via error metrics and subjectively for the rendered scenes.", "paperid": 2025597690, "normalizedname_level1": "artificial intelligence"}
{"index": 50, "text": "The traffic flow information of humans and objects is widely put to use as infrastructural information in urban planning such as traffic plans or area marketing. However, traffic flow information depends on the questionnaire survey of the public transportation users or the visual inspection by the inspector on a street corner, causing a problem of costing a lot of money. Therefore, studies have been conducted to automatically measure traffic flow information using a laser beam sensor or a digital video camera. The former method using the laser beam sensor makes it possible to perform a traffic survey of a movable body with high precision in any environment. However, it is impossible to obtain information of its appearance and thus grasp correct attribute information of the movable body itself. On the other hand, with the latter method using the digital video camera, information about the appearance of the movable body can be obtained and analyzed. However, there is a problem that the measurement precision decreases when the photographic environment changes such as in lighting and sunshine, or under such environment as occlusion occurs due to the overlapping of movable bodies. Thus, this research realizes accurate measurement and acquisition of human flow information with attribute information even under difficult photographic environment or where occlusion occurs, by focusing only on human figures and using a range image sensor to which the characteristics of the laser beam sensor and digital video camera are added.", "paperid": 1992676238, "normalizedname_level1": "artificial intelligence"}
{"index": 51, "text": "This paper describes a road obstacle classification system that recognizes both vehicles and pedestrians in far-infrared images. Different local and global features based on Speeded Up Robust Features (SURF) were investigated and then selected in order to extract a discriminative signature from the infrared spectrum. First, local features representing the local appearance of an obstacle, are extracted from a codebook of scale and rotation-invariant SURF features. Second, global features were used since they provide complementary information by characterizing shape and texture. When compared with the state-of-the-art Haar and Gabor wavelet features, our method provides significant improvement of recognition performances. Moreover, since our SURF based representation is invariant to the scale and the number of local features extracted from objects, our system performs the recognition task without resizing images. Our system was evaluated on a set of far-infrared images where obstacles occur at different scales and in difficult recognition situations. By using a multi-class SVM approach, accuracy rates of 91.51% has been achieved on Surf-based representation, while a maximum rate of 89.11% was achieved on wavelet-based representation.", "paperid": 2065457101, "normalizedname_level1": "artificial intelligence"}
{"index": 52, "text": "The analysis of large and complex parameterized software systems, e.g., systems simulation in aerospace, is very complicated and time-consuming due to the large parameter space, and the complex, highly coupled nonlinear nature of the different system components. Thus, such systems are generally validated only in regions local to anticipated operating points rather than through characterization of the entire feasible operational envelope of the system. We have addressed the factors deterring such an analysis with a tool to support envelope assessment: we utilize a combination of advanced Monte Carlo generation with n-factor combinatorial parameter variations to limit the number of cases, but still explore important interactions in the parameter space in a systematic fashion. Additional test-cases, automatically generated from models (e.g., UML, Simulink, Stateflow) improve the coverage. The distributed test runs of the software system produce vast amounts of data, making manual analysis impossible. Our tool automatically analyzes the generated data through a combination of unsupervised Bayesian clustering techniques (AutoBayes) and supervised learning of critical parameter ranges using the treatment learner TAR3. The tool has been developed around the Trick simulation environment, which is widely used within NASA. We will present this tool with a GN&C (Guidance, Navigation and Control) simulation of a small satellite system.", "paperid": 2152564633, "normalizedname_level1": "artificial intelligence"}
{"index": 53, "text": "In this work, we present a method of determining human subjects via a low-resolution thermal imaging sensor. Since the image quality of the low-resolution thermal imaging sensor could be suffering from heat signatures and recognizable patterns of human subjects are unable to be determined due to resolution issues, it is recommended to employ a probabilistic method. This paper presents how human subjects can be expressed in terms of pixel size, standard deviation, label movement, vector tracking, label lifetime and a rewarding system based on those. Various pre and post-image processing methods will be covered including background collection, Gaussian filtering, segmentation, local/global adaptive threshold and background learning.", "paperid": 2079531190, "normalizedname_level1": "artificial intelligence"}
{"index": 54, "text": "We present an efficient and robust system for landmark localization, segmentation and pose classification of ears from 3D profile facial range data. After defining 18 landmarks on the ear, including Triangular Fossa and Incisure Intertragica, a novel Ear Tree-structured Graph (ETG) is proposed to represent the 3D ear. We trained a flexible mixture model to locate these landmarks automatically. Afterwards, the ear region is outlined as the minimum rectangle including all landmarks. Finally, by calculating the turning angle between landmarks on the helix, the ear is classified as either a left or a right ear. To the best of our knowledge, there is no previous work on automatic landmark localization for 3D ear on 3D facial profile depth images. Experiments are conducted on University of Notre Dame Collection F and Collection J2 datasets, containing large occlusion, scale and pose variations. Results demonstrate the effectiveness of the proposed techniques.", "paperid": 2059812251, "normalizedname_level1": "artificial intelligence"}
{"index": 55, "text": "This paper proposes a new game-control interface using hand poses captured from a digital camera. Extracting skin color from the background-subtracted image is generally used for segmenting hand region. But it suffers from change in illumination, shadows and skin- colored objects so that unwanted subsets reduce accuracy in hand pose recognition. To deal with these problems, first, subtracting skin colors from the background image must be done and second, it mustn 't get affected by changes in background. The paper describes an approach to using variance of homogeneity for skin color extraction. Homogeneity which represents contrast in a window is robust against change in illumination and can be used to effectively extract skin color rather than color or brightness based subtraction from background. The recognition of hand poses proceeds by computing fourier descriptor of extracted pixels. This approach to recognition can robustly identify hand poses comparing to the others. Experiments on real-time game demonstrate the robustness of the proposed method.", "paperid": 2126595791, "normalizedname_level1": "artificial intelligence"}
{"index": 56, "text": "We consider a framework in which a group of agents communicates by means of emails, with the possibility of replies, forwards and blind carbon copies (BCC). We study the epistemic consequences of such email exchanges by introducing an appropriate epistemic language and semantics. This allows us to find out what agents learn from the emails they receive and to determine when a group of agents acquires common knowledge of the fact that an email was sent. We also show that in our framework from the epistemic point of view the BCC feature of emails cannot be simulated using messages without BCC recipients.", "paperid": 2129749388, "normalizedname_level1": "artificial intelligence"}
{"index": 57, "text": "Decentralized coordination can be achieved by the emergence of a consensual choice inside a group of simple agents. The work done on the emergence of social laws and the emergence of a shared lexicon are known examples of possible benefits of consensus formation. In this paper, we show the possibility of creating random succession of social choices inside a group of simple and homogeneous agents with limited perception abilities where there is no central control. The society of agents evolves in a sequence of different consensual choices, from consensus to consensus, in a kind of random collective walk. The transition between different collective choices is conditioned by performance criteria as we do not want that a society oscillates between several possible decisions but that it chooses one of them as quickly as possible. Both the nature of the collective choices and the duration of the consensus are completely random. We introduce a very simple individual behavior that was originally derived from a failure attempt to use an auto-organization model of dominance order formation in the animal world. We show one application of this transition between consensual choices in a group of micro-painters that create random artistic patterns", "paperid": 2016130918, "normalizedname_level1": "artificial intelligence"}
{"index": 58, "text": "In this work, a multiple-expert binarization framework for multispectral images is proposed. The framework is based on a constrained subspace selection limited to the spectral bands combined with state-of-the-art gray-level binarization methods. The framework uses a binarization wrapper to enhance the performance of the gray-level binarization. Nonlinear preprocessing of the individual spectral bands is used to enhance the textual information. An evolutionary optimizer is considered to obtain the optimal and some suboptimal 3-band subspaces from which an ensemble of experts is then formed. The framework is applied to a ground truth multispectral dataset with promising results. In addition, a generalization to the cross-validation approach is developed that not only evaluates generalizability of the framework, it also provides a practical instance of the selected experts that could be then applied to unseen inputs despite the small size of the given ground truth dataset.", "paperid": 2149601298, "normalizedname_level1": "artificial intelligence"}
{"index": 59, "text": "The face conveys information about a person's age, sex, background, and identity; what they are feeling, thinking, or likely to do next. Facial expression regulates face-to-face interactions, indicates reciprocity and interpersonal attraction or repulsion, and enables intersubjectivity between members of different cultures. Facial expression indexes neurological and psychiatric functioning and reveals personality and socioemotional development. Not surprisingly, the face has been of keen interest to behavioral scientists. About 15 years ago, computer scientists became increasingly interested in the use of computer vision and graphics to automatically analyze and synthesize facial expression. This effort was made possible in part by the development in psychology of detailed coding systems for describing facial actions and their relation to basic emotions, that is, emotions that are interpreted similarly in diverse cultures. The most detailed of these systems, the Facial Action Coding System (FACS), informed the development of the MPEG-4 facial animation parameters for video transmission and enabled progress toward automated measurement and synthesis of facial actions for research in affective computing, social signal processing, and behavioral science. This article reports key advances in behavioral science that are becoming possible through these developments. Before beginning, automated facial image analysis and synthesis (AFAS) is briefly described.", "paperid": 2022037281, "normalizedname_level1": "artificial intelligence"}
{"index": 60, "text": "Automatic segmentation of the overlapping cervical cells is one of the most challenging problems in the medical image analysis. This paper presents a novel multi-step level set (LVS) method for segmenting cytoplasm and nuclei from overlapping cells in a single EDF image produced from Pap smear images of multi-layer cervical cell volumes. The first step segments the clump consisting of free or overlapping cells using a region-and edge-based level set method on the Gaussian filtered image. The second step segments the nuclei by a multi-step level set method from the original image. And finally, the most critical step of cytoplasm segmentation is done using level set method optimized by criteria such as curvature of the cytoplasm, duration of retaining the segmented area of the cytoplasm, edge information and a speed regulator depends on the homogeneity of the cell. The performance of the proposed algorithm is evaluated on the real cervical cell image provided by the second overlapping cervical cytology image segmentation challenge at ISBI 2015. Clump detection shows good result in spite of the weak clump boundary. Nuclei detection also shows good result in spite of congestion of nuclei. We are also able to segment two overlapping cells in the real Pap smear image.", "paperid": 2294189136, "normalizedname_level1": "artificial intelligence"}
{"index": 61, "text": "This study tries to examine the impacts of Emotional Learning based Fuzzy Inference System (ELFIS) on completion time of projects. For the project management team, on time delivery within budget is a fundamental and important factor that highlights the importance of estimating the completion time of a project during its execution. This study implies four soft computing methods which are Artificial Neural Network (ANN), Adaptive Neuro-Fuzzy Inference System (ANFIS), ELFIS and Conventional Regression to forecast the completion time of projects. Periodical time series for this study are generated by a progress generator program with typical Resource Constrained Project Scheduling (RCPS) problem projects library. Core variables in proposed model consist of known parameters in Earned Value Management System which is SPI (Schedule Performance Index), CPI (Cost Performance Index) and AD (Actual Duration). Using ELFIS capability in modeling the expert emotion as appropriate emotional signal shows better performance compared with ANFIS, ANN and conventional regression.", "paperid": 2098692659, "normalizedname_level1": "artificial intelligence"}
{"index": 62, "text": "It is a straightforward task for human observers to judge the relative quality of two visual signals of the same content, but subject to different type/level of distortions. However, this comparative image quality assessment (C-IQA) problem remains a difficult challenge for the current research of image quality assessment (IQA). In this paper, we propose a C-IQA approach to predict the relative perceptual quality of a pair of images that are possibly subject to different artifact types/levels. The C-IQA algorithm is designed to emulate the process of comparing the relative quality of two visual stimuli as performed by the human visual system (HVS) within the framework of free energy minimization. The brain's internal generative models initialized on the inputs are used to explain the two images. And their relative quality can then be determined through comparing the free energy level of this model-data fitting process. In the existing work of IQA, the full-reference (FR) and reduced-reference (RR) methods need the prior knowledge of the original images while the no-reference (NR) algorithms usually work with a single input image. The C-IQA approach is inherently different from those existing methods in that it takes as input an image pair and predicts their relative quality without using any knowledge about the original image. A computationally efficient solution to the proposed C-IQA scheme based on a linear autoregressive image model is also introduced. Experimental results show that the proposed method achieves about 98% accuracy in line with the subjective ratings when applied on over 300,000 image pairs sampled from the LIVE database, outperforming the FR metrics such as PSNR, SSIM, and some of the most advanced NR IQA algorithms.", "paperid": 2082030541, "normalizedname_level1": "artificial intelligence"}
{"index": 63, "text": "To construct mind-reading robots, it is important to correctly identify the communicative intention of humans. In human environments, there are usually an enormous number of objects, and each object has many different parts. Therefore, referring to an object or an object part in current communication is not an easy task. We use both linguistic and nonlinguistic information to specify such referential intentions, but exactly how nonlinguistic information is used has not been examined in detail. In this study, video data from eight pairs of adult addressers and addressees who communicated about whole-object or object-part labels were analyzed. The addressers' utterances were transcribed, and nouns and demonstrative words were extracted. Hand motions were divided into five categories: showing, pointing, stroking, functional action, and other. Gaze data obtained using eye movement recordings were also coded using the following categories: gaze on object, face, or body of the experimenter and other. Gaze on object was further divided into two categories; critical and noncritical object parts. The results showed that participants used different patterns of hand motions and gaze to refer to whole-object and object-part labels. When they taught whole-object labels, they showed the object and looked at the addressee's face. When they taught object-part labels, they pointed at, stroked, and looked at the object part. Based on the results, we propose the concept of action contrast in human communication of referential intention. We propose that the need for specification and the use of actions in a contrasting manner are related. Nonlinguistic cues, such as pointing and showing, and timing of utterances are important sources of information that specify human referential intentions in the environment. Action contrast is an important source of information specifying addressers' referential intentions, which can be utilized to construct mind-reading robots.", "paperid": 1991268672, "normalizedname_level1": "artificial intelligence"}
{"index": 64, "text": "A direct method for recovering three-dimensional (3D) head motion parameters from a sequence of range images acquired by Kinect sensors is presented. Based on the range images, a new version of the ...", "paperid": 1991609221, "normalizedname_level1": "artificial intelligence"}
{"index": 65, "text": "Modern touch screen sensors are capable of detecting and reporting finger presence not only upon contact but also as the finger is approaching the screen. This gives us a wealth of additional information, which to the best of our knowledge, has never been analyzed before. Using these new sensor capabilities, we can see exactly how a user performs gestures starting from the finger's approach through the actual touching of the screen.", "paperid": 2064747641, "normalizedname_level1": "artificial intelligence"}
{"index": 66, "text": "This paper describes a distributed layered architecture for resource-constrained multirobot cooperation, which is utilized in autonomic mobile sensor network coverage. In the upper layer, a dynamic task allocation scheme self-organizes the robot coalitions to track efficiently across regions. It uses concepts of ant behavior to self-regulate the regional distributions of robots in proportion to that of the moving targets to be tracked in a nonstationary environment. As a result, the adverse effects of task interference between robots are minimized and network coverage is improved. In the lower task execution layer, the robots use self-organizing neural networks to coordinate their target tracking within a region. Both layers employ self-organization techniques, which exhibit autonomic properties such as self-configuring, self-optimizing, self-healing, and self-protecting. Quantitative comparisons with other tracking strategies such as static sensor placements, potential fields, and auction-based negotiation show that our layered approach can provide better coverage, greater robustness to sensor failures, and greater flexibility to respond to environmental changes", "paperid": 2122950833, "normalizedname_level1": "artificial intelligence"}
{"index": 67, "text": "Handling atypical examples in classification tasks is one of the challenges in machine learning. While there seems to be a race for accuracy, very little has been done to understand and solve the issues related to atypical data. In this paper, coverage-performance (CP) curves are introduced to help a better understanding of atypical data. The concept of CP curves is based on the idea of separating atypical data and visualizing performance of classification as a function of coverage (the fraction of data participating in training or evaluation). To generate CP curves, two schemes are compared in this paper. The first scheme is based on SVMs alone and the second one is a hybrid of a PNN and a SVM. Two generated datasets with overlapping features are used to demonstrate the effectiveness of CP curves obtained by each scheme. Calculated theoretical limits on the generated data show that the hybrid scheme is a very effective way of producing CP curves. It is also shown that by separating atypical data, although we lose some data, the performance of the classification increases significantly.", "paperid": 2156273182, "normalizedname_level1": "artificial intelligence"}
{"index": 68, "text": "In this paper, an improved magnetoencephalography (MEG) source reconstruction technique considering anatomical connectivity of cortical sources is proposed. The anatomical connectivity information was taken into account by calculating three-dimensional geodesic distance between neighboring sources, and then the resultant inverse solutions were compared with those of other cases:1)Inverse estimate without connectivity information; 2)Use of Euclidean distance instead of geodesic distance. The proposed technique was applied to realistic simulations for a real brain anatomy, and the results showed that estimated sources can be smoother and more accurate by using the anatomical connectivity information", "paperid": 2146531283, "normalizedname_level1": "artificial intelligence"}
{"index": 69, "text": "To measure the model's poses in the spin tests, we present a measurement method based on stereo vision and a set of manmade signs on the surface of the spin model in this paper. Through images caught by two monochrome CCD video cameras, the three-dimensional coordinates of manmade signs on the model are rebuilt. By the signs' coordinates in the plane reference frame and their coordinates in the world reference frame, we can judge the model's poses in the world reference frame. The experimental results, the model in a rotary balance, demonstrate the effectiveness of the proposed approach. The data are shown in the paper.", "paperid": 2542436931, "normalizedname_level1": "artificial intelligence"}
{"index": 70, "text": "An adaptive filtering approach to distributed space-borne SAR imaging is proposed in this paper. The signal model and principle of adaptive filtering imaging are presented at first. Then two categories of adaptive filters are evaluated. The performance analysis and comparison of those two filters indicates that the computation load is lower than with a Wiener filter and imaging quality is better than with a matched filter. Finally the numerical simulation results are presented to illustrate the results.", "paperid": 2102654728, "normalizedname_level1": "artificial intelligence"}
{"index": 71, "text": "We describe in this paper a new approach for pattern recognition using modular neural networks with a fuzzy logic method for response integration. We proposed a new architecture for modular neural networks for achieving pattern recognition in the particular case of human faces. Also, the method for achieving response integration is based on the fuzzy Sugeno integral. Response integration is required to combine the outputs of all the modules in the modular network. We have applied the new approach for face recognition with a real database of faces from students of our institution.", "paperid": 2129506444, "normalizedname_level1": "artificial intelligence"}
{"index": 72, "text": "Image inpainting is a technique to automatically recover the damaged or missing regions in images. This paper presents a simple but fast approach for recovering the damaged regions of digital images. It uses a unique idea of dynamic weighted kernels. It proposes kernels of different sizes and weights to fill in the damaged regions with different width. The proposed technique has been tested for quality and execution time over a wide variety of images and results are found favorable.", "paperid": 2062593785, "normalizedname_level1": "artificial intelligence"}
{"index": 73, "text": "The 3D reconstruction of underwater environments have demonstrated its importance in many situations, especially in environments potentially harmful to human beings. Its is being used to reconstruct shipwrecks, coral reefs and expedition planning, documentation and study, as well as to make easier maintenance in structures lying in deep water. In this paper, a set of techniques previously tested is described and has its relevance explained in the matter of underwater 3D reconstruction. The results confirm the efficiency of the chosen techniques and it can be concluded that this framework can be applied to similar case studies without loss of generality.", "paperid": 2015114754, "normalizedname_level1": "artificial intelligence"}
{"index": 74, "text": "We present an automatic optimization approach to outfit synthesis. Given the hair color, eye color, and skin color of the input body, plus a wardrobe of clothing items, our outfit synthesis system suggests a set of outfits subject to a particular dress code. We introduce a probabilistic framework for modeling and applying dress codes that exploits a Bayesian network trained on example images of real-world outfits. Suitable outfits are then obtained by optimizing a cost function that guides the selection of clothing items to maximize the color compatibility and dress code suitability. We demonstrate our approach on the four most common dress codes: Casual, Sportswear, Business-Casual, and Business. A perceptual study validated on multiple resultant outfits demonstrates the efficacy of our framework.", "paperid": 2005855758, "normalizedname_level1": "artificial intelligence"}
{"index": 75, "text": "Over the last few years, several fuzzy-based approaches to linguistic text analysis have suggested that it is possible to find and to use patterns in the surface structure of linguistic utterances (i.e., pragmatic information) which reveal or conform to deeper linguistic regularities or constructs (Le., semantic information). This paper discusses two of these approaches, the fuzzy semantic typing of Subasic and Huettner and the computational semiotics of Rigger. From a theoretical point of view, such methods have been criticized on both philosophical and linguistic grounds. lit this paper, we perform some simple experiments to see whether useful insights or even useful text processing methods can be gleaned from application of techniques derived from one of these pragmatic approaches (computational semiotics). We conclude that the results are inconsistent and provide neither useful surface measures of regularities nor insights into underlying syntactico-semantic properties of linguistic utterance.", "paperid": 2135886457, "normalizedname_level1": "artificial intelligence"}
{"index": 76, "text": "In modern business intelligence systems there is a need to reduce the data flow between operational transactional systems and data warehouses, as well as reduce the time between data update in the warehouse and reflecting this change in the analytical models used to perform business analyses. In a modern data warehouse with incremental data update, at every change there is only a small amount of new data present, however most of the data mining techniques requires training the model on the full training set. In this paper popular data mining segmentation techniques are presented along with incremental learning algorithms, as well as a new segmentation method with the use of genetic algorithm.", "paperid": 2542684096, "normalizedname_level1": "artificial intelligence"}
{"index": 77, "text": "We investigate discriminative features that are able to improve classification accuracy on visually similar classes. To this end, we build a deep feature learning network, which learns features with discriminative constraint in each single layer module, and learns multiple levels of features for hierarchical image representation. Specifically, the network encodes the discriminative information by automatically selecting the informative features, and forcing them to be closer to the features extracted from the same class than the features from different classes. We also collect a new fine-grained dataset containing 51 common tree species in Singapore. All the images are taken at a distance with large intra class variance, which makes the tree species hard to be distinguished. Our experimental results show that we are able to achieve 78.03% in accuracy on this challenging dataset, which is 8.48% higher than general hand-designed feature.", "paperid": 2035478595, "normalizedname_level1": "artificial intelligence"}
{"index": 78, "text": "Spectral similarity measures can differentiate subtle differences between spectral footprints extracted from remotely sensed images. Prior research on urban scenes examined low-to-medium resolution images where many details of an urban scene are lost. For high spatial resolution imagers, a number of problems arise in the analysis of urban scenes since virtually all objects are resolved. Issues of spatially varying scene illumination and unavailability of surface reflectance data prevent classical spectral similarity measures from being extensively applied. Commonly used measures have limitations from a feature space perspective such as favoring either spectral direction or spectral magnitude. In this paper, a novel spectral similarity measure based on Mahalanobis distance is proposed to take into account the unique properties of high resolution urban scenes. A simplified radiometric transfer model is also incorporated. The results confirm the advantage of the new spectral similarity measure when applied to complex urban images.", "paperid": 2007593684, "normalizedname_level1": "artificial intelligence"}
{"index": 79, "text": "In this paper we perform feature extraction by utilizing the Two Dimensions Fractional Fourier Transform(2D-FrFT). PCA is used to reduce the high-dimensional feature information and we perform FLDA on the total samples, then emotional sates are recognized based on multi-classifier fusion with fuzzy integral in decision layer. Simulations based on the Ryerson and Jaffe facial expression database show that the novel method is invalid and superior. Compared to the classical algorithm, the proposed method may flexibly extract the emotional features and effectively classify the different emotional states, so highly improve the recognition rate.", "paperid": 2538117713, "normalizedname_level1": "artificial intelligence"}
{"index": 80, "text": "We present an architecture called the modular neural predictive coding architecture (Modular NPC). The Modular NPC is used for discriminative feature extraction (DFE). It provides an architecture based on phonetics knowledge applied to phoneme recognition. The phonemes are extracted from the Darpa-Timit speech database. Comparisons with coding methods (LPC, MFCC, PLP) are presented: they put in obviousness an improvement of the recognition rates.", "paperid": 2108871692, "normalizedname_level1": "artificial intelligence"}
{"index": 81, "text": "In machine condition monitoring, wear particles formed in rubbing are a source of valuable information on the wear mechanism and severity, while ferrographic image is an information carrier of wear particles. For the significance of image segmentation for wear particle feature extraction and recognition, defects of some traditional methods that can convert color image into binary are introduced and analyzed in this paper. After this, based on image object area and its difference, an auto-threshold confirming segmentation algorithm for ferrographic image is presented. Experimental results show that this algorithm can segment wear particles accurately and automatically. Especially, it is efficient for bright object with black background in micro image with reflex but no transmission light.", "paperid": 2111811194, "normalizedname_level1": "artificial intelligence"}
{"index": 82, "text": "Compressive Sensing (CS) theory has gained widespread attention due to its advantage of breaking through the limits of Nyquist sampling theorem. To make the CS more adaptive, some works based on the human vision system (HVS) have been conducted, but incurring some other problems at the same time, such as additional sensors, higher computation power and added experiments. To solve these problems a perceptual CS scheme based on the masking effect of human eyes towards image textures is proposed in this paper. It is consisted of three steps. First the image signal is represented sparsely through DWT transformation, and the masking matrix for different brightness changing areas is computed based on the DWT decomposition. Second the CS measurement with the masking matrix is performed on the image signal to get a lower-dimension sampling. And finally the image is reconstructed from its sparse sampling data. Through several experiments we can see that the proposed scheme performs better in both visual quality and PSNR assessment than common CS without any increasing of the computational complexity.", "paperid": 2171184713, "normalizedname_level1": "artificial intelligence"}
{"index": 83, "text": "Thanks to the development of music audio analysis, state-of-the-art techniques can now detect musical attributes such as timbre, rhythm, and pitch with certain level of reliability and effectiveness. An emerging body of research has begun to model the high-level perceptual properties of music listening, including the mood and the preferable listening context of a music piece. Towards this goal, we propose a novel text-like feature representation that encodes the rich and time-varying information of music using a composite of features extracted from the song lyrics and audio signals. In particular, we investigate dictionary learning algorithms to optimize the generation of local feature descriptors and also probabilistic topic models to group semantically relevant text and audio words. This text-like representation leads to significant improvement in automatic mood classification over conventional audio features.", "paperid": 1980636726, "normalizedname_level1": "artificial intelligence"}
{"index": 84, "text": "This paper describes the ongoing research on rough sets based classifier applied to induction motors fault diagnosis through motor current signature analysis (MCSA). The results of mechanical failures detection and how a rough sets based classifier is used as a monitoring system using current signature analysis in predictive maintenance are described in this paper.", "paperid": 2538455959, "normalizedname_level1": "artificial intelligence"}
{"index": 85, "text": "In this paper, a multiresolution approach is suggested for texture classification of atherosclerotic tissue from B-mode ultrasound. Four decomposition schemes, namely, the discrete wavelet transform, the stationary wavelet transform, wavelet packets (WP), and Gabor transform (GT), as well as several basis functions, were investigated in terms of their ability to discriminate between symptomatic and asymptomatic cases. The mean and standard deviation of the detail subimages produced for each decomposition scheme were used as texture features. Feature selection included 1) ranking the features in terms of their divergence values and 2) appropriately thresholding by a nonlinear correlation coefficient. The selected features were subsequently input into two classifiers using support vector machines (SVM) and probabilistic neural networks. WP analysis and the coiflet 1 produced the highest overall classification performance (90% for diastole and 75% for systole) using SVM. This might reflect WP's ability to reveal differences in different frequency bands, and therefore, characterize efficiently the atheromatous tissue. An interesting finding was that the dominant texture features exhibited horizontal directionality, suggesting that texture analysis may be affected by biomechanical factors (plaque strains).", "paperid": 2169271980, "normalizedname_level1": "artificial intelligence"}
{"index": 86, "text": "In computer vision, gradient-based tracking is usually performed from monochromatic inputs. However, few researches consider the influence of the chosen colorto- grayscale conversion technique. This paper evaluates the impact of these conversion algorithms on tracking and homography calculation results, both being fundamental steps of augmented reality applications. Eighteen color-togreyscale algorithms were investigated. These observations allowed to conclude that the methods can cause significant discrepancy in the overall performance. As a related finding, experiments also showed that pure color channels (R, G, B) yielded more stability and precision when compared to other approaches.", "paperid": 1963074466, "normalizedname_level1": "artificial intelligence"}
{"index": 87, "text": "Based on the high-resolution IKONOS satellite stereo images, using the digital photogrammetry method, the 3D visualization information of the research area was extracted, including DEM, orthophoto, 3D information and texture of the ground building, then the progress of the DEM extraction was analyzed and the errors of the DEM were corrected. After that, by complementing the advantages of Erdas Imagine and 3ds Max software, the modeling work of the ground building was better accomplished. Finally, based on the VC++6.0 platform, combined with the DirectX SDK, the 3D visualization landscape was constructed, including the visualization of terrain, the loading of the ground feature models. In this process, in order to improve the operating speed, the Level of Detail algorithm and Mipmap technology were used to optimize the terrain visualization and the texture of the models. Further more, the diversification of the smog was simulated by constructing particle system and many factors of the environment and production facilities that can influence the movement of the smog were calculated and added. Through these work, the production activities of the special facility can be inferred, so it can support the decision analysis.", "paperid": 2152249379, "normalizedname_level1": "artificial intelligence"}
{"index": 88, "text": "This paper proposed an effective method for EEG data classification in a Brain-Computer Interfacing system. We use Principal Component Analysis for feature extracting, then use an optimized Support Vector Machine for classification. The SVM's parameters are optimized by Genetic Algorithm. Furthermore, and optimal signal combination search is performed to get a higher classification rate, an explanation from the human physiological point of view is given. Experiment shows that this method can achieve higher classification accuracy than normal SVM classifier and artificial neural network.", "paperid": 2543306369, "normalizedname_level1": "artificial intelligence"}
{"index": 89, "text": "One of the fundamental problems in image search is to rank image documents according to a given textual query. Existing search engines highly depend on surrounding texts for ranking images, or leverage the query-image pairs annotated by human labelers to train a series of ranking functions. However, there are two major limitations: 1) the surrounding texts are often noisy or too few to accurately describe the image content, and 2) the human annotations are resourcefully expensive and thus cannot be scaled up. We demonstrate in this paper that the above two fundamental challenges can be mitigated by jointly exploring the cross-view learning and the use of click-through data. The former aims to create a latent subspace with the ability in comparing information from the original incomparable views (i.e., textual and visual views), while the latter explores the largely available and freely accessible click-through data (i.e., ``crowdsourced\" human intelligence) for understanding query. Specifically, we propose a novel cross-view learning method for image search, named Click-through-based Cross-view Learning (CCL), by jointly minimizing the distance between the mappings of query and image in the latent subspace and preserving the inherent structure in each original space. On a large-scale click-based image dataset, CCL achieves the improvement over Support Vector Machine-based method by 4.0\\% in terms of relevance, while reducing the feature dimension by several orders of magnitude (e.g., from thousands to tens). Moreover, the experiments also demonstrate the superior performance of CCL to several state-of-the-art subspace learning techniques.", "paperid": 1969732194, "normalizedname_level1": "artificial intelligence"}
{"index": 90, "text": "When using appearance-based recognition for self-localization of mobile robots, the images obtained during the exploration of the environment need to be efficiently stored in the memory. PCA offers means for representing the images in a low-dimensional subspace, which allows for efficient matching and recognition. For active exploration it is necessary to use an incremental method for the computation of the subspace. We propose to use an incremental PCA algorithm with the updating of partial image representations in a way that allows the robot to discard the acquired images immediately after the update. Such a model is open-ended, meaning that we can easily update it with new images. We show that the performance of the proposed method is comparable to the performance of the batch method in terms of compression, computational cost and the precision of localization. We also show that by applying the repetitive learning, the subspace converges to that constructed with the batch method.", "paperid": 2158558840, "normalizedname_level1": "artificial intelligence"}
{"index": 91, "text": "The ten papers in this special section focus on applications of probabilistic graphical models in all areas of computer vision.", "paperid": 1971510308, "normalizedname_level1": "artificial intelligence"}
{"index": 92, "text": "Face pose adjustment, as a loop of human face location, is very important in computer face recognition. This work presents a new approach to automatic face pose adjustment on gray-scale static images with a single face. In the first stage, with the degree of mediacy every little image is made, then one piece of image including two eyes using match degree is asked. It is continued with the nose and mouth part horizontal gray projection in small scope. Finally adjust this piece correctly. During the second stage, based on the location and the symmetry feature of eyes, the inclination angle is calculated and the face position is redressed. The experiment shows that the algorithm performs very well both in terms of rate and of efficiency. Due to the precise cation of eyes, the apples of the eyes are detected.", "paperid": 1569169557, "normalizedname_level1": "artificial intelligence"}
{"index": 93, "text": "Humans are capable of agile and adaptive spatial behaviors that are far beyond the capabilities of today's autonomous systems. Spatial behaviors have been investigated in AI, cognitive science and neuroscience. This paper describes an engineering-oriented perspective that is intended to bridge the gap between these fields. We claim that spatial behavior cannot be fully understood by considering the agent and its environment separately. In our approach, we put the emphasis on the interaction between the agent's dynamics and the task environment. We hypothesize that specific patterns emerge from this interaction and that these patterns are used by human operators to mitigate the complexity involved in agile and adaptive spatial performance. This paper describes preliminary experiments and a methodology to investigate these hypotheses.", "paperid": 1991116312, "normalizedname_level1": "artificial intelligence"}
{"index": 94, "text": "This paper uses motion capture data to describe the interpolation of the upper body motion of humanoid robots. For the motion interpolation of humanoid robots, transition trajectory must be created. This trajectory should be based on consideration of the continuity between the first and second motions and the characteristics of humanoid robots, such as the limitations of the joint angle, velocity and torque. In this paper, we use a quintic polynomial to naturally interpolate two motions and we explore techniques of finding the optimal trajectory for humanoids robots. In particular, we find appropriate constraints of the polynomial at the second motion, thereby avoiding the mechanical limitations of the robot. These constraints can make the trajectory more reasonable. Finally, we successfully interpolated two motions and let the robot attempt various motions with continuity.", "paperid": 2133365502, "normalizedname_level1": "artificial intelligence"}
{"index": 95, "text": "This paper introduces efficient 6D global pose estimation algorithms for an unmanned aerial vehicle (UAV) using known lines. The algorithms are designed in the context of International Aerial Robotics Competition (IARC) Mission-7. Two algorithms are introduced in this paper. The first one is to first localize the UAV on a line using two known lines, then the precise position is refined by a third point. The second algorithm recovers the pose of the UAV by observation of a corner of the playground. 3D UAV attitude can also be calculated. Validation results indicate the accuracy and robustness of both algorithms.", "paperid": 1635034095, "normalizedname_level1": "artificial intelligence"}
{"index": 96, "text": "We present a simple patch-based matching scheme for generating novel visual appearance in which a new image is synthesized by the optimal pasting of small patches of input sample texture image. First, we use this patch-based matching method as an efficient and simple textured synthesis algorithm to produce a wide range of textures with superb visual appearance. Second, we extend the algorithm for the texture transfer - rendering an image in the style of a different texture image. Third, we generalize the algorithm for processing images by examples, called \"patch-based image analogies\". From this perspective, the texture synthesis and texture transfer are the special cases of the image texturing. Experimental results show that the proposed scheme is extremely efficient and effective for the image texturing problems, such as texture synthesis, texture transfer, line art rendering, etc. Due to the efficiency of the patch-based sampling, our method outperforms many existing algorithms with comparable visual quality.", "paperid": 2147616632, "normalizedname_level1": "artificial intelligence"}
{"index": 97, "text": "Iris recognition is a novel technology in the field of biometric feature recognition and has many advantages over other techniques. Iris recognition means to recognize personal identity based on the iris of the person. Compared with other biometric feature recognition methods, this technique is accurate and reliable against artifacts. In this paper we propose a new iris recognition system. The idea proposed uses a multiscale edge detection using wavelet maxima for iris localization followed by a Gabor filter bank decomposition for feature extraction while matching is carried out by computing the Hamming codes of different irises. The algorithms used in this paper are computationally effective, reliable, robust, and the accuracy of the presented work shows that this system is ideal for human recognition applications.", "paperid": 2098056388, "normalizedname_level1": "artificial intelligence"}
{"index": 98, "text": "In this article, we present a novel probabilistic iris quality measure based on a Gaussian mixture model (GMM). We compare its behavior to that of other standard iris quality metrics on two different types of noise which can corrupt the iris texture: occlusions and blurring. In the case of occlusions, we compare our GMM-based quality measure to an active contour method for eyelids and eyelashes detection. Finally, in the case of iris blurring, we compare our quality measure to a standard method based on Fourier transform and wavelets. For the latter, we have developed a new method to detect blur suitable for iris images. In our tests, we have used the ICE 2005 database and OSIRIS, an iris reference system based on the classical approach proposed by Daugman and developed in the framework of BioSecure European Network of Excellence for comparative evaluation purposes. Experiments show a significant improvement of performance when our GMM- based quality measure is used instead of the classical methods above mentioned. In particular, results show that this probabilistic quality measure based on a GMM trained on good quality images is independent from the kind of noise involved.", "paperid": 2131423514, "normalizedname_level1": "artificial intelligence"}
{"index": 99, "text": "Through the evaluation of the 31Phosphorus Magnetic Resonance Spectroscopy (31P-MRS), we can distinguish three types of diagnosis: hepatocellular carcinoma, normal and cirrhosis. 71 samples of 31P-MRS data are selected including hepatocellular carcinoma, normal and cirrhosis tissue. Back-propagation neural network (BP) and Radial Basis Function Neural Network (RBF) are applied to analyze 31P-MRS data, develop neural network models of 31P-MRS for the diagnostic classification of hepatocellular carcinoma. The results suggest that BP models have better performance than RBF models. Neural network models based on 31P-MRS data offer an alternative and promising technique for diagnostic prediction of hepatocellular carcinoma in vivo.", "paperid": 2103207995, "normalizedname_level1": "artificial intelligence"}
{"index": 100, "text": "In this paper, we propose a new head pose tracking method for mobile devices. Recently, head pose tracking technologies are expected to be incorporated into various situations such as robot vision. Especially, we focus on the applications using a mobile system. However, head pose estimation methods usually require a high performance hardware although the mobile systems have limited computational resource. Therefore, we propose a low cost computing algorithm of head pose tracking for the purpose of developing mobile system applications. The proposed method has two stages: (1) fast rough tracking and (2) iterative sampling stage. The proposed method has a merit of low computing cost, and is able to be applied to mobile systems. In order to confirm the effectiveness of the proposed method, we show the comparison results of two types of iterative sampling algorithms. The experimental results demonstrate that our method can be incorporated into actual mobile system applications.", "paperid": 2012394303, "normalizedname_level1": "artificial intelligence"}
{"index": 101, "text": "In order to overcome the disadvantages of the K-Means Clustering algorithm, such as the poor global search ability, being sensitive to initial cluster centric, as well as the vulnerable to trap in local optima and the slow convergence velocity in later period of the original Artificial Bee Colony (ABC) algorithm, a Modified ABC algorithm was proposed. Modified Artificial Bee Colony algorithm combined with K-means Clustering algorithm, named it as MABC-K-means algorithm, to establish Hybrid algorithm for solving framework. Through extensive testing, the MABC-K-means algorithm can improve cluster performance effectively. Finally, according to optimization solution strategy, instantiate Customer Relationship Management issue in the process of instantiating framework.", "paperid": 2543325350, "normalizedname_level1": "artificial intelligence"}
{"index": 102, "text": "This paper presents a surveillance system which alerts when controversy or an intruder is detected by using CCTV. The system takes advantage of sound signal processing to identify fighting and image processing to detect an intruder, then will notify and sent a signal to the system administer when an event occurs. The result shows that the energy of sound signal during fighting is higher than when in normal situation and the change pixel is increased when there is an intruder. According to the result, this concept is applied to CCTV system in order to avoid human fatigue leading to the error to work quality after working with the monitor for long hours.", "paperid": 2403518287, "normalizedname_level1": "artificial intelligence"}
{"index": 103, "text": "Advances in research and clinical techniques are providing increasing quantities of data at improved spatio-temporal resolution. It is therefore imperative to develop matching approaches for efficient analysis and intuitive presentation of this data. Using the example of advanced magnetic resonance imaging, this article will illustrate the challenges involved in computational reconstruction and interactive visualization of the three-dimensional cardiac anatomy, based on magnetic resonance imaging data with para-cellular resolution.", "paperid": 2123024678, "normalizedname_level1": "artificial intelligence"}
{"index": 104, "text": "This study investigated how people achieve pointing and path following tasks using a vibrotactile device. In pointing tasks, subjects are asked to reach a target as fast as possible guided by two types of tactile information generated by a specific glove we developed. The first type of information is a constant vibrating stimulation and the second one is a frequency modulation based vibration. A comparison using the visual channel when performing the same task is done is presented. For each modality, two experiments were performed. The first one is 1D: the subjects reach a target situated on a line. The second one is 2D and the target is in the horizontal plane. The results we obtained showed that tactile based pointing tasks obey to the classical Fitts’ law. Furthermore, we show that the frequency modulation based vibration mode is more useful than the constant one for pointing tasks. In path following experiments, we have a comparison between global and local visual feedback. Results show that in case of lacking global spatial information, vibrotactile feedback can give similar performance as visual feedback.", "paperid": 2099568488, "normalizedname_level1": "artificial intelligence"}
{"index": 105, "text": "This paper addresses the reconstruction of 3D human body models from 2D video sequences. Considering that the input frames are already segmented, the proposed technique consists of three stages. These stages are independently applied over each segmented frame. Firstly, a skeleton of a human figure obtained from the segmented image is extracted by means of a fast algorithm based on a Voronoi diagram of the boundary points. Afterwards, the skeleton is labelled according to the human body parts (e.g. head, upper arm, lower arm, torso, etc). Secondly, an initial 3D model posture is estimated from the labelled skeleton. Finally, an iterative closest point (ICP) implementation is used to refine the initial model posture by maximizing the similarity between the projected 3D model and the segmented image. Experimental results with video sequences are presented.", "paperid": 1603909895, "normalizedname_level1": "artificial intelligence"}
{"index": 106, "text": "This paper reports our recent work on optimizing the AF (articulatory features) based confidence measures, and combining them with the traditional HMM-based confidence measures. Different articulatory properties are analyzed using a separate AF-based confidence calculation method proposed in this paper, and are observed to be both complementary and redundant. A more compact subset is chosen and assembled based on the above analyses and contrast experiments, which gets a relative improvement of 12.7% on EER compared with using the whole AF set. The optimized AF-based confidence is finally combined with the HMM-based confidence, which increases the rejection rate for the out-of-vocabulary tests with no accuracy loss of the in-vocabulary tests compared with the baseline HMM system, and the relative improvement for the false acceptance rate is 34% on the development sets and 35.3% on the testing sets.", "paperid": 1991839768, "normalizedname_level1": "artificial intelligence"}
{"index": 107, "text": "One of the most difficult problems in fingerprint recognition has been that the recognition performance is significantly influenced by distorted fingertip surface condition, which may vary depending on environmental or personal causes. Addressing this problem, this paper presents the three-rate hybrid Kohonen neural network (TRHKNN) for distorted fingerprint image processing in conditions of wide variation in degree of distortion. This TRHKNN consists of ldquofastrdquo Kohonen neural network (FKNN), ldquomiddlerdquo Kohonen neural network (MKNN) and ldquoslowrdquo Kohonen neural network (SKNN). The received TRHKNN has not only high speed of image recognition, but also high speed of image restoration. This approach demonstrates that the proposed TRHKNN is capable not only to identify the distorted image of fingerprint but also to restore the undistorted image of fingerprint. These examples with simulations by MATLAB/Simulink environment show the computing procedure and applicability of TRHKNN for fast-acting fingerprint image recognition in distorted fingertip surface conditions.", "paperid": 2096478795, "normalizedname_level1": "artificial intelligence"}
{"index": 108, "text": "Recognition of temporal/dynamical patterns is among the most difficult pattern recognition tasks. In this paper, based on a recent result on deterministic learning theory, a deterministic framework is proposed for rapid recognition of dynamical patterns. First, it is shown that a time-varying dynamical pattern can be effectively represented in a time-invariant and spatially distributed manner through deterministic learning. Second, a definition for characterizing similarity of dynamical patterns is given based on system dynamics inherently within dynamical patterns. Third, a mechanism for rapid recognition of dynamical patterns is presented, by which a test dynamical pattern is recognized as similar to a training dynamical pattern if state synchronization is achieved according to a kind of internal and dynamical matching on system dynamics. The synchronization errors can be taken as the measure of similarity between the test and training patterns. The significance of the paper is that a completely dynamical approach is proposed, in which the problem of dynamical pattern recognition is turned into the stability and convergence of a recognition error system. Simulation studies are included to demonstrate the effectiveness of the proposed approach", "paperid": 2160588587, "normalizedname_level1": "artificial intelligence"}
{"index": 109, "text": "In this paper, a nonlinear transversal fuzzy filter with online clustering is proposed. It is based on radial-basis-function networks (RBFN) and functionally is equivalent to the TSK fuzzy system. The proposed filter has the following features: (1) hierarchical structure for self-construction. The fuzzy rules, i.e., the RBF neurons are generated automatically during the training process. (2) Online clustering. Instead of selecting the centers and widths of membership functions arbitrarily, an online clustering method is applied to ensure reasonable representation of input terms associated with an input variable. It not only ensures proper feature representation, but also it optimizes the structure of the filter by reducing the number of fuzzy rules. (3) All free parameters in the premise and consequent parts are determined online by a hybrid sequential algorithm without repeated computation to facilitate real-time applications. Using the proposed hybrid learning algorithm, low computation load and less memory requirements are achieved. Simulation results show that the proposed filter can obtain better accuracy with lower system resource requirements compared with other existing approaches.", "paperid": 2153455590, "normalizedname_level1": "artificial intelligence"}
{"index": 110, "text": "Transcription is typically a long and expensive process. In the last year, crowdsourcing through Amazon Mechanical Turk (MTurk) has emerged as a way to transcribe large amounts of speech. This paper presents a two-stage approach for the use of MTurk to transcribe one year of Let's Go Bus Information System data, corresponding to 156.74 hours (257,658 short utterances). This data was made available for the Spoken Dialog Challenge 2010 [1]1. While others have used a one stage approach, asking workers to label, for example, words and noises in the same pass, the present approach is closer to what expert transcribers do, dividing one complicated task into several less complicated ones with the goal of obtaining a higher quality transcript. The two stage approach shows better results in terms of agreement with experts and the quality of acoustic modeling. When “gold-standard” quality control is used, the quality of the transcripts comes close to NIST published expert agreement, although the cost doubles.", "paperid": 2154624463, "normalizedname_level1": "artificial intelligence"}
{"index": 111, "text": "In order to implement visual surveillance system, many problems have to be considered. Paper deals with one of them, illumination variations. Illumination variations, in the stream, produce additive noise which could cause false motion detection. To overcome the problem, wavelet motion detection algorithm based on energy minimization has been considered. It is important to notice, in order to get an efficient algorithm, it is required to devise a method for choosing wavelets. The method covers correct detection and processing time.", "paperid": 2140465357, "normalizedname_level1": "artificial intelligence"}
{"index": 112, "text": "This paper addresses the problem of joint decoding of stereo JPEG image pairs. Such images typically contain a high degree of redundancy. Predictive coding could efficiently capture this redundancy, but cameras would have to implement proprietary encoding solutions in this case as no such standard technology is available. We propose to rather use the popular JPEG compression tools in the cameras, and focus on the joint decoding problem for quality enhancement. We formulate this as a constrained optimization problem and show how regularization leads to more consistent results. It is similar to a distributed source coding framework, where the exploitation of the correlation at the decoder permits to save on the overall bandwidth. Experiments on natural stereo images show an improvement in both visual quality and PSNR when compared to separate decoding.", "paperid": 2078156724, "normalizedname_level1": "artificial intelligence"}
{"index": 113, "text": "Ambient obscurance (AO) is an effective approximation of global illumination, and its screen-space (SSAO) versions that operate on depth buffers only are widely used in real-time applications. We present an SSAO method that allows the obscurance effect to be determined from the entire depth buffer for each pixel. Our contribution is two-fold: Firstly, we build an obscurance estimator that accurately converges to ray traced reference results on the same screen-space geometry. Secondly, we generate an intermediate representation of the depth field which, when sampled, gives local peaks of the geometry from the point of view of the receiver. Only a small number of such samples are required to capture AO effects without undersampling artefacts that plague previous methods. Our method is unaffected by the radius of the AO effect or by the complexity of the falloff function and produces results within a few percent of a ray traced screen-space reference at constant real-time frame rates.", "paperid": 2059108264, "normalizedname_level1": "artificial intelligence"}
{"index": 114, "text": "In this paper we present A Framework for the Combination of Different Arabic Handwritten Word Recognition Systems to achieve a decision with a higher performance. This performance can be expressed by lower rejection rates and higher recognition rates. The used methods range from voting schemes based on results of different recognizer to a neural network decision based on normalized confidences. This work presents an extension of the well known combination methods for a large lexicon, an extension from maximum 30 classes (e.g., 10 classes for digits classification) to 937 classes for the IfN/ENIT-database. In addition, different reject rules based on the evaluation and analysis of individual and combined systems output are discussed. Different threshold function for reject levels are tested and evaluated. Tests with a set of recognizer, which participated in the ICDAR 2007 competition and based on set coming from the IfN/ENIT-database show that a word error rate (WER) of 5.29% without reject and with a reject rate less than 25% even a word error rate of less than 1%.", "paperid": 2047018143, "normalizedname_level1": "artificial intelligence"}
{"index": 115, "text": "A new scheme of object tracking with mean shift is put forward in this paper. At first, texture feature is fused in the processing by Local Ternary Pattern (LTP). Since LTP is sensitive to local noise, least median of squares (LMedS) algorithm is used to adaptively calculate the noise threshold for accurate estimation of the LTP texture information. Furthermore, target scale and orientation is estimated in case of partial occlusion or rotation, so as to realize robust object tracking. Experimental results show that the proposed algorithm can acquire robust tracking performance under complex background .", "paperid": 2055446098, "normalizedname_level1": "artificial intelligence"}
{"index": 116, "text": "Abdominal aortic aneurysm (AAA) is a cardiovascular disease which mostly appears in elderly people. Due to the weakening of aortic wall, a rupture occurs in the most inner layer of aorta and a thrombus is generated. If the diameter exceeds greater than 5.5 cm, a treatment strategy is required. As a result, CT imaging is utilized to screen and evaluate thrombus parameters for treatment. Exploitation of automatic techniques improves the pre- and post-treatment evaluation. In this paper, at first an algorithm based on spatial fuzzy c-means is applied to the CT image. Then lumen and thrombus are segmented automatically by tuning of morphological operators and thresholding parameters.", "paperid": 2158537398, "normalizedname_level1": "artificial intelligence"}
{"index": 117, "text": "To improve the positioning accuracy of a certain type of bridge gantry machining center, a positioning error compensation method based on support vector regression (SVR) is presented. After analyzing the influence of training data sets distribution, and pointing out that SVR may have a substandard performance when the training data are distributed sparsely at neighborhood of outliers or distributed in a small range, a training data sets constructing criterion is proposed. SVR is employed in position error compensation of a bridge gantry machining center to establish the model of the positioning error, actual positioning error compensation effects shows that the positioning accuracy and compensation operating efficiency are improved effectively.", "paperid": 2046543878, "normalizedname_level1": "artificial intelligence"}
{"index": 118, "text": "This paper describes a fully automated stone fly-larvae classification system using a local features approach. It compares the three region detectors employed by the system: the Hessian-affine detector, the Kadir entropy detector and a new detector we have developed called the principal curvature based region detector (PCBR). It introduces a concatenated feature histogram (CFH) methodology that uses histograms of local region descriptors as feature vectors for classification and compares the results using this methodology to that of Opelt [Opelt, A, et.al., 2006.] on three stonefly identification tasks. Our results indicate that the PCBR detector outperforms the other two detectors on the most difficult discrimination task and that the use of all three detectors outperforms any other configuration. The CFH methodology also outperforms the Opelt methodology in these tasks", "paperid": 2159407290, "normalizedname_level1": "artificial intelligence"}
{"index": 119, "text": "In this paper we present a novel approach to motion segmentation. Initially, the video sequence is divided in overlapping temporal windows. Our algorithm performs over-segmentation on each window separately. Concretely, quadruples of trajectories are used as motion subspaces and the Ordered Residual Kernel is employed as an affinity metric between trajectories. The corresponding graph of the computed affinity matrix is partitioned via a random walk algorithm. A motion dissimilarity score is proposed to correlate the computed segments as well as a merging mechanism that fuses the individual segmentation results of successive windows. Experiments on the Berkeley motion segmentation dataset demonstrate the scalability and accuracy of our method compared to the existing approaches.", "paperid": 2014776913, "normalizedname_level1": "artificial intelligence"}
{"index": 120, "text": "The study is on a caLibration method for grating projection plane in 3D image measurement system and discussing the problems need to be noted in appLication. It brings forward a new method for curving surface based on 3D on-Line measuring points, according to the on-Line distribution rule of 3D points group on the grating projection measuring system. In portrait image reconstructing, this method can generate good-character triangles, avoid missing points, cracks or holes and produce a smooth and consecutive curving surface.", "paperid": 2026300485, "normalizedname_level1": "artificial intelligence"}
{"index": 121, "text": "In this paper, a highly effective system for Thai speech recognition is proposed. The speech recognizer for so-called speaker-independent is created by using Continuous Density Hidden Markov Model (CDHMM). In the acoustic level, the models trained for both speaker genders, and for each separate gender are investigated and tested in terms of accuracy. Experimental evaluation shows that with the acoustic models combination, the accuracy could be improved considerably in the acoustic level. The acoustic combination can support spoken utterances from both genders and still provide the high accuracy simultaneously. Interestingly, when using the acoustic models combination, the syllable accuracy of 89.84% is achieved with 4.53% improvement over using the conventional acoustic models trained for both genders.", "paperid": 2547936031, "normalizedname_level1": "artificial intelligence"}
{"index": 122, "text": "A crucial component of an intelligent system is its knowledge base that contains knowledge about a problem domain. Knowledge base development involves domain analysis, context space definition, ontological specification, and knowledge acquisition, codification and verification. Knowledge base anomalies can affect the correctness and performance of an intelligent system. In this paper, we adopt a fixpoint semantics that is based on a multi-valued logic for a knowledge base. We then use the fixpoint semantics to provide formal definitions for four types of knowledge base anomalies: inconsistency, redundancy, incompleteness, circularity. We believe such formal definitions of knowledge base anomalies helps pave the way for a more effective knowledge base verification process.", "paperid": 2128897049, "normalizedname_level1": "artificial intelligence"}
{"index": 123, "text": "This paper presents the study of time series in gravitational lensing to solve the time delay problem in astrophysics. The time series are irregularly sampled and noisy. There are several methods to estimate the time delay between this kind of time series, and this paper proposes a new method based on artificial neural networks, in particular, General Regression Neural Networks (GRNN), which is based on Radial Basis Function (RBF) networks. We also compare other typical artificial neural network architectures, where the learning time of GRNN is better. We analyze artificial data used in the literature to compare the performance of the new method against state-of-the-art methods. Some statistics are presented to study the significance of results.", "paperid": 2115244539, "normalizedname_level1": "artificial intelligence"}
{"index": 124, "text": "This work is meant as a step towards developing an effective and efficient procedure for a special type of the text categorization problem. A set of documents and a set of their categories are assumed. However, in addition to being assigned to a specific category, each document belongs to a certain sequence of documents, referred to as a case, comprising of documents from the same class. The problem considered is how to classify a document to a proper sequence of documents, or case, within a specified category. If each case is treated as a separate category, then the potential training datasets are rather small. We propose an algorithm which is based on a combination of two indicators characterizing a document to be classified: one reflecting its similarity to a case and one reflecting its similarity to a category. These indicators are based on the measure of subsethood of fuzzy sets. We study the effectiveness of the proposed algorithm for various combinations of weights of both indicators and subsethood measures employed.", "paperid": 1648704183, "normalizedname_level1": "artificial intelligence"}
{"index": 125, "text": "Accurate segmentation of breast on MR images is an essential and crucial step for computer-aided breast disease diagnosis and surgical planning. In this paper, an effective approach is proposed for segmenting the breast image into different regions, each corresponding to a different tissue. The segmentation work flow comprises two key steps. Firstly, we use the threshold-based method and morphological operations to determine the breast-air boundary and breast-chest wall so that the breast region can be extracted. Then a kernelled fuzzy C-means algorithm with spatial information (SKFCM) is used to separate the fibroglandular tissues from the fat. The proposed method is used to segment the clinical breast MR images. Experimental results have been shown visually and achieve reasonable consistency. The SKFCM method is appropriate for the problem of breast tissue segmentation.", "paperid": 1975423116, "normalizedname_level1": "artificial intelligence"}
{"index": 126, "text": "Principal component analysis (PCA) is a useful statistical technique for the reduction of data dimensionality. When applied to the accelerometer data in gait analysis PCA assigns common gait patterns among subjects or provides gait classification. In this paper, we study the results of PCA applied to datasets recorded with three-axial accelerometers placed on thigh, shank, and foot in subjects with hemiplegia. In particular, we analyze the impact of both representative stride (the most similar to all other strides in the sequence) and outlier stride (the most different from all other strides in the sequence) on PCA results. PCA sensitivity to data preparation was tested on three datasets: complete gait sequence, gait sequence without the outlier stride, and on representative stride.", "paperid": 2025987138, "normalizedname_level1": "artificial intelligence"}
{"index": 127, "text": "Current neuroscience has identified several constructs to increase the effectiveness of upper extremity rehabilitation. One is the use of progressive, skill acquisition-oriented training. Another approach emphasizes the use of bilateral activities. Building on these principles, this paper describes the design and feasibility testing of a robotic/virtual environment system designed to train the arm of persons who have had strokes. The system provides a variety of assistance modes, scalable workspaces and hand-robot interfaces allowing persons with strokes to train multiple joints in three dimensions. The simulations utilize assistance algorithms that adjust task difficulty both online and offline in relation to subject performance. Several distinctive haptic effects have been incorporated into the simulations. An adaptive master-slave relationship between the unimpaired and impaired arm encourages active movement of the subject's hemiparetic arm during a bimanual task. Adaptive anti-gravity support and damping stabilize the arm during virtual reaching and placement tasks. An adaptive virtual spring provides assistance to complete the movement if the subject is unable to complete the task in time. Finally, haptically rendered virtual objects help to shape the movement trajectory during a virtual placement task. A proof of concept study demonstrated this system to be safe, feasible and worthy of further study.", "paperid": 1989139345, "normalizedname_level1": "artificial intelligence"}
{"index": 128, "text": "This paper describes a way to adapt the recognizer to pronunciation variability by dynamically sharing Gaussian densities across phonetic models. The method is divided in three steps. First, given an input utterance, an HMM recognizer outputs a lattice of the most likely word hypotheses. Then, the canonical pronunciation of each hypothesis is checked by comparing its theoretical phonetic features to those automatically extracted from speech. If the comparisons show that a phoneme of an hypothesis has likely been pronounced differently, its model is transformed by sharing its Gaussian densities with the ones of its possible alternate phone realization(s). Finally, the transformed models are used in a second-pass recognition. Sharings are dynamic because they are automatically adapted to each input speech. Experiments showed a 5.4% relative reduction in word error rate compared to the baseline and a 2.7% compared to a static method.", "paperid": 2121354157, "normalizedname_level1": "artificial intelligence"}
{"index": 129, "text": "In this work we propose a new algorithm for fragile, high capacity yet file-size preserving watermarking of MPEG-2 streams. Watermarking is done entirely in the compressed domain, with no need for full or even partial decompression. The algorithm is based on a previously developed concept of VLC mapping for compressed domain watermarking. The entropy-coded segment of the video is first parsed out and then analyzed in pairs. It is recognized that there are VLC pairs that never appear together in any intra-coded block. The list of unused pairs is systematically generated by the intersection of \"pair trees.\" One of the trees is generated from the main VLC table given in ISO/IEC 13818-2:2000 standard. The other trees are dynamically generated for each intra coded blocks. Forcing one VLC pairs in a block to one of the unused ones generates a watermark block. The change is done while maintaining run/level change to a minimum. At the decoder, the main pair tree is created offline using publicly available VLC tables. Through a secure key exchange, the indices to unused code pairs are communicated to the receiver. We show that the watermarked video is reasonably resistant to forgery attacks and remains secure to watermark detection attempts.", "paperid": 2060149200, "normalizedname_level1": "artificial intelligence"}
{"index": 130, "text": "A novel fault diagnostics and prognostics algorithm based on hidden Markov model (HMM) is proposed. The algorithm combines fault diagnostics and prognostics in a unified framework. The algorithm has been fully tested by using experimental data from a rotating shift testbed in our laboratory.", "paperid": 2122066118, "normalizedname_level1": "artificial intelligence"}
{"index": 131, "text": "Human action recognition based on joints is a challenging task. The 3D positions of the tracked joints are very noisy if occlusions occur, which increases the intra-class variations in the actions. In this paper, we propose a novel approach to recognize human actions with weighted joint-based features. Previous work has focused on hand-tuned joint-based features, which are difficult and time-consuming to be extended to other modalities. In contrast, we compute the joint-based features using an unsupervised learning approach. To capture the intra-class variance, a multiple kernel learning approach is employed to learn the skeleton structure that combine these joints-base features. We test our algorithm on action application using Microsoft Research Action3D (MSRAction3D) dataset. Experimental evaluation shows that the proposed approach outperforms state-of-the-art action recognition algorithms on depth videos.", "paperid": 1910723988, "normalizedname_level1": "artificial intelligence"}
{"index": 132, "text": "Aiming at the disadvantage (premature convergence) of the Ant Colony Optimization (ACO), a mechanism called global random-proportional rule (GRP) is presented. The mechanism adjusts the transition probability proportionally and facilitates the exploration by increasing the probability of selecting solution components with low pheromone trail. It can avoid premature convergence of ACO and exploit more strongly solutions. The results show that ACO with GRP are superior to the existing ACO and the mechanism is useful to improve the performance of any versions of ACO by investigating the functioning of GRP in the Traveling Salesman Problem (TSP).", "paperid": 1963995412, "normalizedname_level1": "artificial intelligence"}
{"index": 133, "text": "Object classes generally contain large intra-class variation, which poses a challenge to object detection schemes. In this work, we study visual subcategorization as a means of capturing appearance variation. First, training data is clustered using color and gradient features. Second, the clustering is used to learn an ensemble of models that capture visual variation due to varying orientation, truncation, and occlusion degree. Fast object detection is achieved with integral image features and pixel lookup features. The framework is studied in the context of vehicle detection on the challenging KITTI dataset.", "paperid": 2131087702, "normalizedname_level1": "artificial intelligence"}
{"index": 134, "text": "We propose a new framework for image recognition by selectively pooling local visual descriptors, and show its superior discriminative power on fine-grained image classification tasks. The representation is based on selecting the most confident local descriptors for nonlinear function learning using a linear approximation in an embedded higher dimensional space. The advantage of our Selective Pooling Vector over the previous state-of-the-art Super Vector and Fisher Vector representations, is that it ensures a more accurate learning function, which proves to be important for classifying details in fine-grained image recognition. Our experimental results corroborate this claim: with a simple linear SVM as the classifier, the selective pooling vector achieves significant performance gains on standard benchmark datasets for various fine-grained tasks such as the CMU Multi-PIE dataset for face recognition, the Caltech-UCSD Bird dataset and the Stanford Dogs dataset for fine-grained object categorization. On all datasets we outperform the state of the arts and boost the recognition rates to 96.4%, 48.9%, 52.0% respectively.", "paperid": 1969700546, "normalizedname_level1": "artificial intelligence"}
{"index": 135, "text": "This report proposes an adaptive recognition system, which is based on Kohonen self-organization network (KSOM). As the goals in the research on artificial neural network are to improve the recognition capability of the network and at the same time minimize the time needed for learning the patterns, these goals could be achieved by combining two types of learning, i.e. supervised learning and unsupervised learning. We have developed a new kind of hybrid neural learning system, combining unsupervised KSOM and supervised back-propagation learning rules. This hybrid neural system will henceforth be referred to as hybrid adaptive SOM with winning probability function and supervised BP or KSOM(WPF)-BP. This hybrid neural system could estimate the cluster distribution of given data, and directed it into predefined number of cluster neurons through creation and deletion mechanism. Comparison with other developed hybrid neural system is done for determination of various odors from Martha Tilaar Cosmetics product in an artificial odor recognition system. The performance of our developed learning system in term of its recognition ability and its learning time is explored in this report.", "paperid": 2148532949, "normalizedname_level1": "artificial intelligence"}
{"index": 136, "text": "Compressive sensing (CS) takes advantage of the spatial and spectral redundancy in hyperspectral imagery to take fewer measurements than traditional sensors. We simulate compressively sensed hyperspectral airborne images of a HyMap image of Cooke City, Montana using the Coded Aperture Snapshot Spectral Imager Dual Disperser (CASSI-DD) sensor model. Flake et al's novel algorithm (2013), which incorporates both spatial total variation (TV) and spectral smoothing, is used to reconstruct the hyperspectral image cube from the CS measurement.[1] We evaluate the effect of the number of physical measurements (nt) on atmospheric compensation, anomaly detection and target detection. The results indicate that the utility of CS is application-dependent and the optimal nt value is application-driven.", "paperid": 2765431141, "normalizedname_level1": "artificial intelligence"}
{"index": 137, "text": "In this paper we introduce a new scheme for making decision in multiple classifier systems through a fuzzy interface system. The rules of the fuzzy interface are provided by majority vote combiner. The base classifiers we have used in the multiple classifier system is Support Vector Machines (SVM) classifiers and the difference between these classifiers is in their feature-sets and the parameters of SVM classifiers. The combination of multiple classifiers, with different features sets is performed by majority vote combination method. In order to demonstrate the performance of this multiple classifier system, we implement this system for classifying EEG signals. The task is to classify EEG signals in order to translate a subject's intentions into a technical control signal to control the peripheral environment. We compare the individual classifiers with their ensemble counterparts and discuss the results.", "paperid": 2154860509, "normalizedname_level1": "artificial intelligence"}
{"index": 138, "text": "This paper considers the problem of shape-from-shading using nearby planar distributed illuminants. It is shown that a rectangular planar nearby distributed uniform isotropic illuminant shining on a small Lambertian surface patch is equivalent to a single isotropic point light source at infinity. A closed-form solution is given for the equivalent point light source direction in terms of the illuminant corner locations. Equivalent point light sources can be obtained for multiple rectangular illuminants allowing standard photometric stereo algorithms to be used. An extension is given to the case of a rectangular planar illuminant with arbitrary radiance distribution. It is shown that a Walsh function approximation to the arbitrary illuminant distribution leads to an efficient computation of the equivalent point light source directions. A search technique employing a solution consistency measure is presented to handle the case of unknown depths. Applications of the theory presented in this paper include visual user-interfaces using shape-from-shading algorithms making use of the illumination from computer monitors, or movie screens.", "paperid": 2159788100, "normalizedname_level1": "artificial intelligence"}
{"index": 139, "text": "Making a computer generate its own emotion is an important part of the affective computing, and this would have wide applications in human-computer interaction and artificial intelligence. In this paper, we will describe an emotion generation model for a multimodal virtual human. The relationship among the emotion, mood and personality are discussed firstly, and the PAD (pleasure-arousal-dominance) emotion space is used to define the emotion and the mood. Then, we use a random graphical model to generate emotion based on the evaluation of the overall influence of mood, previous emotion and the outside stimulations. Finally, a 3D virtual human head with facial expressions is designed to show the emotion generation outputs. Experimental results demonstrate that the emotion generation model based on random graphs works effectively and meets the basic principle of human emotion generation.", "paperid": 2141764352, "normalizedname_level1": "artificial intelligence"}
{"index": 140, "text": "The detection of ground moving targets, such as humans and ground vehicles is important for border security. Using the data received from a passive radar system, an algorithm was developed to pinpoint the location of ground moving objects over a fixed area. The algorithm combines the data from cross-correlation range-Doppler techniques with the data from angle of arrival Multiple Signal Classification (MUSIC) techniques. The extracted time-delay values from the derived ambiguity function are converted into ellipse ranges and then joined with MUSIC'S derived target angles; the intersection point between the range ellipses and angle of arrival (AOA) lines provides estimation of the target location. By combining the derived intersection points from all the various still images of a moving target, a target tracking plot was generated. These algorithms were tested theoretically in both free space and in the presence of artificial background noise particles. The system and algorithm was also tested experimentally using a 1-GHz cell phone antenna (emitting a BPSK signal) as the source, dipole antennas as the receivers, and a conducting cylinder as the target. This experiment was conducted in an outdoor environment to simulate noisy and realistic conditions.", "paperid": 2136626854, "normalizedname_level1": "artificial intelligence"}
{"index": 141, "text": "This paper presents a music mood classification model based on arousal-valence (AV) values for music recommendation system. We analyse the collected music mood tags and AV values from 10 subjects and classify music mood on the AV plane into 8 regions by using k-means clustering algorithm. For each region, we propose representative mood tags by using statistics. We find that some regions can be identified by representative mood tags like previous models but some mood tags are overlapped in almost regions. The proposed model can express a region with several representative moods and a mood distributed uniformly in almost regions.", "paperid": 1558959230, "normalizedname_level1": "artificial intelligence"}
{"index": 142, "text": "Facial paralysis is a debilitating condition in which sufferers experience unilateral paralysis of the left or right facial nerve. An evidence based assessment of a patient's condition is almost impossible because all current grading scales are subjective. A quantitative, practical, reliable system would be an invaluable tool in this field of neurootology. Demonstrated here is a system which intelligently quantifies the facial damage in 43 testing videos from 14 subjects. Using an artificial neural network the average mean squared error for the system is 1.6%.", "paperid": 2167643171, "normalizedname_level1": "artificial intelligence"}
{"index": 143, "text": "Glaucoma, the second leading cause of blindness worldwide, is an optic neuropthy characterized by distinctive changes in the optic nerve head (ONH) and visual field. In this context, the Heidelberg Retina Tomograph (HRT), a confocal scanning laser technology, has been commonly used to detect glaucoma and monitor its progression. In this paper, we present a new framework for detection of glaucomatour progression using the HRT images. In contrast to previous works that do not integrate a priori knowledge available on the images and particularly the spatial pixel dependency in the change detection map, we propose the use of the Markov Random Field to handle a such dependency. To our knowledge, the task of inferring the glaucomatous changes with a Variational Expectation Maximization VEM algorithm will be used for the first time in the glaucoma diagnosis framework. We then compared the diagnostic performance of the proposed framework to existing methods of progression detection.", "paperid": 2022788996, "normalizedname_level1": "artificial intelligence"}
{"index": 144, "text": "The paper presents an Arabic automatic segmentation system to be utilized in the development of an Arabic speech recognition system. The automatic segmentation system is used to label a speech database system that is used in the training of continuous, phoneme based speaker independent Hidden Markov Models. Our experiments showed that 7.8 % of the phoneme boundaries of automatic segmented data deviate from those that were manually segmented more than 30 milliseconds while 0.78 % deviate more than 70 milliseconds. Our experiments showed also that automatic segmentation led to improvement in speech recognition accuracy of 0.49 % for a 306 words bigram language model test and 0.14% for 1340 words bigram model", "paperid": 1554482151, "normalizedname_level1": "artificial intelligence"}
{"index": 145, "text": "In this paper, we propose a Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) and multiple kernel learning (MKL) based multi-modal affect recognition scheme (LSTM-MKL). It takes the LSTM-RNN advantage to model the long range dependencies between successive observations, and uses the MKL power to model the non-linear correlations between the inputs and outputs. For each of the affect dimensions (arousal, valence, expectancy, and power), two LSTM-RNN models are trained, one for each modality. In the recognition phase, the audio and visual features are input to the corresponding learned LSTM models, which in turn produce initial estimates of the affect dimensions. The LSTM outputs are further input into a multi-kernel support vector regression (MK-SVR) for the final recognition. Experimental results carried out on the AVEC2012 database, show that compared to the traditional SVR-LLR (Support Vector Machine — local linear regression) or MK-SVR fusion scheme, the proposed LSTM-MKL fusion scheme obtains higher recognition results, with an correlation coefficient (COR) of 0.354, compared to a COR of 0.124 for SVR-LLR, and 0.168 for MK-SVR, respectively.", "paperid": 2011326218, "normalizedname_level1": "artificial intelligence"}
{"index": 146, "text": "Image retargeting techniques aim to obtain retargeted images with different sizes or aspect ratios for various display screens. Various content-aware image retargeting algorithms have been proposed recently. However, there is still no effective objective metric for visual quality assessment of retargeted images. In this paper, we propose a novel full-reference objective metric for assessing visual quality of a retargeted image based on perceptual geometric distortion and information loss. The proposed metric measures the geometric distortion of a retargeted image based on the local variance of SIFT flow vector fields of the image. Furthermore, a visual saliency map is derived to characterize human perception of the geometric distortion. Besides, the information loss in the retargeted image, which is estimated based on the saliency map, is also taken into account in the proposed metric. Subjective tests are conducted to evaluate the performance of the proposed metric. Our experimental results show the good consistency between the proposed objective metric and the subjective rankings.", "paperid": 2102285609, "normalizedname_level1": "artificial intelligence"}
{"index": 147, "text": "We present interactive visual aids to support the exploration and navigation of graph layouts. They include Fisheye Tree Views and Composite Lenses. These views provide, in an integrated manner, overview+detail and focus+context. Fisheye Tree Views are novel applications of the well known fisheye distortion technique. They facilitate the exploration of the hierarchy trees associated with clustered graphs. Composite Lenses are the result of the integration of several lens techniques. They facilitate the display of local graph information that may be otherwise difficult to grasp in large and dense graph layouts.", "paperid": 2163449287, "normalizedname_level1": "artificial intelligence"}
{"index": 148, "text": "The evaluation of the carotid artery wall is fundamental for the assessment of cardiovascular risk. This paper presents the general architecture of an automatic strategy, which segments the lumen-intima and media-adventitia borders, classified under a class of Patented AtheroEdge™ systems (Global Biomedical Technologies, Inc, CA, USA). Guidelines to produce accurate and repeatable measurements of the intima-media thickness are provided and the problem of the different distance metrics one can adopt is confronted. We compared the results of a completely automatic algorithm that we developed with those of a semi-automatic algorithm, and showed final segmentation results for both techniques. The overall rationale is to provide user-independent high-performance techniques suitable for screening and remote monitoring.", "paperid": 2138057604, "normalizedname_level1": "artificial intelligence"}
{"index": 149, "text": "The domain area of this topic is biometric. Speaker Recognition is biometric system. This paper deals with speaker recognition by HMM (Hidden Markov Model) method. The system is able to recognize the speaker by translating the speech waveform into a set of feature vectors using MelFrequency Cepstral Coefficients (MFCC) technique. But, input speech signals at different time may contain variations. Same speaker may utter the same word at different speed which gives us variation in the total number of MFCC coefficients. Vector Quantization (VQ) is used to make the same number of MFCC coefficients. Hidden Markov Model (HMM) provides a highly reliable way of recognizing a speaker. Hidden Markov Models have been widely used, which are usually considered as a set of states with Markovian properties and observations generated independently of those states. With the help of Viterbi decoding most likely state sequence is obtained. This state sequence is used for speaker recognition. For a database of size 50 in normal environment, obtained result is 98% which is better than previous methods used for speaker recognition.", "paperid": 1992908141, "normalizedname_level1": "artificial intelligence"}
{"index": 150, "text": "Muscle noises, line noises and eye movements are the main interferences that make difficulties when interpreting and analyzing electroencephalographic signals. Many methods have been proposed for artifacts removing from EEG measurements, and especially those arising from an ocular source. Principal Component Analysis (PCA) and Independent Component Analysis (ICA) have been proposed to remove ocular artifacts from multichannel EEG. In contrast to this, we present a new algorithm for ocular artifacts removal from a single electroencephalographic channel recording. This method is based on a set of information on brain wave frequencies. Our results on EEG data, collected from healthy subjects, show that our algorithm can effectively detect and remove ocular artifacts in EEG recordings.", "paperid": 1595177580, "normalizedname_level1": "artificial intelligence"}
{"index": 151, "text": "We propose a vision-based method for tracking multiple persons with gray-scale image sequence acquired by a monocular vision sensor in cluttered office environments. This method is based on a novel algorithm for acquiring depth of targets with primitive and robust features, position and size of targets. To cope with long-term occlusions caused by both fixed and moving objects, the method memorizes and utilizes history data of targets' state. We employ MCMC-based particle filter (MCMC-PF) to implement such domain knowledge including, interactions between targets, as well as radial reach filter (RRF) to extract objects in noisy gray-scale images. In experiments, the method could track multiple persons reliably, and recover from errors even when it loses sight of targets.", "paperid": 2134970816, "normalizedname_level1": "artificial intelligence"}
{"index": 152, "text": "The miniaturization required for interfacing with the brain demands new methods of transforming neuron responses (spikes) into digital representations. The sparse nature of neural recordings is evident when represented in a shift invariant basis. Although a compressive sensing (CS) framework may seem suitable in reducing the data rates, we show that the time varying sparsity in the signals makes it difficult to apply. Furthermore, we present an adaptive sampling scheme which takes advantage of the local characteristics of the neural spike trains and electrocardiograms (ECG). In contrast to the global constraints imposed in CS our solution is sensitive to the local time structure of the input. The simplicity in the design of the integrate-and-fire (IF) make it a viable solution in current brain machine interfaces (BMI) and ambulatory cardiac monitoring.", "paperid": 2124050581, "normalizedname_level1": "artificial intelligence"}
{"index": 153, "text": "A single-valued index, relative contrast sensitivity (RCS), was developed to quantitatively evaluate color break-up (CBU) phenomenon. The sensibility of CBU fringe was hypothesized to strongly correlate with RCS, which was defined as the integrated difference of the opponent-color signals filtered by spatial contrast sensitivity functions. Psychometric experiments, based on methods of category scaling and adjustment respectively, were conducted to examine the proposed numerical metric. The results showed that the maximal correlation coefficient was as high as 98.9% between the subjective judgments and the computed numerical values of current experimental results. A useful application of RCS index was demonstrated by the evaluation of four driving schemes for CBU suppression.", "paperid": 2154452737, "normalizedname_level1": "artificial intelligence"}
{"index": 154, "text": "Generative models of pattern individuality attempt to represent the distribution of observed quantitative features, e.g., by learning parameters from a database, and then use such distributions to determine the probability of two random patterns being the same. Considering fingerprint patterns, Gaussian distributions have been previously used for minutiae location and von-Mises distributions for minutiae orientation so as to determine the probability of random correspondence (PRC) between two fingerprints. Motivated by the fact that ridges have not been modeled in generative models, and using representative ridge points in fingerprint matching, ridge information is incorporated into the generative model by using a third distribution for ridge types. The joint probability of minutiae location, minutiae orientation and ridge type is modeled as a mixture distribution. The proposed model offers a more accurate fingerprint representation from which more reliable PRCs can be computed. Based on parameters estimated from fingerprint databases, PRCs using ridge types are seen to be much smaller than PRCs computed with only minutiae.", "paperid": 2128641101, "normalizedname_level1": "artificial intelligence"}
{"index": 155, "text": "Real-time noise removal and depth propagation is a crucial component for surface reconstruction algorithms. Given the recent surge in the development of RGB-D sensors, a host of methods are available for detecting and tracking RGB-D features across multiple frames as well combining these frames to yield dense 3D point clouds. Nevertheless the sensor outputs are sparse in areas where textures are low (for traditional stereo cameras) and high reflectance regions (for Kinect like active sensors). It is crucial to employ a depth estimate propagation or diffusion algorithm to generate best approximation surface curvature in these regions for visualization. In this paper, we extend the Depth Diffusion using Iterative Back Substitution scheme to Kinect like RGB-D sensor data for real time surface reconstruction.", "paperid": 1966821277, "normalizedname_level1": "artificial intelligence"}
{"index": 156, "text": "Regression test is an important part in software development and maintenance. After changing, in order to assure the validity of the modified part and validate there is no side affect to other parts, we must carry out the process of regression test. This paper makes use of the characteristic of hierarchy of Java, first choose the impacted packages. Then by analyzing the relationship of classes in these packages, we can get a set of classes that are impacted by the change. After that, we choose these classes to make a class member dependence analysis, then we can get a set of class members that are impacted by the change. Finally by mapping class members selected to test cases and add some test cases if necessary, we can get a test suite that should be retested. Use this progressive refinement approach, we only need to analyze part of the software and retest the impacted part, and this can reduce the cost of regression test especially when the software is huge", "paperid": 2138736358, "normalizedname_level1": "artificial intelligence"}
{"index": 157, "text": "Steganography is the art and science of hiding information inside a cover object. Although we have number of steganographic techniques like text, audio, video, network and image, image steganography is the one of the most popular steganographic technique in communications. In image steganography, there is a technique in which the least significant bit is modified to hide the secret message, known as the least significant bit (LSB) steganography. Several steganalyzers are developed to detect least significant bit (LSB) matching steganography. Least significant bit matching images are still not well detected, especially, at low embedding rate. In this paper, two features are generated, one is the relative feature and the other is the concatenated feature and compared with the features of other two existing methods and experimentally shown that the detection performance is improved while comparing proposed method features with the other two existing method features.", "paperid": 1506726386, "normalizedname_level1": "artificial intelligence"}
{"index": 158, "text": "This paper reports on a morphological segmentation model for Afaan Oromo based on suffix sequences approach. Understanding and identifying the suffix sequences of a language allow us to detect morpheme boundaries of many words of Afaan Oromo. Morphological segmentation models can be used in many Natural Language Processing applications such as machine translation, speech recognition, information retrieval and part-of-speech tagging. A divisive hierarchical clustering and frequency distribution were used to build a tree of candidate stems from which segmented suffix sequences can be modeled. The proposed morphological segmentation model was evaluated with test word-lists. The accuracy obtained by our morphological segmentation model is encouraging.", "paperid": 2142195562, "normalizedname_level1": "artificial intelligence"}
{"index": 159, "text": "A patch-based super-resolution algorithm is proposed to enlarge a face image from a single low resolution input. Inspired by the property of highly structured faces, the training patches are selected and weighted based on both patch appearance and patch position. The selected patches with certain weights are called bilateral patches, which are incorporated into data consistency to reconstruct the high resolution face image in a patch-wise fashion. Experimental results demonstrate the superiority of the proposed method to some state-of-the-art methods in both visual and quantitative comparisons.", "paperid": 2026319643, "normalizedname_level1": "artificial intelligence"}
{"index": 160, "text": "Understanding the behaviour of well-known algorithms for classical NP-hard optimisation problems is still a difficult task. With this paper, we contribute to this research direction and carry out a feature based comparison of local search and the well-known Christofides approximation algorithm for the Traveling Salesperson Problem. We use an evolutionary algorithm approach to construct easy and hard instances for the Christofides algorithm, where we measure hardness in terms of approximation ratio. Our results point out important features and lead to hard and easy instances for this famous algorithm. Furthermore, our cross-comparison gives new insights on the complementary benefits of the different approaches.", "paperid": 2031283589, "normalizedname_level1": "artificial intelligence"}
{"index": 161, "text": "Encounter the low speed of the general fuzzy methods for image processing, a new fast fuzzy method is proposed in detail, which includes fuzzy image enhancement, fuzzy smoothing and fuzzy edge detection. The fast speed and the effectiveness of the proposed method are verified with application to B-scan image. The better results are obtained over the other image processing methods, which make this new fuzzy B-scan image method to be used in clinical practice.", "paperid": 2141830577, "normalizedname_level1": "artificial intelligence"}
{"index": 162, "text": "Groo (Generic Robot, Object-Oriented) and tt14m (Trash-Talking 14-year-old Moron) are two systems that use simple and computationally inexpensive artificial intelligence mechanisms to produce engaging character behavior for computer games, while remaining within the performance constraints of modern game development. Groo engages in intelligent tactical behavior in a first-person-shooter death-match game using a fairly simple and static behavior network, while tt14m uses simple text processing to attempt engagement in the social aspects of the game \"Counter-Strike\".", "paperid": 2157778994, "normalizedname_level1": "artificial intelligence"}
{"index": 163, "text": "Epidemic models often incorporate contact networks along which the disease can be passed. This study incorporates a restarting-recentering evolutionary algorithm, previously developed to locate extremal epidemic networks, together with a new representation, the toggle-delete representation, for evolvable networks. The goal is to locate networks that were likely to have produced a given epidemic behavior. This goal subsumes a new fitness function for driving selection in network evolution. Earlier representations used networks with a fixed sequence of contact numbers. The new representation can add and remove edges from the network, permitting a search that varies contact numbers within the network. A parameter setting study is performed on an epidemic profile obtained from an random network and then tested on a bimodal profile invented by the researchers. The algorithm succeeds in producing networks that cause epidemics run on them to mimic the specified epidemic profiles.", "paperid": 2114643549, "normalizedname_level1": "artificial intelligence"}
{"index": 164, "text": "One of the grand challenges of AI is to build systems that learn by reading. The ideal system would construct a rich knowledge base capable of automated reasoning. We have built a Learning-by-Reading system and this paper focuses on one aspect of it: the task of integrating together snippets of knowledge drawn from multiple texts to build a single coherent knowledge base. Our evaluation shows that our approach to the knowledge integration is both feasible and promising.", "paperid": 1981852259, "normalizedname_level1": "artificial intelligence"}
{"index": 165, "text": "Hypernetworks consist of a large number of hyperedges that represent higher-order features sampled from training patterns. Evolutionary algorithms have been used as a method for evolving hypernetworks. The order of a hyperedge is defined as the number of feature variables in the hyperedge and it is an important parameter of the hypernetwork model. Previous studies used fixed-order hyperedges which limit model spaces and, thus, the best performance achievable by hypernetworks. Here, we present a method for evolving variable-order hypernetwork models. To find the proper orders automatically, the fitness values are calculated for each hyperedge and the hyperedges with low fitness values are substituted by new hyperedges. The method was tested on three data sets from UCI machine learning repository. The results show that the evolutionary hypernetworks show classification accuracies comparable to those of other conventional algorithms, find appropriate orders of hyperedges automatically, and extract important rules in the hyperedges for the given pattern classification problems.", "paperid": 2154184095, "normalizedname_level1": "artificial intelligence"}
{"index": 166, "text": "Class imbalance problem refers to unequal distribution of data instances between classes. Due to this, popular classifiers misclassify data instances of minority class into majority class. Initially, Extreme learning machine was proposed with the prime objective of handling real valued datasets. Though, it a fast learning technique, it suffers from the drawback of misclassification of imbalanced dataset which leads to the class imbalance problem. So, a new variant of ELM called Weighted Extreme Learning Machine was developed. This technique aimed at handling imbalance data by assigning more weight to minority class and less weight to majority class. The limitation of this technique lied in that it generates weight according to class distribution of training data, thereby, creating dependency on input data. This leads to the lack of finding optimal weight at which good generalization performance could be achieved. This work uses Genetic Algorithm to find optimal weight which is given to minority and majority class instances.", "paperid": 2050573901, "normalizedname_level1": "artificial intelligence"}
{"index": 167, "text": "The usual approach to named-entity detection is to learn extraction rules that rely on linguistic, syntactic, or document format patterns that are consistent across a set of documents. However, when there is no consistency among documents, it may be more effective to learn document-specific extraction rules.This paper presents a knowledge-based approach to learning rules for named-entity extraction. Document-specific extraction rules are created using a generate-and-test paradigm and a database of known named-entities. Experimental results show that this approach is effective on Web documents that are difficult for the usual methods.", "paperid": 1968067123, "normalizedname_level1": "artificial intelligence"}
{"index": 168, "text": "This paper contributes to automatic classification and localization of human actions in video. Whereas motion is the key ingredient in modern approaches, we assess the benefits of having objects in the video representation. Rather than considering a handful of carefully selected and localized objects, we conduct an empirical study on the benefit of encoding 15,000 object categories for action using 6 datasets totaling more than 200 hours of video and covering 180 action classes. Our key contributions are i) the first in-depth study of encoding objects for actions, ii) we show that objects matter for actions, and are often semantically relevant as well. iii) We establish that actions have object preferences. Rather than using all objects, selection is advantageous for action recognition. iv)We reveal that object-action relations are generic, which allows to transferring these relationships from the one domain to the other. And, v) objects, when combined with motion, improve the state-of-the-art for both action classification and localization.", "paperid": 2095242101, "normalizedname_level1": "artificial intelligence"}
{"index": 169, "text": "The spectral power of 5 frequently considered frequency bands (Alpha, Beta, Gamma, Theta and Delta) for 6 EEG channels is computed and then all the possible pairwise combinations among the 30 features set, are used to create a 435 dimensional feature space. Two new feature selection methods are introduced to choose the best candidate features among those and to reduce the dimensionality of this feature space. The selected features are then fed to Support Vector Machines (SVMs) that classify the cerebral state in preictal and non-preictal classes. The outputs of the SVM are regularized using a method that accounts for the classification dynamics of the preictal class, also known as “Firing Power” method. The results obtained using our feature selection approaches are compared with the ones obtained using minimum Redundancy Maximum Relevance (mRMR) feature selection method. The results in a group of 12 patients of the EPILEPSIAE database, containing 46 seizures and 787 hours multichannel recording for out-of-sample data, indicate the efficiency of the bivariate approach as well as the two new feature selection methods. The best results presented sensitivity of 76.09% (35 of 46 seizures predicted) and a false prediction rate of 0.15−1.", "paperid": 1989742411, "normalizedname_level1": "artificial intelligence"}
{"index": 170, "text": "In this work, a novel methodology for robots executing informed object search is proposed. The methodology is based mainly on a Bayesian framework that uses convolutions between observation likelihoods and spatial relation masks for estimating the probability map of the object being search for. By using spatial relation masks, complex spatial relations between objects can be defined as weighted sums of basic spatial relations using co-occurrence matrices as weights. The methodology is validated in an office environment in which four object classes (“monitor,” “keyboard,” “system unit,” and “router”) and four basic spatial relations (“very near,” “near,” “far,” and “very far”) are considered. Experiments combine statistics about object's coocurrence and about object detection in real environments, and search trials using a realistic simulation tool in which extensive tests comparing six object search algorithms are carried out. Results show that the use of the proposed methodology increases the object detection rate in a search task from 27.5% up to 53.2%.", "paperid": 2026619527, "normalizedname_level1": "artificial intelligence"}
{"index": 171, "text": "Fuzzy system modeling (FSM) is one of the most prominent tools in order to capture the hidden behavior of highly nonlinear systems with uncertainty. In this paper, a new type 2 FSM approach is proposed in order to increase the predictive power of traditional Takagi-Sugeno fuzzy system models. One of the biggest problems of type 2 fuzzy system models is computational complexity. In order to remedy this problem, the proposed inference mechanism performs type reduction as a first step. Then, the type 1 inference mechanisms are utilized to deduce a model output for a given crisp observation.", "paperid": 2100723763, "normalizedname_level1": "artificial intelligence"}
{"index": 172, "text": "Recently, it is expected that a robot plays an active part in living space where a person lives. When the robot moves in dynamic environments, the robot has to achieve the path planning that considers human movements so as to care about the person. In this research, the human's walking route is observed to generate the route for the robot automatically, assuming that sensors are distributed in the environmental side. Then, we extend “Human Frequency Map (HFM)” to handle the recognition of a door. The extended HFM can reflect the human movements to the path planning by considering the human motion trend more than the existing HFM.", "paperid": 2092064914, "normalizedname_level1": "artificial intelligence"}
{"index": 173, "text": "It was a great privilege to be given the opportunity to be the Guest Editor of this Special Issue of papers arising from the 2008 Workshop on 3D Face Processing, held in conjunction with the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) which took place in Anchorage, Alaska.", "paperid": 2013177265, "normalizedname_level1": "artificial intelligence"}
{"index": 174, "text": "Liver diseases and liver cancer has been the top ten causes of death among ten leading causes of death in Taiwan and always in the top. Usually, in the general liver function examination, biochemical test is the most common and a test that show the most direct reflection of the body condition. The health subject can determine the further inspection according to the inspection results. Nevertheless, the result of biochemical tests is not absolute, nor is the results represent in the normal range will not be suffering from liver disease. Therefore, how to let the health subject understand his/her own health condition is the main purpose of this paper. So, this paper makes use of grey relational analysis method and according to the standard values of hospital test results to be divided into Level method and compare with the health subject's test results. It will convert the past ordinal decision into cardinal quantitative evaluation value and evaluate liver function's level. At the same time, it develops MATLAB GUI toolbox to assist the numerical calculation, which can complete a medical assistance platform for liver function evaluation. It not only provides a further step of making decision for doctors in order to promote medical quality, but also provides a new research direction of this field.", "paperid": 2166565834, "normalizedname_level1": "artificial intelligence"}
{"index": 175, "text": "The aim of this study is to propose a complete instrumentation and signal processing method able to detect the presence of a person seated on the rear bench of a vehicle. The sensor is based on a piezoelectric film (EMFI sensor), designed to detect mechanical vibrations. In order to avoid confusion between humans and heavy objects or empty seats, the authors focused on the extraction of a biological signature from the acquired signals. This physiological pattern was extracted using an original wavelet denoising algorithm and was used further as a matched filter, in order to detect human presence in the vibration signals. Physiologically significant features were extracted from the output of the (on-line) filtering process and fed further-on into a classical Bayes-based classifier. After training, the proposed method yielded very promising results, the output of the classifier being almost error-free for different acquisition conditions (stopped and on-road vehicle, new and artificially aged sensor).", "paperid": 2045549769, "normalizedname_level1": "artificial intelligence"}
{"index": 176, "text": "We present a novel plenoptic sampling scheme that permits an efficient representation of the full light ray field in a space limited by a convex closed surface. We show that a convenient way to sample the light ray field around an observer consists in using a discrete set of perspective imagers with overlapping field-of-views that are distributed on a closed convex surface and looking along the normal to the surface. Taking inspiration from the vision system of flying insects, we choose to constrain the cameras on a sphere of finite radius. Building on spectral analysis we propose a sampling scheme that permits to reconstruct the spherical light field without aliasing. We validate our framework through experiments in a synthetic environment and we show that our constructive sampling scheme permits to effectively reconstruct the light field without artifacts.", "paperid": 2003746761, "normalizedname_level1": "artificial intelligence"}
{"index": 177, "text": "We present a computer vision technique to detect when the user brings their thumb and forefinger together (a pinch gesture) for close-range and relatively controlled viewing circumstances. The technique avoids complex and fragile hand tracking algorithms by detecting the hole formed when the thumb and forefinger are touching; this hole is found by simple analysis of the connected components of the background segmented against the hand. Our Thumb and Fore-Finger Interface (TAFFI) demonstrates the technique for cursor control as well as map navigation using one and two-handed interactions.", "paperid": 2134418753, "normalizedname_level1": "artificial intelligence"}
{"index": 178, "text": "In recent years, rate control plays an increasing important role in real-time video communication applications using MPEG-4 AVC/H.264. An important step in many existing rate control algorithms, which employs the quadratic rate-distortion (R-D) model, is to determine the target bits for each P frame. This paper aims in improving video distortion, due to high motions or scene changes, by more accurately predicting frame complexity using the statistics of previously encoded frames. We use mean absolute difference (MAD) ratio as a measure for global frame encoding complexity. Bit budget is allocated to frames according to their MAD ratio, combined with the bits computed based on their buffer status. Simulation results show that the H.264 coder, using our proposed algorithm with virtually little computational complexity added, effectively alleviates PSNR surges and sharp drops for frames caused by high motions or scene changes.", "paperid": 2149698389, "normalizedname_level1": "artificial intelligence"}
{"index": 179, "text": "Synthetic aperture radar (SAR) systems have been successful in applications of monitoring stationary terrain. Recently, bistatic/ multistatic SAR systems with the ability to obtain information from multiple directions attract intensive attentions in the research field of SAR. Considering that the images registration method is the fundamental of making full use of multi-angle information, this paper propose a novel PS points' association method based on sliding-type scattering centers for bistatic/ multistatic SAR images in order to realize the images registration. This method has solved the problem that traditional images registration method by PS points only can be used with single angle observation and is of great significance for multi-angle information fusion and multi-baseline interferometric by bistatic/ multistatic SAR systems.", "paperid": 2319449762, "normalizedname_level1": "artificial intelligence"}
{"index": 180, "text": "In this paper, a novel approach is proposed for perceptual grouping and localization of ill-defined curvilinear structures. Our approach builds upon the tensor voting and the iterative voting frameworks. Its efficacy lies on iterative refinements of curvilinear structures by gradually shifting from an exploratory to an exploitative mode. Such a mode shifting is achieved by reducing the aperture of the tensor voting fields, which is shown to improve curve grouping and inference by enhancing the concentration of the votes over promising, salient structures. The proposed technique is validated on delineating adherens junctions that are imaged through fluorescence microscopy. However, the method is also applicable for screening other organisms based on characteristics of their cell wall structures. Adherens junctions maintain tissue structural integrity and cell-cell interactions. Visually, they exhibit fibrous patterns that may be diffused, heterogeneous in fluorescence intensity, or punctate and frequently perceptual. Besides the application to real data, the proposed method is compared to prior methods on synthetic and annotated real data, showing high precision rates.", "paperid": 2132797060, "normalizedname_level1": "artificial intelligence"}
{"index": 181, "text": "For successful deployment, personal robots must adapt to ever-changing indoor environments. While dealing with novel objects is a largely unsolved challenge in AI, it is easy for people. In this paper we present a framework for robot supervision through Amazon Mechanical Turk. Unlike traditional models of teleoperation, people provide semantic information about the world and subjective judgements. The robot then autonomously utilizes the additional information to enhance its capabilities. The information can be collected on demand in large volumes and at low cost. We demonstrate our approach on the task of grasping unknown objects.", "paperid": 2051441824, "normalizedname_level1": "artificial intelligence"}
{"index": 182, "text": "Choosing appropriate values for kernel parameters is one of the key problems in many kernel-based methods. Beside common used cross validation method which is time-consuming, another kind of rapid methods using kernel matrix evaluation criteria such as Kernel Target Alignment(KTA) and Feature space-based kernel matrix evaluation measurement(FSM) criteria were proposed by researchers. However, we find KTA and FSM maybe failing in learning Gaussian kernel parameter in the case of small sampling size and tend to obtain an overfit solution. In this paper, a novel approach is proposed to learn Gaussian the kernel parameter which works in reproducing kernel mapping space and can avoid above problem. Experiments on real-world datasets show that the proposed approach using the two proposed criteria in this paper works well on learning Gaussian kernel parameter.", "paperid": 2102872135, "normalizedname_level1": "artificial intelligence"}
{"index": 183, "text": "A clustering method that combines an advanced statistical distribution with spatial contextual information is proposed for multilook polarimetric synthetic aperture radar (PolSAR) data. It is based on a Markov random field (MRF) model that integrates a K-Wishart distribution for the PolSAR data statistics conditioned to each image cluster and a Potts model for the spatial context. Specifically, the proposed algorithm is constructed based upon the expectation maximization (EM) algorithm. A new formulation of EM is developed to jointly address parameter estimation in the K-Wishart distribution and the spatial context model, and also minimization of the energy function. Experiments are presented with simulated and real quad-pol L-band data.", "paperid": 2162727965, "normalizedname_level1": "artificial intelligence"}
{"index": 184, "text": "This paper proposes a human tracking method with a wearable acceleration sensor and networked laser range scanners in an intelligent space. Human walking activities are measured and associated between laser range scanners and acceleration sensor. In this paper, person identification rules between two types of sensors are improved. Especially, similarity calculation using difference of behaviors and optimal parameters is introduced. Optimal parameters are derived from many tracking results obtained with actual system. Some experimental results with optimal parameters show higher performance of target recognition than cases with the conventional parameters.", "paperid": 2145872490, "normalizedname_level1": "artificial intelligence"}
{"index": 185, "text": "This paper presents a new particle swarm optimization method through the concept of complex networks for nonlinear programming problems. The purpose of this paper is to clarify the impact of the network structure between individuals on the solution performance. Realizing diversification and intensification of search processes, a new solution method using the ideas of complex networks is introduced. Illustrative numerical examples are provided to demonstrate the feasibility and efficiency of the proposed method.", "paperid": 1993455942, "normalizedname_level1": "artificial intelligence"}
{"index": 186, "text": "This paper presents a real-time neural network based color classifier for machine vision applications. Classification is made by a self-organizing map and to achieve realtime performance, it is built a lookup table with all possible input colors and their corresponding classes. To minimize the size of the lookup table and to reduce the levels of input image quantization, it is applied an uniform quantization, an operation that reduces the total combination of possible colors and decreases the number of positions in the lookup table. By using the lookup table, our color classifier uses a single memory access strategy to mask a complex color classification operation saving processing time. Results of experiments performed are presented to show the performance of the proposed approach.", "paperid": 2545356929, "normalizedname_level1": "artificial intelligence"}
{"index": 187, "text": "Two main challenges lie in tracking the partially occluded targets in a high-similarity background: 1) similar intensities increase the difficulty of discriminating targets from the background, and 2) occlusion (illumination and shape) decreases the relativity of targets to templates. In this paper, a novel eigenspace-based hybrid particle filter tracking method combined with online non-local appearance model is proposed to track the objects under highly similar environment with occlusions. By on-line training of the templates through non-local methods to generate the active appearance model, it is more likely find the maximum-likelihood distribution correctly. The projective transformation is utilized to cover all of the possible motion factors between the templates. The extended and unscented Kalman filters are switched to update the particles according to the linearity of the motion parameters. The experiment results show the effectiveness of our algorithm while dealing with occluded target in a high-similarity background.", "paperid": 2112555547, "normalizedname_level1": "artificial intelligence"}
{"index": 188, "text": "Automatic surveillance video footage indexing is much more desirable while providing an assistive tool for personnel security. Since the most relevant object that attracts our attention in surveillance videos is human face, we focus in this paper on building a system for indexing surveillance videos based on human face features. The proposed system has three main stages: Video Surveillance Summarisation, Face Detection, and Facial Feature Descriptors, and Indexing. A keyframe selection technique based on local foreground entropy is used for video surveillance summarisation. In the Face Detection stage, a skin color based method using measurements derived from the color-space components of the keyframe is used to locate eye, mouth and face boundary. Subsequently, SURF algorithm is applied to extract the feature descriptors of interest point from the detected face region. These descriptors are then indexed using vocabulary tree. The integration of the above-mentioned methods that are all good in their results, have made our overall system robust and efficient. Therefore, good results have been obtained while testing in ChokePoint public dataset contains 48 video sequences with a total of 179 349 frames including 64 204 face images.", "paperid": 2544677896, "normalizedname_level1": "artificial intelligence"}
{"index": 189, "text": "This work presents a case study in land cover classification using ms-NN, an extension of k-NN classification algorithm. The case study focuses on an area in the Brazilian Amazon region, with data obtained from LANDSAT5 satellite Thematic Mapper (TM) sensor and Advanced Land Observing System satellite (ALOS) Phase Array L-Band Synthetic Aperture Radar (PALSAR), using Fine Beam Dual. The results obtained with ms-NN are compared with k-NN and Support Vector Machine algorithms, considering the use of a single training set, a Monte Carlo procedure for testing and an extensive number of parameterizations for the classification methods. Considering only the best results for each classifier, ms-NN obtained better results than the other methods.", "paperid": 2044918921, "normalizedname_level1": "artificial intelligence"}
{"index": 190, "text": "This paper presents an algorithm that efficiently retrieves audio data similar to an audio query. The proposed method utilises a feature extraction method for acoustical music sequences. The extracted features are grouped by Minimum Bounding Rectangles (MBRs) and indexed by means of a spatial access method. We also present a novel false alarm resolution method that utilises a reverse order schema while calculating the distance of the query and results, in order to avoid costly operations. Performance evaluation results show that the proposed technique achieves considerable performance improvement in comparison to an existing method.", "paperid": 2138679406, "normalizedname_level1": "artificial intelligence"}
{"index": 191, "text": "High dynamic range (HDR) images can be generated by capturing a sequence of low dynamic range (LDR) images of the same scene with different exposures and then merging those images to create an HDR image. During capturing of LDR images, any changes in the scene or slightest camera movement results in ghost artifacts in the resultant HDR image. Over the past few years many algorithms have been proposed to produce ghost free HDR images of dynamic scenes. In this study we performed subjective psychophysical experiments to evaluate four algorithms for removing ghost artifacts in the final HDR image. To our best knowledge, no evaluation of deghosting algorithms for HDR imaging has been published. Thus, the aim of this paper is not only to evaluate different ghost removal algorithms but also to introduce a methodology to evaluate such algorithms and to present some of the challenges that exist in evaluating ghost removal algorithms in HDR images. Optical flow algorithms have been shown to produce successful results in aligning input images before merging them into an HDR image. As a result one of the state-of-the-art deghosting algorithm for HDR image alignment is based on optical flow. To test the limits of the evaluated deghosting algorithms the scenes used in our experiments were selected following the criteria proposed by Baker et al. [2011], which is considered as de facto standard for evaluating optical flow methodologies. The scenes used in the experiments serve to provide challenges that need to be dealt with by not only algorithms based on optical flow methodologies but also by other ghost removal algorithms for HDR imaging. The results reveal the scenes for which the evaluated algorithms fail and may serve as a guide for future research in this area.", "paperid": 2090522151, "normalizedname_level1": "artificial intelligence"}
{"index": 192, "text": "In this paper, an effective method for face photo-cartoon synthesis is used based on Markov Network model. The basic idea of this method is to synthesize a cartoon drawing from the database based on a cartoon drawn by an artist. This method consists of three steps. We first manually labeled eyes at fixed position to ensure the structure of the face and the cartoon face alignment roughly. Second, those faces and cartoons are divided into small overlapped patches, which can generate photo-cartoon pairs in the sample set. We build the joint photo-cartoon model based on the Markov Network model finally. Compared with the existing approaches, our method is simple to implement and more similar. We evaluate our approach on CUHK student database and demonstrate good performance.", "paperid": 2547827218, "normalizedname_level1": "artificial intelligence"}
{"index": 193, "text": "In this paper, a low complexity algorithm for GPS signal acquisition is proposed. It is shown that the samples acquired by a bank of much fewer correlators suffice to detect the GPS signals in contrast to that in existing GPS acquisition algorithms. Our acquisition scheme is based on the recently developed analog compressed sensing framework and employs compressive multichannel correlators as the matched filters, which allows to identify the strongest satellite signals in the field of view and pinpoint the correct code-phase and Doppler shifts for acquisition.", "paperid": 2170322935, "normalizedname_level1": "artificial intelligence"}
{"index": 194, "text": "The Gaussian mixture model (GMM) is a widely used probabilistic clustering model. The incremental learning algorithm of GMM is the basis of a variety of complex incremental learning algorithms. It is typically applied to real-time or massive data problems where the standard Expectation Maximum (EM) algorithm does not work. But the output of the incremental learning algorithm may exhibit degraded cluster quality than the standard EM algorithm. In order to achieve a high-quality and fast incremental GMM learning algorithm, we develop an algorithmic method for incremental learning of GMM in a GPU-CPU hybrid system. Our method uses model evolution history to approximate the model order and adopts both hypothesis-test and Euclidean distance to do mixture component equality test. Through experiments we show that our method achieves high performance in terms of both cluster quality and speed.", "paperid": 2072935211, "normalizedname_level1": "artificial intelligence"}
{"index": 195, "text": "Plantar pressure distribution and acceleration measurements are important to detect walking postures and states. This paper presents an in-shoe measurement and analysis system to do the detection. In-shoe information is acquired by the circuit board which is attached to the human's shank and transmitted to the remote receiver wirelessly. A real-time display and analysis software is developed. Experiments are taken on different terrains to analyze the walking states. The whole system presents a robust method to measure the walking phases and states.", "paperid": 2072723671, "normalizedname_level1": "artificial intelligence"}
{"index": 196, "text": "A universal query mechanism (UQM) for shape-based similarity retrieval in image/video database is proposed. In UQM, we investigate statistically common and salient features among multi-instance query samples to reflect user's definition on shape similarity. For two-dimensional binary shape images, Zernike moments (ZMs) adopted by MPEG-7 are computed as the normal shape descriptors. To get un-biased description by the ZMs, we proposed to locate the minimum bounding circle (MBC) for shape content, which accommodates major shape information in images inside while excludes erroneous noises outside. A fast algorithm to locate the MBC for shapes in image has been developed. The best functionality of the proposed UQM is that, excluding simplicity, new feature sets could be plugged directly into system to improve, while free of degradation in the retrieval performance. Simulations demonstrate that, the UQM-retrieved candidates from thirty thousand images in database really match subjective definitions on shape similarity as the complicated boosting query can do.", "paperid": 2073682157, "normalizedname_level1": "artificial intelligence"}
{"index": 197, "text": "Although entropy and relative entropy (K-L distance) are widely applied in many bioinformatics areas, there is no method given to compute the multispecies gene entropy yet. This paper presents the first multispecies gene entropy estimation method from the data mining point of view. In this study, a self-organizing map (SOM) is employed to mine a multispecies gene set to obtain the probability distribution of each gene in the feature space, which is the approximation of its corresponding probability distribution in the original sequence space. The multispecies gene entropy is computed by the probability distribution of a gene in the feature space. The phylogenetic applications of the multispecies gene entropy are investigated in an example of resolving incongruence between gene trees and species trees. It is found that genes with nearest K-L distances to the minimum entropy gene are more likely to be phylogenetically informative. A K-L distance based gene concatenation approach under gene clustering is proposed to resolve the gene tree and species tree problem. Under the same testing dataset, the K-L distance based approach not only avoids the ad-hoc mechanism of the original gene concatenation method but also is easy to extend to other dataset and free from prohibitive phylogenetic computing from large number of taxa.", "paperid": 2145882637, "normalizedname_level1": "artificial intelligence"}
{"index": 198, "text": "This paper describes a method for robust real-time pattern matching. We first introduce a family of image distance measures, the Image Hamming Distance Family. Members of this family are robust to occlusion, small geometrical transforms, light changes, and nonrigid deformations. We then present a novel Bayesian framework for sequential hypothesis testing on finite populations. Based on this framework, we design an optimal rejection/acceptance sampling algorithm. This algorithm quickly determines whether two images are similar with respect to a member of the Image Hamming Distance Family. We also present a fast framework that designs a near- optimal sampling algorithm. Extensive experimental results show that the sequential sampling algorithm's performance is excellent. Implemented on a Pentium IV 3 GHz processor, the detection of a pattern with 2,197 pixels in 640times480 pixel frames, where in each frame the pattern rotated and was highly occluded, proceeds at only 0.022 seconds per frame.", "paperid": 2137277421, "normalizedname_level1": "artificial intelligence"}
{"index": 199, "text": "Breast Cancer, the most common malignancy in women and the second leading cause of death for women all over the world. By earlier detection of cancer, the better treatment can be provided. SonoElastography, a new medical imaging technique reduces un necessary biopsies compared to mammography and conventional ultrasonography. The diagnosis and treatment of the cancer rely on segmentation of SonoElastographic images. Texture features are widely used in classification problems, i. e. mainly for diagnostic purposes where the Region Of Interest (ROI) is delineated manual ly. It has not yet been considered for SonoElastographic segmenta tion. SonoElastographic images of 15 patients taken using Siemens Acuson Antares are considered for experimentation. The images contain both benign and malignant tumors. From the experimental procedure it is proposed that the combination of texture features, Local Binary Pattern (LBP), Contrast and Variance are best suited for segmentation of SonoElastographic breast images. The images are first enhanced using sticks filter to remove noise, to improve contrast, and emphasize tumor boundary. Then extract the features to segment the breast image. The resultant images undergo some post-processing steps to remove the spurious spots. The segmented image is thinned to mark the tumor boundary. The results are then quantified with the help of an expert radiologist. The proposed work can be used for further diagnostic process, to decide if the segmented tumor is benign or malignant.", "paperid": 2061436668, "normalizedname_level1": "artificial intelligence"}
{"index": 200, "text": "Recently, multi-objective evolutionary algorithms have been also applied to improve the difficult tradeoff between interpretability and accuracy of fuzzy rule-based systems. It is know that both requirements are usually contradictory, however, a multi-objective genetic algorithm can obtain a set of solutions with different degrees of trade-off. This contribution presents a multi-objective evolutionary algorithm to obtain linguistic models with improved accuracy and the least number of possible rules. In order to minimize the number of rules and the system error, this model performs a rule selection and a tuning of the membership functions of an initial set of candidate linguistic fuzzy rules.", "paperid": 2110669667, "normalizedname_level1": "artificial intelligence"}
{"index": 201, "text": "A lot of plumbings such as gas pipes and water pipes exist in public utilities, factories, power plants and so on. It is difficult for humans to inspect them directly because they are long and narrow. Therefore, automated inspection by robots equipped with camera is desirable, and great efforts have been done to solve this problem. However, many of existing inspection robots have to rotate the camera to record images in piping because a conventional camera with a narrow view can see only one direction while piping has a cylindrical geometry. The use of an omni-directional camera that can take images of 360° in surroundings at a time is effective for the solution of the problem. However, the shape measurement is difficult only with the omni-directional camera. Then, in this paper, we propose a reconstruction method of piping shape by using an omni-directional camera and an omni-directional laser with a light section method and a structure from motion analysis. The validity of the proposed method is shown through experiments.", "paperid": 2091484553, "normalizedname_level1": "artificial intelligence"}
{"index": 202, "text": "Ant Colony Optimization (ACO) is a well known and rapidly evolving meta-heuristic technique. A large number of optimization problems have already taken advantage of the ACO technique while countless others are on their way. A copious amount of effort has also been put in by the researchers for applying ACO in solving various software testing problems. This paper presents a survey of twenty-one such studies, identified as relating to the use of ACO in diverse software testing concepts. To the best of our knowledge, no literature survey could be found published in the same context till date. Consequently, the twenty one studies have been rigorously analyzed to find some common parameters which can be grouped together or compared in order to provide a useful insight into the field.", "paperid": 2034899622, "normalizedname_level1": "artificial intelligence"}
{"index": 203, "text": "Cities are faced with many problems such as urban sprawl, congestion, and segregation. They are constantly changing. The heterogeneous nature of cities, make it difficult to generalize localized problems from that of city-wide problems. Computer modeling is becoming an increasingly important tool when examining how cities operate. Agent-based models allow for testing of different hypotheses and theories for urban changes, thus leading to a greater understanding of how cities work. This paper propose a multi-agents systems/simulation model  based on integration among parallel processing, coupling with vector GIS,temporal logic analysis and the integration with the rule engine. The efficiency of our approach is shown through some experimental results, which highlights how different theories can be incorporated into one model and demonstrates how the well-known principle that local action can give rise to global pattern but also how such pattern emerges as the consequence of positive feedback and learned behavior.", "paperid": 1982450299, "normalizedname_level1": "artificial intelligence"}
{"index": 204, "text": "The paper deals with a alternative tool for symbolic regression - Analytic Programming which is able to solve various problems from the symbolic regression domain. In this contribution main principles of Analytic Programming are described and explained. Then follows how Analytic Programming was used for setting an optimal trajectory for an artificial ant according to Koza. An ability to create so called programs, as well as Genetic Programming or Grammatical Evolution do, is shown in that part. Analytic Programming is a superstructure of evolutionary algorithms which are necessary to run Analytic Programming. In this contribution SelfOrganizing Migrating Algorithm and Differential Evolution as two evolutionary algorithms were used to carry simulations out.", "paperid": 2081893876, "normalizedname_level1": "artificial intelligence"}
{"index": 205, "text": "This paper discusses the implementation of open doorway detection in an indoor environment by utilizing the low cost narrow band radar system, i.e. USRP (Universal Software Radio Peripheral) based test bed. OFDM waveform is considered due to its flexibility of subcarrier occupation and ability of reusing the same waveforms as communication signals. We propose power comparison detection method and ranging detection method with the comparison between these two methods. The results show that our proposed method can work well under indoor fading environment. This scenario allows for the development of an autonomous detection robotics in an indoor unknown environment.", "paperid": 2102227312, "normalizedname_level1": "artificial intelligence"}
{"index": 206, "text": "Among the approaches for solving the semantic image segmentation problem that has proven successful is in formulating an energy minimization expressed on top of a conditional random field (CRF) over image pixels. Recently, high order potentials (cliques of size greater than 2) over superpixels have been incorporated in the CRF energy function yielding promising results. These potentials encourage pixels within the same superpixel to take the same label by penalizing inconsistent labeling within the superpixel. While some of the earlier attempts modeled higher order potentials without considering the conditional dependencies between superpixels, others modeled these dependencies at the cost of oversimplified models at higher levels. In this paper, we propose incorporating superpixel neighborhood information within the high order potential, hence modeling dependencies between superpixels without the need of oversimplifying or constraining the model. Results show that the proposed method achieves state-of-the-art results on the challenging PASCAL VOC 2007 dataset.", "paperid": 1991503434, "normalizedname_level1": "artificial intelligence"}
{"index": 207, "text": "Iris became an important biometric in the last decade, due to its uniqueness and richness of features. In this paper, a novel super-resolution and image registration technique for visual (non-infra-red) iris images is presented. In the proposed technique, a full face, 3 second long, 90 frames, visual video is captured with a digital camera located 3 feet away from each subject. Iris images are segmented from the full face image. A cross correlation model is applied for the registration/ alignment of full gray scale iris images. A high resolution iris image, that is 4 times higher in terms of size and resolution, is constructed from every 9 low resolution images. This process of building a high resolution image is based on an auto_regressive signature model between consecutive low resolution images in filling the sub pixels in the constructed high resolution image. Then this process is iterated until a 16 times higher resolution iris image is constructed. Illustrative images are shown that prove the effectiveness of the proposed technique.", "paperid": 2104856882, "normalizedname_level1": "artificial intelligence"}
{"index": 208, "text": "The relative importance of variables is different when comprehensively evaluating samples from multidimensional systems; however, traditional Mahalanobis-Taguchi System (MTS) method ignores this difference and treats all variables equally. To improve the accuracy of diagnosis or prediction when using MTS to measure the degree of abnormality of multidimensional observations, this paper makes an improvement on Mahalanobis distance function. Firstly, the necessity and feasibility of weighted Mahalanobis distance are analyzed. Secondly, the weighted Mahalanobis distance function in MTS is proposed. Finally, based on the new method a blood viscosity diagnosis system is optimized and analyzed and the result proves the effectiveness of this method.", "paperid": 2011138284, "normalizedname_level1": "artificial intelligence"}
{"index": 209, "text": "The effect of using downsampling for arbitrary views inside a multi-view sequence on the multi-view coding (MVC) efficiency is explored. A bit rate adaptive approach is proposed to consider downsampling certain views prior to encoding with relevant downscaling ratios. The inter-view references, if any, are downsampled to the same resolution and the decoded view is upsampled back to the original resolution. The results over several multi-view test sequences imply that up to 0.9 dB gain or 20% reduction in bit rate can be achieved, reducing the computational complexity in the encoder significantly at the same time.", "paperid": 1982846206, "normalizedname_level1": "artificial intelligence"}
{"index": 210, "text": "Various methods of hand gesture recognition have been proposed in the literature, with high recognition rate. But implementing these methods in embedded system is still challenging since image processing applications needs a high-performance processor. In this paper, a hand gesture recognition system is implemented on a system with an OK6410B board. This board has a processor that runs at 532 MHz, which is relatively high for a small processor. The hand gesture recognition method proposed in this paper is based on the Neural Network Shape Fitting. In this paper we propose some modifications to this method. The modifications were pixel randomizing during the initialization step, addition of several neurons in the iterations, using lookup table for distance measurement and simplification of the finger detection. These modifications yielded a faster processing time (0.95s on the OK6410B) and a higher recognition rate (94.44% using still images as input and 84.53% using live input from a webcam).", "paperid": 2025490799, "normalizedname_level1": "artificial intelligence"}
{"index": 211, "text": "In this paper, a distributed and adaptive approach for resource discovery in peer-to-peer networks is presented. This approach is based on the mobile agent paradigm and the random walk technique with reinforcement learning to allow for dynamic and self-adaptive resource discovery. More precisely, this approach augments random walks with a reinforcement learning technique where mobile agents are backtracked over the walked path in the network. A metric recording an affinity value that incorporates knowledge from past and present searches is maintained between nodes. The affinity value is used during a search to influence the selection of the next hop. This approach is evaluated with the network simulator ns2.", "paperid": 2002287704, "normalizedname_level1": "artificial intelligence"}
{"index": 212, "text": "This paper proposes an algorithmic modification in the rate control test model 5 (TM5) of MPEG-2 video encoder. A drawback of the TM5 model is that it allocates the same number of bits to all macroblocks in a picture (April 2003) (M. Sipitca, August 1996). A more appropriate distribution of the available bits can be done based on the complexity of the macroblocks. For this, an algorithm to classify the macroblocks based on their complexity is presented. Next, using this classification, a bit allocation algorithm is developed. The adaptation process of the quantization parameter is modified accordingly. By this approach, the overall PSNR quality of the encoded video increases by about 0.2 to 0.3 dB as compared to TM5 model.", "paperid": 2115269226, "normalizedname_level1": "artificial intelligence"}
{"index": 213, "text": "The ecological sciences have benefited greatly from recent advances in wireless sensor technologies. These technologies allow researchers to deploy networks of automated sensors, which can monitor a landscape at very fine temporal and spatial scales. However, these networks are subject to harsh conditions, which lead to malfunctions in individual sensors and failures in network communications. The resulting data streams often exhibit incorrect data measurements and missing values. Identifying and correcting these is time-consuming and error-prone. We present a method for real-time automated data quality control (QC) that exploits the spatial and temporal correlations in the data to distinguish sensor failures from valid observations. The model adapts to each deployment site by learning a Bayesian network structure that captures spatial relationships between sensors, and it extends the structure to a dynamic Bayesian network to incorporate temporal correlations. This model is able to flag faulty observations and predict the true values of the missing or corrupt readings. The performance of the model is evaluated on data collected by the SensorScope Project. The results show that the spatiotemporal model demonstrates clear advantages over models that include only temporal or only spatial correlations, and that the model is capable of accurately imputing corrupted values.", "paperid": 1989650021, "normalizedname_level1": "artificial intelligence"}
{"index": 214, "text": "This paper compares the use of temporal difference learning (TDL) versus co-evolutionary learning (CEL) for acquiring position evaluation functions for the game of Othello. The paper provides important insights into the str-engths and weaknesses of each approach. The main findings are that for Othello, TDL learns much faster than CEL, but that properly tuned CEL can learn better playing strategies. For CEL, it is essential to use parent-child weighted averaging in order to achieve good performance. Using this method a high quality weighted piece counter was evolved, and was shown to significantly outperform a set of standard heuristic weights.", "paperid": 2072701327, "normalizedname_level1": "artificial intelligence"}
{"index": 215, "text": "Mini-problems are a kind of problems, which should be solved for incremental innovation for existing products. The major method to solve them is to find the contradictions, technical or physical, existed in a product, and to solve them. The current reality tree (CRT) and conflict resolution diagram (CRD) of theory of constraints (TOC) are good at finding contradictions. Theory of inventive problem solving (TRIZ) is applicable for solving them. It is reasonable to integrate TOC and TRIZ to form a process from finding them to solving them. It is core function to generate ideas for the solving contradictions for the fuzzy front end. A four-step process model for that function of FFE is developed.", "paperid": 2120593963, "normalizedname_level1": "artificial intelligence"}
{"index": 216, "text": "This paper presents an approach to address the problem of the correction of perspective distortion text image which every vertex has maximum or minimum vertical or horizontal coordinate in one fourth image. In order to recognize this kind of perspective distortion text image by OCR (Optical Character Recognition), we correct it into a hardly distorted regular text image based on a gradient method. Our approach is binarizing and denoising the distortion text image firstly. Then a gradient method is applied to obtain the four vertexes of the irregular convex quadrilateral composed from the perspective distortion text block. Thus, the mapping parameters of the correction model can be obtained through one-to-one relation between the irregular quadrilateral and the rectangle of the original text image. At last, the perspective distortion text image is corrected with the perspective transformation model. The experimental results show that our approach can correct the kind of perspective text image effectively.", "paperid": 1997552065, "normalizedname_level1": "artificial intelligence"}
{"index": 217, "text": "Summary form only given: In this work we consider the problem of object parsing, namely detecting an object and its components by composing them from image observations. We build to address the computational complexity of the inference problem. For this we exploit our hierarchical object representation to efficiently compute a coarse solution to the problem, which we then use to guide search at a finer level. Starting from our adaptation of the A* parsing algorithm to the problem of object parsing, we then propose a coarse-to-fine approach that is capable of detecting multiple objects simultaneously. We extend this work to automatically learn a hierarchical model for a category from a set of training images for which only the bounding box is available. Our approach consists in (a) automatically registering a set of training images and constructing an object template (b) recovering object contours (c) finding object parts based on contour affinities and (d) discriminatively learning a parsing cost function.", "paperid": 2147248617, "normalizedname_level1": "artificial intelligence"}
{"index": 218, "text": "We apply methods for selecting subsets of dimensions from high-dimensional score spaces, and subsets of data for training, using submodular function optimization. Submodular functions provide theoretical performance guarantees while simultaneously retaining extremely fast and scalable optimization via an accelerated greedy algorithm. We evaluate this approach on two applications: data subset selection for phone recognizer training, and semi-supervised learning for phone segment classification. Interestingly, the first application uses submodularity twice: first for score space sub-selection and then for data subset selection. Our approach is computationally efficient but still consistently outperforms a number of baseline methods.", "paperid": 2120173366, "normalizedname_level1": "artificial intelligence"}
{"index": 219, "text": "Using multiple source-receiver combinations in sonar tracking systems in multistatic active sonar scenarios has been drawing a lot of attention. Significant challenges in tracking targets in this scenario are the large number of contacts acquired by the receivers, the high level of false alarm clutters and the uncertainty of the track hypothesis. Conventional tracking approaches (such as the Kalman filter) require a predetermined kinematic model of the target. This paper presents a rapid and reliable tracking algorithm for underwater targets in the multistatic active sonar scenario that combines the use of Self Organizing Maps, Dynamic Programming and the Hough transform, leaving aside the need for a target kinematic model. Results on simulated data demonstrating the improvement that can be achieved with the new method are also presented.", "paperid": 2111653337, "normalizedname_level1": "artificial intelligence"}
{"index": 220, "text": "For the re-evolution of the mobile robot behavior in unknown environments, the mapping relation was constructed between input of sensors and output of actuators based on echo state network. An algorithm of adaptive behavior learning was presented based on echo state network for evolutionary robotics. The composite architecture with responsive behavior and behavior learning was adopted. The responsive behavior was drived by the samples composed with sensor information and decision. The weights of echo state network were optimized via (µ+λ)-evolution strategy. The new control rules were generated via evolutionary algorithms, and new samples were added to the database constantly. The high intelligent behaviors of robot were transmitted to responsive behaviors. The experimental results indicate that the proposed approach has a better adaptability.", "paperid": 1971374160, "normalizedname_level1": "artificial intelligence"}
{"index": 221, "text": "This paper considers content-based retrieval of a MP3-based (MPEG 1 layer III) digital music archive. In our approach, two kinds of value, scale-factor (SCF) and sub-band coefficient (SBC), in a MP3 frame are used. These two values are extracted from the MP3 decoder to compute the MP3 features for indexing the MP3 objects. Evaluations on a content-based MP3 retrieval system indicate that our approach can achieve a good performance.", "paperid": 2116816832, "normalizedname_level1": "artificial intelligence"}
{"index": 222, "text": "The accelerometers integrated in today’s phones can be used to estimate the distance travelled from the accelerations made while walking. The placement of the sensor on the body is important to take into consideration. In this paper, the accelerations recorded with a daily-used phone in the trouser pocket were processed on a mobile device to detect steps and estimate the distance travelled. The outcome of the distance estimates shows an error of 0.05 metres per one metre and can be improved through calibration. This distance was applied in the motion model of a particle filter, and fused with a map of the building. The results establish that the estimates of the algorithm are valuable when fusing with other technologies or environment information, to aid the estimation of the location.", "paperid": 2113188254, "normalizedname_level1": "artificial intelligence"}
{"index": 223, "text": "This paper proposes a 3D face fitting algorithm based on the 3D Morphable Face Model. This algorithm updates parameters at each stage in the fitting process. To simplify measurement of parameters, this algorithm updates the parameters in cylindrical coordinates. The parameters are updated using a cost function which is the difference between the input 3D face data and the fitted 3D face model. This proposed algorithm shows good results when shape, texture and extrinsic variations occur in the 3D domain. The average correlation of the fitted shape and texture parameters to the test values are 0.72 and 0.99, respectively. This 3D face fitting algorithm can be widely used for 3D face analysis and 3D face recognition.", "paperid": 2119384314, "normalizedname_level1": "artificial intelligence"}
{"index": 224, "text": "The design and implementation of a computer vision system called DNAScan for the automated analysis of DNA hybridization images is presented. The hybridization of a DNA clone with a radioactively tagged probe manifests itself as a spot on the hybridization membrane. A recursive segmentation procedure is designed and implemented to extract spot-like features in the hybridization images in the presence of a highly inhomogeneous background. Positive hybridization signals (hits) are extracted from the spot-like features using grouping and decomposition algorithms based on computational geometry. A mathematical model for the positive hybridization patterns and a pattern classifier based on shape-based moments are proposed and implemented to distinguish between the clone-probe hybridization signals.", "paperid": 2118987760, "normalizedname_level1": "artificial intelligence"}
{"index": 225, "text": "This paper proposes a classification approach that incorporates the statistical methods GMM and Support Vector Machines. The proposed GMM-SVM system is presented and experimentally evaluated on text independent speaker identification. Our results prove that the combination approach GMM-SVM is significantly superior than SVM approach. We report improvements of 85,37% amelioration in identification rate compared to the SVM identification rate.", "paperid": 2113392271, "normalizedname_level1": "artificial intelligence"}
{"index": 226, "text": "This paper reports a pipelined architecture that can support on-line compression/decompression of image data. Spatial and spectral redundancy of an image data file is detected and removed with a simple and elegant scheme that can be easily implemented on a pipelined hardware. The scheme provides the user with the ability of trading off the image quality with the compression ratio. The basic theory of byte error correcting code (ECC) is employed in this work to compress a pixel row with reference to its adjacent row. A simple scheme is developed to encode pixel rows of a color image.", "paperid": 2106961023, "normalizedname_level1": "artificial intelligence"}
{"index": 227, "text": "Non-negative matrix factorization (NMF) is an excellent tool for unsupervised parts-based learning, but proves to be ineffective when parts of a whole follow a specific pattern. Analyzing such local changes is particularly important when studying anatomical transformations. We propose a supervised method that incorporates a regression constraint into the NMF framework and learns maximally changing parts in the basis images, called Regression based NMF (RNMF). The algorithm is made robust against outliers by learning the distribution of the input manifold space, where the data resides. One of our main goals is to achieve good region localization. By incorporating a gradient smoothing and independence constraint into the factorized bases, contiguous local regions are captured. We apply our technique to a synthetic dataset and structural MRI brain images of subjects with varying ages. RNMF finds the localized regions which are expected to be highly changing over age to be manifested in its significant basis and it also achieves the best performance compared to other statistical regression and dimensionality reduction techniques.", "paperid": 2076177832, "normalizedname_level1": "artificial intelligence"}
{"index": 228, "text": "To alleviate the workload of labeling before estimating certain color distributions, integrative labeling is introduced, which merely needs to figure out whether a picture contains positive-class regions or not and then all pixels of the picture are treated as positive or negative class training samples. Integrative labeling, however, results in heavy mixture of training samples. Thus traditional generative density estimation methods can't be used directly in that they perform poorly with heavily polluted training samples. In this paper, by utilizing the prior knowledge of high separability between positive and negative class color distributions, a discriminative learning based GMM(DiscGMM) is proposed for integrative labeling. Besides generating the polluted positive-class samples with comparatively high probability, optimal parameters found by DiscGMM also enjoy a comparatively low probability of generating negative-class samples. The parameter learning problem is solved by a modified Expectation Maximization (EM) algorithm. In an integrative labeling experiment of skin detection, DiscGMM is testified to enjoy much better performance than generative density estimation methods and shows qualified results.", "paperid": 2061871179, "normalizedname_level1": "artificial intelligence"}
{"index": 229, "text": "This paper presents algorithms for estimating parameters that characterize weak levels of printer banding in complex images. Flat field test images are typically used as test patterns for banding evaluation; however, the images of this study contain complex image content to demonstrate the algorithm's robustness and extend the utility of these defect characterization methods. The test images are from color printers in the development phase and include multiple visible defects such as banding, grain, and streaking. The banding characterization includes an estimation of the fundamental frequency and average power extracted from local regions dominated by low frequency content where banding is likely to be most visible and offensive. Grain and mottle defects combined with other image content form a difficult noise environment from which the quasi-periodic banding characteristics must be extracted. The algorithm is based on the autocorrelation function and uses special averaging and a pre-whitening filter designed to minimize the influence of the interfering factors. Experimental results show that this method provides accurate banding frequency and power characterization even for multiple banding sequences that are present in the image test area. This new algorithm proves computationally efficient and more accurate than parameter estimates based on frequency domain analysis using the power spectrum. Experimental results show accurate banding characterizations for periods ranging between 0.93 and 10.5 mm over a range of banding-to-noise ratios from 5.5 to -6.5 dB.", "paperid": 2168442418, "normalizedname_level1": "artificial intelligence"}
{"index": 230, "text": "Vision is a very rich sensor with a proven critical role in the control of balance. However, it is widely underused for robotics postural control. This paper presents and compares two approaches, one model-based and one model-free, to ensure stability of the COMAN compliant humanoid robot standing on a moving platform. The model-based approach uses inverse kinematics, while the model-free one relies on a neural network as mapping between sensors and actuators. The sensory information is composed of proprioceptive cues (gyroscope) and visual cues, used separately or together. We present methods of using vision as sensory input without relying on a particular object or feature of the scene, but only on the optical flow. The performance of both approaches are compared systematically in a realistic robotics simulator, for different movements of the platform and using different sensory cues. We aim to see if vision can replace proprioceptive sensors or be fused with them to improve the performance of the stabilizing controller. While both model-based and model-free approaches successfully stabilize the robot, the model-free approach shows better overall performance. Preliminary results on the real COMAN robot are shown.", "paperid": 2119622558, "normalizedname_level1": "artificial intelligence"}
{"index": 231, "text": "This paper describes how the recently developed constrained least squares (CLS) filtering algorithm can be made iterative to improve the resolution gain (RG) of medical ultrasound images. We propose the use of the iterative CLS (ICLS) filter, by incorporating the recently proposed ultrasound tissue model, to account for the random fluctuations of the tissue signal within the received ultrasound radio frequency (RF) echo signal. The resulting improvement in RG is demonstrated by eight different abdomen ultrasound images where progressive improvements in both the axial and lateral directions can be observed.", "paperid": 1518336998, "normalizedname_level1": "artificial intelligence"}
{"index": 232, "text": "A speech recognition system based on HTK for Polish is presented. It was trained on 365 utterances, all spoken by 26 males. The features of Polish with respect to speech recognition are described. Some aspects of speech recognition differ in comparison to English. Errors in recognition were analysed in details in an attempt to find reasons and scenarios of wrong recognitions.", "paperid": 2127620337, "normalizedname_level1": "artificial intelligence"}
{"index": 233, "text": "Among the many practices of composers, instrumentalists and singers which clearly correspond with fuzzy logic our focus here is on tuning. Different criteria have been used to select the sounds that music uses. A set containing these sounds (musical notes) is called a tuning system. Several tuning systems coexist in a classical orchestra. The pitches of the notes are different and very precisely defined for each system; however the consequences of small deviations from these theoretical frequencies are not serious. Actually, the orchestra members are aware of the necessity of reaching a consensus and adjust their instruments to tune well. Because of this, many musicians feel that the mathematical arguments that justify tuning systems are impractical. Modeling the notes as fuzzy sets provides a flexible theoretical framework which helps to integrate tuning theory and practice. We illustrate our approach comparing some tuning systems and analyzing a excerpt by Bela Bartok.", "paperid": 2111311855, "normalizedname_level1": "artificial intelligence"}
{"index": 234, "text": "Dozens of image features have been proposed in recent decades, which could measure the similarity of images and promote the improvement of performance in image retrieval. Different features focus on different views of the image, where two image quite distant with one feature may close with another. In this paper, we attempt to integrate different measures together to improve the image retrieval accuracy. Main point of which is fusing several given measures into one and then propagating on the resulted data space. As different features are involved in the similarity measurement, it would help to retrieval more similar image for the query from the dataset. Experiment results on several image dataset show the benefit of the proposed algorithm.", "paperid": 2009811121, "normalizedname_level1": "artificial intelligence"}
{"index": 235, "text": "We present a new interaction paradigm for digital cameras aimed at making interactive imaging algorithms accessible on these devices. In our system, the user creates visual cues in front of the lens during the live preview frames that are continuously processed before the snapshot is taken. These cues are recognized by the camera's image processor to control the lens or other settings. We design and analyze vision-based camera interactions, including focus and zoom controls, and argue that the vision-based paradigm offers a new level of photographer control needed for the next generation of digital cameras.", "paperid": 2062094687, "normalizedname_level1": "artificial intelligence"}
{"index": 236, "text": "This paper looks at a parsing-based alternative to word error rate (WER) for optimizing recognition, SParseval, hypothesizing that it may be a better objective for applications such as translation. We find that SParseval is more correlated than WER with human measures of subsequent translation performance, but that optimizing explicitly for SParseval does not give a significant reduction in translation error as measured by automatic methods based on a single translation reference. However, anecdotal examples indicate that SParseval does improve automatic speech recognition (ASR) results, leaving open the possibility that it may be more useful in the future or for other language processing tasks.", "paperid": 2100475772, "normalizedname_level1": "artificial intelligence"}
{"index": 237, "text": "We present a method to extract salient regions in natural images based on local extrema. Behavioral and physiological studies on human vision indicate that the peripheral vision has an important role to get salient regions into the fovea to obtain detailed information from the environment. Modeling of the peripheral is the key to create a salient region extraction method with the behavioral plausibility. To extract salient regions in a natural image, we focus on the multiresolutional 2-D distribution of local extrema. Local extrema detect regions in the image that are either brighter or darker than the surroundings. They will provide useful information to extract salient regions regardless of the image resolution. As for the function of human vision, an image is more blurred in more peripheral. So we model the peripheral using scale-space image representation. In this study, we define the saliency based on the stability of local extrema on scale-space and create a method to extract salient regions. Comparing the human map of fixations, we ensure that our method successfully extracts salient regions.", "paperid": 2045934580, "normalizedname_level1": "artificial intelligence"}
{"index": 238, "text": "In sign language, hand positions and movements represent meaning of words. Hence, we have been developing sign language recognition methods using both of hand positions and movements. However, in the previous studies, each feature has same weight to calculate the probability for the recognition. In this study, we propose a sign language recognition method by using a multi-stream HMM technique to show the importance of position and movement information for the sign language recognition. We conducted recognition experiments using 21,960 sign language word data. As a result, 75. 6% recognition accuracy was obtained with the appropriate weight (position:movement=0. 2:0. 8), while 70. 6% was obtained with the same weight. From the result, we can conclude that the hand movement is more important for the sign language recognition than the hand position. In addition, we conducted experiments to discuss the optimal number of the states and mixtures and the best accuracy was obtained by the 15 states and two mixtures for each word HMM.", "paperid": 2116129837, "normalizedname_level1": "artificial intelligence"}
{"index": 239, "text": "This work describes a novel form of robotic therapy for the upper extremity in chronic stroke. Based on previous results, we hypothesized that a training task that encourages subjects to consciously guide endpoint forces generated by the hemiparetic arm will result in significant gains in functional ability of the arm, superior to more conventional methods of therapy. In addition, since stroke survivors present with varying degrees of arm movement ability, we developed an adaptive algorithm that tailors the amount of assistance provided in completing the guided force training task. The algorithm adapts a coefficient for velocity-dependent assistance based on measured movement speed, on a trial-to-trial basis. The training algorithm has been implemented with a simple linear robotic device called the ARM Guide. One participant completed a two month training program with the adaptive algorithm, resulting in significant improvements in the performance of functional tasks.", "paperid": 2131509297, "normalizedname_level1": "artificial intelligence"}
{"index": 240, "text": "In order to facilitate the visually impaired person in navigation, we have developed a prototype guidance system. The main assumption of this guidance system is that there are many straight paths in different real world scenarios. These straight paths have parallel edges, which when captured as an image seem to converge to a single point called the vanishing point. Proper feature extraction and mathematical modelling of the captured frame leads to the detection of these parallel edges. The vanishing point is then calculated and a decision system is formed which notifies the blind person about his/her deviation from a straight path. The scope of this system is limited to a straight path and has been tested in different lighting conditions and with different level of occlusion. A laptop mounted on a 2D robotic platform is used to develop and verify the robustness of the algorithm. Finally, a smartphone based real-time application has been implemented for this visual guidance system, in which the decision system returns an audio output to guide the visually impaired person. This application has an average execution rate of 20 frames per second, with each frame being of 320 by 240 pixel size. The system has an accuracy of 84.1% in a scenario with pedestrians and other objects, while without pedestrians it produces an accuracy of over 90%.", "paperid": 1974736993, "normalizedname_level1": "artificial intelligence"}
{"index": 241, "text": "Intelligent Systems are able technical of incorporate knowledge and, therefore, are being employed in different areas, improving and innovating conventional methods. As an example, the presence of artificial intelligence in monitoring systems to identify faults in electric motors. The purpose of such systems is to prevent unscheduled maintenance or avoid significant losses in the production line. Therefore, this paper describes the performance of two topologies of neural networks for identification of short circuit in the stator windings and bearing failures. The input data to the neural networks are statistical parameters extracted from on power supplies induction motor. Thus, the intelligent system proposed in this paper proved to be efficient and able to be implemented in monitoring systems failures in induction motors.", "paperid": 2063177987, "normalizedname_level1": "artificial intelligence"}
{"index": 242, "text": "The paper mentions that a contributing factor to failures of machine translation and large-scale intelligent decision support systems is the unavailability of \"common sense\" about the world and about language and its contextual use.", "paperid": 2024419703, "normalizedname_level1": "artificial intelligence"}
{"index": 243, "text": "Under-complete models, which derive lower dimensional representations of input data, are valuable in domains in which the number of input dimensions is very large, such as data consisting of a temporal sequence of images. This paper presents the under-complete product of experts (UPoE), where each expert models a one-dimensional projection of the data. Maximum-likelihood learning rules for this model constitute a tractable and exact algorithm for learning under-complete independent components. The learning rules for this model coincide with approximate learning rules proposed earlier for under-complete independent component analysis (UICA) models. This paper also derives an efficient sequential learning algorithm from this model and discusses its relationship to sequential independent component analysis (ICA), projection pursuit density estimation, and feature induction algorithms for additive random field models. This paper demonstrates the efficacy of these novel algorithms on high-dimensional continuous datasets.", "paperid": 2010980582, "normalizedname_level1": "artificial intelligence"}
{"index": 244, "text": "This paper describes jMarkov, an object-oriented framework designed to facilitate the construction and analysis of large-scale Markov Chains. The object-oriented design allows a natural translation from a conceptual mathematical model to a computer representation. Additionally, the framework provides the user with the solvers to analyze both the steady-state and the transient behaviors, but its design is flexible enough so new solvers can be implemented. Finally, jMarkov provides the tools to model Quasi-Birth and Death Processes with the same philosophy used to construct general Markov Chains.", "paperid": 1999605332, "normalizedname_level1": "artificial intelligence"}
{"index": 245, "text": "This paper presents a new SLAM (simultaneous localization and mapping) method using genetic algorithm (GA) for mobile robots. A laser range finder (LRF) is installed on a mobile robot for collecting point-distance information about the surroundings. From the LRF points, several important ones are extracted for describing the main features of the surroundings. A new form of chromosomes for representing the changes of feature LRF points that are caused by the robot's movement is designed. The matching of current LRF features and the robot's possible movement is done by a fast genetic algorithm. A restart mechanism that re-initializes all chromosomes for increasing the diversity of solutions is developed and works with the matching process. Some constrains are developed for filtering out irrational chromosomes after the operation of crossover and mutation. With these mechanisms and constrains, our proposed method generates feasible solutions in several hundreds of GA iterations. Experiments are conducted on a real mobile robot. The experimental results show that our proposed method is efficient and effective for SLAM.", "paperid": 2092723008, "normalizedname_level1": "artificial intelligence"}
{"index": 246, "text": "In this paper, an automatic lesson generation system is presented which is suitable in a learning-by-mimicking scenario where the learning objects can be represented as multiattribute time series data. The dance is used as an example in this paper to illustrate the idea. Given a dance motion sequence as the input, the proposed lesson generation system automatically generates the lesson plan for students. It first extracts patterns from the input dance sequence to form the learning objects. The prerequisite structure is then built by considering the relations between the learning objects. Afterward the knowledge structure is constructed from the prerequisite structure based on the knowledge space theory. Finally, the learning path is derived according to an easy-to-complex manner while respecting the prerequisite relations. A user study that involved 40 students was conducted to evaluate the proposed work. The average learning time required for the treatment group (learning with the proposed system) was found to be lower than that of the control group (learning by free browsing) thus demonstrating the learning efficiency of the proposed system. The feedback from the questionnaires indicated that a majority of the subjects showed positive response toward the usefulness and rationality of our proposed system.", "paperid": 2042028059, "normalizedname_level1": "artificial intelligence"}
{"index": 247, "text": "Since CMAC (cerebellar model articulation controller), which mimics the recognition process of human cerebellum, is an artificial neural network (ANN) characterized by the fast learning or high convergence, it is easy to be programmed for online learning and real-time control. Diversified applications have exhibited the learning and solution abilities of CMAC; nevertheless, CMAC has an unstable learning performance in certain experimental cases presented by Chen and Chang (1996, 1995 and 1994). To overcome the above CMAC learning instability, a GA (genetic algorithm) is used as an alternative approach to train CMAC in this paper. GA is an optimization technique that mimics genetic changes in the life evolutionary process. It takes advantage of the multiple starting points for searching an optimal solution within a relatively short computational time. By comparing the standard CMAC and the GA-based CMAC (GACMAC), the usefulness of the GACMAC for overcoming the CMAC learning instability was verified by experimenting the Chan and Chang's time series control problem. More general cases are needed to study for the confirmation of the alternative CMAC training performance in the near future", "paperid": 1497053395, "normalizedname_level1": "artificial intelligence"}
{"index": 248, "text": "Pixel-level image fusion is an important part of image fusion algorithms which can combine spectral information of coarse resolution imagery with finer spatial resolution imagery. The objective of this paper is to present an overview of pixel-level image fusion algorithms used for effective information interpretation of remotely sensed imagery of various spatial and spectral characteristics. According to their different characteristics, these algorithms are categorized into four types, i.e., color space model algorithms, statistical/numerical algorithms, multiresolution decomposition algorithms and radiometric/spectral algorithms. They are investigated respectively and comparative analysis on the performance of these algorithms is conducted on a set of multispectral(MS) images and a Systeme Pour l'Observation de la Terre (SPOT) Panchromatic(PAN) image of the same scene. The effectiveness of these algorithms is evaluated quantitatively and qualitatively. Experimental results indicate that multiresolution decomposition based algorithms especially the discrete wavelet transform shows a comparative better performance on the test data than the other three types of fusion algorithms. However the suitable selection of a proper pixel-level fusion algorithm depends on the merits of each method, relevant applied situations and the characteristics of the source data.", "paperid": 2118491436, "normalizedname_level1": "artificial intelligence"}
{"index": 249, "text": "Detecting cars in high-resolution aerial images has attracted particular attention in recent years. However, scene complexity, large illumination change and occlusions make the task very challenging. In this paper, we propose a robust and effective framework for car detection from high-resolution aerial imagery. More specifically, we first incorporate multiple diverse and complementary image descriptors, Histogram of Oriented Gradients (HOG), Local Binary Pattern (LBP) and Opponent Histogram. Subsequently taking computational efficiency and runtime complexity into account, we adopt an interactive bootstrapping approach to collect hard negatives for training an intersection kernel support vector machine (IKSVM). After training, detection is performed by exhaustive search. Finally for post-processing, we employ a greedy procedure for eliminating repetitive detections via non-maximum suppression. Furthermore, contextual information is utilized to refine the detections. Experimental results on Vaihingen dataset have demonstrated that the proposed method can achieve state-of-the-art performance in various real scenes.", "paperid": 2064022862, "normalizedname_level1": "artificial intelligence"}
{"index": 250, "text": "Our course provides a study of philosophy, psychology and physiology of the emotions. Since ancient times emotions have been treated philosophically by Plato, Confucius, Descartes and Spinoza, then, in the 19th century Darwin and James study the most scientifically studied and it is in the light of modern science and technology invasive brain that our knowledge emotions fundamentally changing today thanks to researchers like A. Damasio and J. LeDoux. Armed with this basic knowledge, we will study the methods used by artists to induce emotions in the history of ancient and contemporary arts. Based on scientific studies, we will highlight the methods to be implemented to induce basic emotions such as fear or joy but also complex as empathy and compassion. We will develop a specific method of emotion induction to the problems of competing game play and video game script. After explaining the concepts of presence in virtual environments, and their sensory and psychological mechanisms we study how emotions play a role in the implementation of presence.", "paperid": 1979470765, "normalizedname_level1": "artificial intelligence"}
{"index": 251, "text": "In embedded video surveillance system, many moving target detection methods are used. But for some complex environmental, they have some problems in detection accuracy and speed of moving target detection. In this paper, a target detection method based on improved adaptive threshold hybrid difference is proposed, then for wiping off the noise, the method of morphological filtering is adopted to gain exact moving objects. Finally, according to the requirement, the area method is used to choose the final images to avoid false alarms. Experimental results show that the proposed method not only gets moving objects completely, but also adapts to the complex environmental change, at the same time, the method is rapid, efficient, and is applied to embedded system well.", "paperid": 2148434173, "normalizedname_level1": "artificial intelligence"}
{"index": 252, "text": "This paper describes a novel domain-shift tracking scheme that includes Bayesian formulation on the Grass-mann/Riemannian manifold for tracking, and domain-shift online object learning as well as occlusion handling on the manifold. Since out-of-plane object images do not lie in a single vector space, smoothing manifolds are more suitable tools for describing domain-shift nature of such dynamic object images. The proposed domain-shift scheme is designed for tracking large-size dynamic objects (i.e. camera is close to the object) in video that contain significant out-of-plane pose changes, and may be accompanied with long-term partial occlusions. The main features of such domain-shift tracker include: (a) Bayesian formulation defined on a manifold instead of vector space, performing posterior state estimation on the manifold based on nonlinear state space modeling; (b) Two particle filters defined on the manifold, one for online learning, another for tracking; (c) Occlusion handling is added to the online learning process to prevent learning occluding objects/clutter. To show the variant of domain-shift trackers, two example schemes are described: one uses instantaneous data on Riemannian manifolds, another uses a sliding-window of data on Grassmann manifolds. Tests on videos from the proposed domain-shift trackers have shown very robust tracking performance when large-size objects contain significant out-of-plane pose changes accompanied with long-term partial occlusions. Comparisons with three existing state-of-the-art methods provide further support to the proposed scheme.", "paperid": 1974550803, "normalizedname_level1": "artificial intelligence"}
{"index": 253, "text": "Computing the template, or the mean, of a set of spike trains is a novel and important task in neural coding. Due to the random nature of spike trains taken from experimental recordings, probabilistic and statistical methods have gained prominence in examining underlying firing patterns. However, these methods focus on modeling neural activity at each given time and therefore their results depend heavily on model assumptions. Taking a model-free and metric-based approach, we analyze the space of spike trains directly and reach algorithms for estimating statistical summaries, such as the mean spike train, of a given set. In our data-driven approach the mean is defined directly in a function space in which the spike trains are viewed as individual points. Here we develop an efficient and convergence-proven algorithm to compute the mean spike train in a general scenario. Experimental result from a neural recoding in primate motor cortex indicates that the estimated means successfully capture the typical patterns in spike trains. In addition, these mean spike trains provide an accurate and efficient performance in decoding motor behaviors.", "paperid": 2034767725, "normalizedname_level1": "artificial intelligence"}
{"index": 254, "text": "Our experience with applying model-based testing on industrial systems showed that the generated test suites are often too large and costly to execute given project deadlines and the limited resources for system testing on real platforms. In such industrial contexts, it is often the case that only a small subset of test cases can be run. In previous work, we proposed novel test case selection techniques that minimize the similarities among selected test cases and outperforms other selection alternatives. In this paper, our goal is to gain insights into why and under which conditions similarity-based selection techniques, and in particular our approach, can be expected to work. We investigate the properties of test suites with respect to similarities among fault revealing test cases. We thus identify the ideal situation in which a similarity-based selection works best, which is useful for devising more effective similarity functions. We also address the specific situation in which a test suite contains outliers, that is a small group of very different test cases, and show that it decreases the effectiveness of similarity-based selection. We then propose, and successfully evaluate based on two industrial systems, a solution based on rank scaling to alleviate this problem.", "paperid": 2157693531, "normalizedname_level1": "artificial intelligence"}
{"index": 255, "text": "This paper describes our efforts to apply various advanced supervised machine learning and natural language processing techniques, including Binomial Logistic Regression, Support Vector Machines, Neural Networks, Ensemble Techniques, and Latent Dirichlet Allocation (LDA), to the problem of detecting fraud in financial reporting documents available from the United States’ Security and Exchange Commission EDGAR database. Specifically, we apply LDA to a collection of type 10-K financial reports and to generate document-topic frequency matrix, and then submit these data to a series of advanced classification algorithms. We then apply evaluation metrics, such as Precision, Receiver Operating Characteristic Curve, and Area Under the Curve to evaluate the performance of each algorithm. We conclude that these methods show promise and suggest applying the approach to a larger set of input documents.", "paperid": 1514675060, "normalizedname_level1": "artificial intelligence"}
{"index": 256, "text": "Cognitive radio networks (CRNs) or self-organizing mobile cellular networks are a promising technology for 5G that manages the spectrum frequency domain more efficiently. At the heart of CRNs is the cognitive engine (CE), which is responsible for decision making on the optimal configuration settings for the CRN in real time if possible. In this paper a novel paradigm for decision making in the CE will be presented called hierarchical random neural networks (HRNNs). The proposed HRNN model decomposes a large complex neural network into a network of loosely interconnected localized subnets, which allow the simplified understanding of network behaviour and also allows the addition of more nodes for long-term memory (LTM). The model can also accurately capture the dynamic nature of the system. Simulation results of the proposed HRNN structure has shown improvements in learning efficiency (based on required execution time for convergent result) in the range of 33% to 35% with reduced computations.", "paperid": 2069615307, "normalizedname_level1": "artificial intelligence"}
{"index": 257, "text": "Robustness to variations in environmental conditions and camera viewpoint is essential for long-term place recognition, navigation and SLAM. Existing systems typically solve either of these problems, but invariance to both remains a challenge. This paper presents a training-free approach to lateral viewpoint- and condition-invariant, vision-based place recognition. Our successive frame patch-tracking technique infers average scene depth along traverses and automatically rescales views of the same place at different depths to increase their similarity. We combine our system with the condition-invariant SMART algorithm and demonstrate place recognition between day and night, across entire 4-lane-plus-median-strip roads, where current algorithms fail.", "paperid": 2242625550, "normalizedname_level1": "artificial intelligence"}
{"index": 258, "text": "This paper introduces a fusion method to merge the IKONOS low resolution multispectral image (MS) with the IKONOS high resolution panchromatic (PAN) image based on the Multiwavelet transform. Different fusion rules are used with the Multiwavelet method to improve the fusion quality based on pixel level fusion or feature level fusion. The best three old techniques for image fusion (IHS, Brovey, and Wavelet) are also tested in this paper. The performances of the existing and the proposed methods are calculated using Correlation Coefficient and Root Mean Square Error. Here we used our method to merge the panchromatic image of IKONOS sensor (1m resolution) with its multispectral image (4m resolution). Multiwavelet based image fusion method provides richer information in both spatial and spectral domains than the Wavelet based method, and the results clearly demonstrate the advantages of this approach.", "paperid": 1966079689, "normalizedname_level1": "artificial intelligence"}
{"index": 259, "text": "To avoid the restriction of neuron activation functions of neural learning algorithm and the disadvantage of getting into local optimum solution with general numerical computation method, a novel independent component analysis (ICA) based on improved quantum genetic algorithm (IQGA) is proposed in our paper. Moreover, Han's quantum genetic algorithm (QGA) is improved by adopting the quantum crossover and quantum mutation to overcome the premature convergence and increase the search capability in our work. The proposed algorithm is applied to hyperspectral anomaly detection. The effectiveness of the algorithm is evaluated by HYDICE hyperspectral images. It is demonstrated that the proposed algorithm has better detection effect and time efficiency than QGA", "paperid": 2165323846, "normalizedname_level1": "artificial intelligence"}
{"index": 260, "text": "This paper proposes a new corner detection method based on the Hessian matrix. The proposed method can detect features of a pattern or input image using eigenvalue and eigenvector of images. The Hessian matrix has information of ellipse with intensity variance, and corner can be detected by using the eigen-value and eigen-vector analysis and decided weight value. In order to evaluate the proposed algorithm, experiments are performed in many type images. As the result of the test image, it shows the better performance than that of conventional Harris, SUSAN, and symmetric corner detectors.", "paperid": 2056691983, "normalizedname_level1": "artificial intelligence"}
{"index": 261, "text": "I describe the Utrecht Machine (UM), a discrete artificial regulatory network designed for studying how evolution discovers biochemical computation mechanisms. The corresponding binary genome format is compatible with gene deletion, duplication, and recombination. In the simulation presented here, an agent consisting of two UMs, a sender and a receiver, must encode, transmit, and decode a binary word over time using the narrow communication channel between them. This communication problem has chicken-and-egg structure in that a sending mechanism is useless without a corresponding receiving mechanism. An in-depth case study reveals that a coincidence creates a minimal partial solution, from which a sequence of partial sending and receiving mechanisms evolve. Gene duplications contribute by enlarging the regulatory network. Analysis of 60,000 sample runs under a variety of parameter settings confirms that crossover accelerates evolution, that stronger selection tends to find clumsier solutions and finds them more slowly, and that there is implicit selection for robust mechanisms and genomes at the codon level. Typical solutions associate each input bit with an activation speed and combine them almost additively. The parents of breakthrough organisms sometimes have lower fitness scores than others in the population, indicating that populations can cross valleys in the fitness landscape via outlying members. The simulation exhibits back mutations and population-level memory effects not accounted for in traditional population genetics models. All together, these phenomena suggest that new evolutionary models are needed that incorporate regulatory network structure.", "paperid": 2072150974, "normalizedname_level1": "artificial intelligence"}
{"index": 262, "text": "A novel approach to construct fuzzy classification system based on fuzzy association rules is proposed in this paper. Competitive agglomeration algorithm is employed to partition quantitative attributes from each data record into several optimized fuzzy sets, resulting in an initial fuzzy classification system. A fuzzy classification system with high accuracy and interpretability can be further achieved by genetic strategies. Simulation applied to an existent diabetes dataset demonstrates the performance of the proposed approach is better than those of other popular classification methods.", "paperid": 2039118316, "normalizedname_level1": "artificial intelligence"}
{"index": 263, "text": "The detection and analysis of retinal vessels in ophthalmology is of great use in the diagnosis and progression monitoring of diabetic retinopathy. Automatic Detection of the vessel network has however been challenging due to noise from uneven contrast and illumination during the retinal image acquisition process. This paper presents a robust segmentation technique that combines phase congruence and Gray level co-occurrence matrix (GLCM) sum entropy information for the detection of vessel network. While compared with the previously used techniques on DRIVE database, the proposed technique yields high mean sensitivity and mean accuracy rates in the same range of very good specificity.", "paperid": 1571720115, "normalizedname_level1": "artificial intelligence"}
{"index": 264, "text": "In this paper, an image coding algorithm based on a rate-distortion optimized orthonormal finite ridge let transform (OFRIT) decomposition and on an improved listless block-partitioning coding scheme which quantizes each sub band separately is proposed. The ridge let transform can provide optimally sparse representation of objects with singularities along straight edges and the orthonormal finite ridge let transform(OFRIT) can decompose the high frequency parts of the images. A linear indexing technique is used to rep resent the coordinate of a coefficient with a single number instead of two for computational efficiency and algorithm simplicity. Instead of lists, a state table with four bits per coefficient keep s track of the significance of the set and pixel. Each sub band is encoded by a quad tree based set partitioning process. This algorithm needs no lists and thus can avoid unfixed memory requirement and the operations of list nodes. The experimental results show that the proposed algorithm runs faster than SPIHT and JPEG2000 and set partitioning in hierarchical trees. The proposed algorithm outperforms SPIHT and JPEG2000 schemes in novel image with straight lines significantly or curve lines significantly coding in terms of both PSNR and visual quality, it has a fixed predetermined memory requirement of about 50% of the image size.", "paperid": 2031793349, "normalizedname_level1": "artificial intelligence"}
{"index": 265, "text": "Metabolic P systems, also called MP systems, are discrete dynamical systems which proved to be effective for modeling biological systems. Their dynamics is generated by means of a metabolic algorithm based on \"flux regulation functions\". A significant problem related to the generation of MP models from experimental data concerns the synthesis of these functions. In this paper we introduce a new approach to the synthesis of MP fluxes relying on neural networks as universal function approximators, and on evolutionary algorithms as learning techniques. This methodology is successfully tested in the case study of mitotic oscillator in early amphibian embryos.", "paperid": 2022074957, "normalizedname_level1": "artificial intelligence"}
{"index": 266, "text": "Based on compressed sensing (CS) theory and its classical OMP reconstruction algorithm, as to the problems that large storage of measurement matrix is needed when sampling and calculating the whole signal which has a large amount of data and that a huge amount of time is consumed when reconstructing it, we can decompose the signal into many sub-signals through a certain way, and then measure and reconstruct the sub-signals. Finally, we can realize the processing of the whole segment signal. As above, the application of compressed sensing which based on signal decomposition can be called decomposed compressed sensing method in this paper. Currently, a commonly used decomposed compressed sensing method is segmented compressed sensing (SCS). For the problem that there are errors in processing certain types of sparse signals by SCS method, we propose a new decomposed compressed sensing in this paper. The simulation results show that, compared with SCS, the new method can improve the reconstruction efficiency to a certain extent. At the same time, this method is applicable to any type of signal, which has wider application range.", "paperid": 2544652799, "normalizedname_level1": "artificial intelligence"}
{"index": 267, "text": "In this paper, we have investigated the inductive inference of complex grammar of subset of Marathi (Indian) language and results are reported. We have investigated Elman recurrent networks (ERNs), Jordon recurrent networks (JRNs), time lagged recurrent networks (TLRNs) and recurrent neural networks (RNNs). In this empirical study, we consider the task of classifying Marathi language sentences as grammatical or ungrammatical as well as modeled the problem as a prediction problem. We have also analyzed the operation of the networks by investing rule approximation.", "paperid": 2125965633, "normalizedname_level1": "artificial intelligence"}
{"index": 268, "text": "The structure and motion problem of multiple one- dimensional projections of a two-dimensional environment is studied. One-dimensional cameras have proven useful in several different applications, most prominently for autonomous guided vehicles, but also in ordinary vision for analysing planar motion and the projection of lines. Previous results on one-dimensional vision are limited to classifying and solving minimal cases, bundle adjustment for finding local minima to the structure and motion problem and linear algorithms based on algebraic cost functions. In this paper, we present a method for finding the global minimum to the structure and motion problem using the max norm of reprojection errors. We show how the optimal solution can be computed efficiently using simple linear programming techniques. The algorithms have been tested on a variety of different scenarios, both real and synthetic, with good performance. In addition, we show how to solve the multiview triangulation problem, the camera pose problem and how to dualize the algorithm in the Carlsson duality sense, all within the same framework.", "paperid": 2160011559, "normalizedname_level1": "artificial intelligence"}
{"index": 269, "text": "This paper deals with a topic of phonetic search in text form, which is a type of string matching algorithms. It describes a proposed solution for phonetic searching of the Slovak and neighborhood languages surnames. This solution was designed to improve a search precision and recall for persons with the surnames originated in these languages.", "paperid": 2015043351, "normalizedname_level1": "artificial intelligence"}
{"index": 270, "text": "A new approach to web search that is based on a bee hive metaphor is presented. We proposed a modified model of a bee hive. Our model comprises of a dance floor, an auditorium, and a dispatch room. We have shown that the model is a true model of a bee hive in the sense it simulates several kinds of its typical behaviour. However, more importantly it is a simple model that describes some processes taking place in web search. Our model incorporates also several kinds of uncertainty. The experiments show that uncertainty increases robustness of the search and makes it actually more efficient.", "paperid": 2077870734, "normalizedname_level1": "artificial intelligence"}
{"index": 271, "text": "Expert systems for classification tasks in medical diagnosis systems require two properties. The true positives should be very high, as well as the true negatives, i.e. the system should correctly catch those who are ill, and correctly dismiss those who are healthy. The multi-modal evolutionary classifier uses a genetic algorithm to learn a reference vector for each class, and classification is done by measuring the distance of the new example to reference vectors. For complex datasets such as medical diagnosis, interactions between features are typically complex and the multi-modal classifier's single reference vector is not able to capture this. In this work an extension to the algorithm is proposed, which learn sets of multi-modal classifiers using resampling and form an ensemble from these, using a genetic algorithm. The algorithm is evaluated on a sample of publicly available medical diagnosis datasets. While this is a work-in-progress, initial findings are that compared to the base classifier, using evolutionary learned ensembles improves accuracy in all cases, and is a direction for future work.", "paperid": 1662593795, "normalizedname_level1": "artificial intelligence"}
{"index": 272, "text": "The understandability of rule sets is an important issue in knowledge discovery, where classification rules, for example, are extracted from large data sets. An important criterion in this context is the goodness of fit of a given classifier, i.e., a measure that gives an quantitative answer to the question, how good a classifier fits to the data it has to classify. In this article we provide an appropriate measure for a Mamdani-type fuzzy classifier with Gaussians and singletons as membership functions, sum-prod inference, and height method for defuzzification. That is, goodness of fit must be measured for multivariate Gaussian mixture models. Therefore, we adopt conventional test methods for univariate, unimodal probability distributions (e.g., Kolmogorov-Smirnov for chi-square), provide a measure for the goodness of fit of our fuzzy classifier, and discuss its properties. In a second step we go even beyond this point by showing how this measure could be extended to an analysis tool that gives detailed hints which rules or which membership functions are not suitably realized.", "paperid": 2138370288, "normalizedname_level1": "artificial intelligence"}
{"index": 273, "text": "We propose to use the n-multigram model to help the automatic text classification task. This model could automatically discover the latent semantic sequences contained in the document set of each category. Based on the n-multigram model and the n-gram language model, we put forward two text classification algorithms. The experiments on RCV1 show that our proposed algorithm based on n-multigram model can achieve the similar classification performance compared with the one based on n-gram model. However, the model size of our algorithm is only 4.21% of the latter one. Another proposed algorithm based on the combination of nmultigram model and n-gram model improves the micro- F1 and macro-F1 values by 3.5% and 4.5% respectively which support the validity of our approach.", "paperid": 2129611069, "normalizedname_level1": "artificial intelligence"}
{"index": 274, "text": "The task of location parameter estimation by means of adaptive censored procedure in SαS noise environment is considered. Based on the carried out research it is proposed to use the percentile coefficient of kurtosis and median absolute deviation as parameters able to describe processes with SαS pdf in case of absence of a priori knowledge about its statistical characteristics. The new formula is presented to evaluate the censoring parameter of the investigated estimator. Based on comparative analysis it is shown that the proposed adaptive procedure allows one to obtain the smallest variance among the considered robust and adaptive estimators for α ≪ 0.9. The proposed adaptive estimate can be put into basis of noise suppressing filter to be applied in image homogeneous regions in case of mixed noise.", "paperid": 2124011255, "normalizedname_level1": "artificial intelligence"}
{"index": 275, "text": "In this paper, a novel approach based on acoustic cues for automatic segmenting television stream into individual programs is proposed. This presented method is composed of the following steps: Several sets of repetitions in the audio track is detected by using silence detection and robust audio hashing; The found repetitions are treated as advertisements if the range of their length is from 5 seconds to 120 seconds; Programs are segmented from the recorded TV streams using the detected advertisements. Experiments on real-world TV recordings show the effectiveness of the proposed approach.", "paperid": 2020617985, "normalizedname_level1": "artificial intelligence"}
{"index": 276, "text": "When developing Chinese character input methods and its associated software, we are always suffering from the multiple Chinese character encodings. The article firstly introduces a universal Chinese character encoding on simplified-Chinese platform and traditional-Chinese platform. Then it discusses how to build an 118N Chinese character database. After analyzing the relationship between native Chinese character encodings and this ISO standard, the article describes how to convert between these native Chinese character encoding and ISO/IEC 10646-1:2000 standard. At last, the article gives an instance to use in the real-time screen dictionary system.", "paperid": 1566709417, "normalizedname_level1": "artificial intelligence"}
{"index": 277, "text": "A new blind geometric robust watermarking scheme is presented based on singular value decomposition (SVD) and discrete wavelet transform (DWT). In the scheme, the host image is decomposed with n-level DWT and the low frequency sub-band is divided into nonoverlapping blocks. SVD is applied to every block. The watermark is embedded into the low frequency DWT coefficients by modifying the largest singular value of each block. In the detection process, the geometric distortion parameters of the attacked watermarked image are estimated first by the proposed improved geometric correction method which is based on the Zernike moments and invariant centroid. Then the watermark is extracted from the recovered image. Compared with traditional geometric estimation method, the suggested improved geometric correction method can estimate the geometric distortion parameters more accurately. The experimental results demonstrate that the proposed watermarking scheme is robust to geometric attacks, and as well, a variety of common attacks.", "paperid": 2071283346, "normalizedname_level1": "artificial intelligence"}
{"index": 278, "text": "In this paper, we describe a novel approach to investigate negative behavior dynamics in online social networks as epidemic phenomena. We present a finite-state machine model for time-varying epidemic dynamics, and validate this model with experiments over a large dataset of Youtube commentaries, indicating how different epidemic patterns of behavior can be tied to specific interaction patterns among users. A full version of this paper is available on arXiv.org.", "paperid": 2289824173, "normalizedname_level1": "artificial intelligence"}
{"index": 279, "text": "In spoken dialog, speakers are simultaneously engaged in various mental processes, and it seems likely that the word that will be said next depends, to some extent, on the states of these mental processes. Further, these states can be inferred, to some extent, from properties of the speaker's voice as they change from moment to moment. As a illustration of how to apply these ideas in language modeling, we examine volume and speaking rate as predictors of the upcoming word. Combining the information which these provide with a trigram model gave a 2.6% improvement in perplexity.", "paperid": 2123025256, "normalizedname_level1": "artificial intelligence"}
{"index": 280, "text": "Now image denoising is an important process in image processing. The proposed method focuses on rain streak removal frame work based on morphological component analysis. Bilateral filter is used in the denoising stage. Then the filtered image partitioned into low frequency and high frequency component. The high frequency component undergone various processes such as patch extraction, dictionary learning and dictionary partitioning. The output of dictionary partitioning approach undergone morphological component analysis as an image decomposition process. As a result, the rain component can be successfully removed from the image while preserving most of the original image details.", "paperid": 2052166832, "normalizedname_level1": "artificial intelligence"}
{"index": 281, "text": "Due to the large volume of data generated in healthcare organizations, the use of data mining techniques becomes essential for improving the quality of care, physician practices and disease management. However, expert knowledge is not based only on rules, but also on a mixture of knowledge and experiences. It is in this context that we set the involvement of data mining techniques and CBR to support medical decision making in order to optimize the time and benefit from the experience of experts. We propose a support system for medical decision-making based on CBR and data mining. This system allows, from a database of examples, engaging a method of Symbolic induction and Cellular Inference Engine (MIC) for the construction of a case retrieval model. To evaluate this new approach we have customized the platform jCOLIBRI with a real case base about the treatment of tuberculosis.", "paperid": 2028277586, "normalizedname_level1": "artificial intelligence"}
{"index": 282, "text": "This paper presents asymmetric taper (or window)-based robust Mel frequency cepstral coefficient (MFCC) feature extraction for automatic speech recognition (ASR). Commonly, MFCC features are computed from a symmetric Hamming-tapered direct-spectrum estimate. Symmetric tapers have linear phase and also imply longer time delay. In ASR systems, phase information is usually discarded as human speech perception is relatively insensitive to short-time phase distortion. So, any linearity constraint on phase can be removed without adverse effects. Use of asymmetric tapers, having better frequency response and shorter time delay, for MFCC feature extraction in speech recognition can lead to better recognition performance. Using our proposed method it is possible to introduce asymmetry in any symmetric taper by adjusting only one additional parameter, which controls the degree of asymmetry. Experimental results on the AURORA-2 corpus show that the proposed asymmetric tapers outperform the symmetric Hamming taper in terms of word accuracy both in clean and noisy environments.", "paperid": 2148725087, "normalizedname_level1": "artificial intelligence"}
{"index": 283, "text": "Amounts of data increases perform data overload is a critical issue in IT organizations. Therefore, clustering techniques are proposed for reducing the size of data to keep the performance of the entire system by selecting groups of data that is depend on their similarity. According to this k-means and hierarchical clustering techniques have been designed. The well-known techniques are clustering large application-based upon randomized search and clustering and balanced iterative reducing and clustering using hierarchy. However, one of the remained problems is size of data still large. This can make the whole processes of data mining works slowly. Another is that the similarity value of the clusters is still not high enough for to be used, particularly, in the process of decision making. Therefore, this paper presents a model of clustering to provide higher efficiency of the process of k-mean and hierarchical clustering. The efficiency of the proposed model is better than traditional techniques by 0.5–1.3 % approximately in term of giving higher similarity value. Besides, the running time of the model is also faster than the comparative studies about 2.7–7.4 times.", "paperid": 1578989341, "normalizedname_level1": "artificial intelligence"}
{"index": 284, "text": "In this paper, an adaptive postprocessor figured with discrete cosine transform (DCT)-based block classification to effectively remove the so-called blocky effect from compressed video sequences is proposed. The proposed DCT-based detection algorithms for both intraframes and interframes require much lower computation complexity than the spatial-domain approaches. In order to preserve the edge information, the adaptive postprocessor is also designed with a DCT-based edge detection mechanism such that a one-dimensional median filter can be adaptively adjusted to match with the edge orientation. Simulation results show that the proposed DCT-based detection algorithms accurately classifies smooth, edge, or nonsmooth blocks to help the adaptive postprocessor to effectively remove the blocky effect and sharply preserve the edge information.", "paperid": 2160515577, "normalizedname_level1": "artificial intelligence"}
{"index": 285, "text": "In this paper, we approach the task of appearance based person re-identification for scenarios where no biometric features can be used. For that, we build on a person re-identification approach that uses the Implicit Shape Model (ISM) and SIFT features for re-identification. This approach builds identity models of persons during tracking and employs these models for re-identification. We apply this re-identification, which was until now only evaluated in the infrared spectrum, to data acquired in the visible spectrum. Furthermore we evaluate view independence of the re-identification approach and introduce methods that extend view invariance. Specifically, we (i) propose a method for online view-determination of a tracked person, (ii) use the online view-determination to generate view specific identity models of persons which increase model distinctiveness in re-identification, and (iii) introduce a method to convert identity models between views to increase view independence.", "paperid": 2100028243, "normalizedname_level1": "artificial intelligence"}
{"index": 286, "text": "The system of community tap-water is influenced by many factors, which is a typical nonlinear dynamic system. Both neural networks and Volterra series are widely used in nonlinear dynamic system. This paper discusses the relations between Volterra series and BP neural network, and proposes the Volterra series-based neural network and the solution of the hight order Volterra series kernel. In this paper, the ARMA model, BP neural network and Volterra series-based neural network are applied to short-term forecast a community tap-water flows. According to the results of the comparison, it shows that the Volterra series-based neural network is better than other methods.", "paperid": 2068871563, "normalizedname_level1": "artificial intelligence"}
{"index": 287, "text": "This paper presents a content-adaptive coding scheme for immersive networked experience of sports events, in particular, soccer games. We assume that future sports events are captured by an array of fixed high-definition cameras which provide multiview image sequences for a free-viewpoint immersive networked experience in a home environment. We discuss a content-adaptive coding scheme for image sequences that exploits properties of such sequences and that permits efficient user interactions. In this work, we construct a rate distortion model for an image sequence to obtain the optimal bitrate allocation among static and dynamic content items. The optimal bitrate allocation results in a rate distortion performance of the coding scheme that outperforms that of conventional H.264/AVC coding significantly.", "paperid": 2036031004, "normalizedname_level1": "artificial intelligence"}
{"index": 288, "text": "This research develops methods of automating the production of behavioral robotics controllers. Population-based artificial evolution was employed to train neural network-based controllers to play a robotic version of the team game Capture the Flag. The robot agents used processed video data for sensing their environment. To accommodate the 35 to 150 sensor inputs required, large neural networks of arbitrary connectivity and structure were evolved. An intra-population competitive genetic algorithm was used and selection at each generation was based on whether the different controllers won or lost games over the course of a tournament. This paper focuses on the evolutionary neural controller architecture. Evolved controllers were tested in a series of competitive games and transferred to real robots for physical verification.", "paperid": 2115131604, "normalizedname_level1": "artificial intelligence"}
{"index": 289, "text": "Successfully tracking targets in a video sequence has many important applications, including unmanned aerial vehicle (UAV) surveillance. A robust and efficient video tracking algorithm based on the continuous wavelet transform (CWT) is presented, which has been proven to be effective in capturing motion information over multiple frames and excellent in velocity selectivity. The CWT converts target trajectories in a spatio-temporal domain into target energy volumes in a wavenumber-frequency domain. By integrating over different motion parameters, three target energy densities are obtained, which then serve as cost functions for estimating target trajectories and sizes. Because of excellent velocity selectivity, the energy-based tracker has the capability of detecting and tracking targets with a particular velocity range. To best handle target interferences among multiple nearby or crossing targets, a novel joint processing technique using expectation-maximization-based Gaussian mixture estimation is developed. A global nearest neighbourhood algorithm is employed to perform data association and maintain continuous kinematic trajectories. In addition to computer simulations, the developed energy-based algorithm is applied to a UAV surveillance application where multiple vehicles move closely to each other on a multi-lane road.", "paperid": 2047398583, "normalizedname_level1": "artificial intelligence"}
{"index": 290, "text": "Spectral clustering has been widely used in data mining in the past years. The performance of spectral clustering is very sensitive to the selection of scale parameter. Especially, when data has multi-scale it is very difficult to find a proper value for the scale parameter. To solve the problem, an improved method based on adaptive neighbor distance sort order has been proposed in this paper. The method enlarges the affinity between two points in the same cluster and reduces that in different clusters. Our experiments on the synthetic and real life datasets have shown promising results comparing with tradition method and k-means.", "paperid": 2039140876, "normalizedname_level1": "artificial intelligence"}
{"index": 291, "text": "Introducing distortions into perspective views is a popular technique to direct our visual attention to specific objects, as seen in hand-drawn illustrations and cartoon animations. This type of image expression, called nonperspective projection, is feasible in visual communication, because the human visual system can reconstruct the target three-dimensional (3D) scene correctly provided that the corresponding image distortions are within a certain perceptual tolerance. In this paper, we develop a perceptual approach to guiding the design of such nonperspective images by referring to the 3D perception induced by pictorial depth cues. We formulate an acceptable tolerance by investigating how we perceive image distortion according to the change in the configuration of depth cues. The obtained formulation is then incorporated into our new algorithm, with which we can automatically control plausible image deformation by simply modifying the positions and sizes of specific objects in a scene.", "paperid": 2030956486, "normalizedname_level1": "artificial intelligence"}
{"index": 292, "text": "Stereo vision has become a very interesting sensing technology for robotic platforms. It offers various advantages, but the drawback is a very high algorithmic effort. Due to the aptitude of certain non-parametric techniques for field programmable gate array (FPGA) based stereo matching, these algorithms can be implemented in highly parallel design while offering adequate real-time behavior. To enable the provision of color images by the stereo sensor for object classification tasks, we propose a technique for extending the rank and the census transform for increased robustness on gray scaled Bayer patterned images. Furthermore, we analyze the extended and the original algorithmspsila behavior on image sets created in controlled environments as well as on real world images and compare their resource usage when implemented on our FPGA based stereo matching architecture.", "paperid": 2105956270, "normalizedname_level1": "artificial intelligence"}
{"index": 293, "text": "Environment sampling is a popular technique for rendering scenes with distant environment illumination. However, the temporal consistency of animations synthesized under dynamic environment sequences has not been fully studied. This paper addresses this problem and proposes a novel method, namely spatiotemporal sampling, to fully exploit both the temporal and spatial coherence of environment sequences. Our method treats an environment sequence as a spatiotemporal volume and samples the sequence by stratifying the volume adaptively. For this purpose, we first present a new metric to measure the importance of each stratified volume. A stratification algorithm is then proposed to adaptively suppress the abrupt temporal and spatial changes in the generated sampling patterns. The proposed method is able to automatically adjust the number of samples for each environment frame and produce temporally coherent sampling patterns. Comparative experiments demonstrate the capability of our method to produce smooth and consistent animations under dynamic environment sequences.", "paperid": 2131408567, "normalizedname_level1": "artificial intelligence"}
{"index": 294, "text": "Brain Computer Interface (BCI) is useful as an interface to operate a computer and external equipment using brain activities. A Steady-State Visual Evoked Potential (SSVEP) is known as the powerful tool to achieve BCI systems. A user of BCI focuses on the visual stimulus which produces brain activities used as an input of BCI. On the other hand, the user needs to pay own attention to surrounding situation for avoiding accidents. This means that the user should turn own attention to both the environment and the visual stimulus. The practical SSVEP based BCI often places visual stimuli around a fixation point which displays the surrounding situation captured by a camera. In addition, the visual stimulus equipped with moving vehicles has a slight movement due to our body movement and vehicle movement. This fact shows that the visual stimulus is moving at the side of user's vision. However, the relation of a SSVEP and a place and moving of visual stimulus has not been discussed. In this paper, we measure and analyze SSVEP for visual stimulus located on the surrounding area of fixation point, and a moving visual stimulus.", "paperid": 2343552433, "normalizedname_level1": "artificial intelligence"}
{"index": 295, "text": "Automatic modulation classification (AMC) is a scheme to identify the data samples automatically. Empirical mode decomposition (EMD) is a self-adaptive signal processing method that can be applied to non-linear and non-stationary process perfectly. This paper presents a new method for AMC, using empirical mode decomposition (EMD) method. By utilizing the proposed feature extraction method, the disadvantages of conventional AMC algorithms, such as the feature value is sensitive to outliers in the data, the sample sequence is long and so on could be overcome. The advantage of our new algorithm is we don't need the channel information as a priori. Simulation results show that the performance of the proposed algorithm is comparable with other existing AMC algorithm.", "paperid": 2106890450, "normalizedname_level1": "artificial intelligence"}
{"index": 296, "text": "In this paper, we study the problem of rotation invariant texture classifications. There are several methods in texture recognition problem, we compare three best known methods such us: Gabor wavelet filter, Local Binary Pattern operators (LBP) and co-occurrence matrix (GLCM). A multi-class Support Vector Machines (SVM) is used as a classifier. The three methods are evaluated based on two different databases: Brodatz and Outex to bring out a comparative study about the discrimination capabilities of those different families of texture classification methods. The experimental results show that some of the studied methods are more compatible with this classification problem than the others. The SVM classifier approve the running time of the algorithm of classification.", "paperid": 2062888480, "normalizedname_level1": "artificial intelligence"}
{"index": 297, "text": "Inspired by SWOT, which is a method for strategic analysis, we propose a new FMCDM for software selection. In our method, two matrices are constructed to represent the strength and weakness of the alternatives. With our method, we provide decision makers more information than the single index to make more subtle decision.", "paperid": 2119593267, "normalizedname_level1": "artificial intelligence"}
{"index": 298, "text": "In this paper, we applied online neuroevolution to evolve nonplayer characters for The Open Racing Car Simulator (TORCS). While previous approaches allowed online learning with performance improvements during each generation, our approach enables a finer grained online learning with performance improvements within each lap. We tested our approach on three tracks using two methods of online neuroevolution (NEAT and rtNEAT) combined with four evaluation strategies ( -greedy, -greedy-improved, softmax, and interval-based) taken from the literature. We compared the eight resulting configurations on several driving tasks involving the learning of a driving behavior for a specific track, its adaptation to a new track, and the generalization capability to unknown tracks. The results we present show that, notwithstanding the several challenges that online learning poses, our approach 1) can successfully evolve drivers from scratch, 2) can also be used to transfer evolved knowledge to other tracks, and 3) can generalize effectively producing controllers that can drive on difficult unseen tracks. Our results also suggest that the approach performs better when coupled with online NEAT and also indicate that -greedy-improved and softmax are generally better than the other evaluation strategies. A comparison with typical offline neuroevolution suggests that online neuroevolution can be competitive and even outperform traditional offline approaches on more difficult tracks while providing all the interesting features of online learning. Overall, we believe that this study may represent an initial step toward the application of online neuroevolution in games.", "paperid": 2157390785, "normalizedname_level1": "artificial intelligence"}
{"index": 299, "text": "Sparse Decomposition (SD) of a signal on an overcomplete dictionary has recently attracted a lot of interest in signal processing and statistics, because of its potential application in many different areas including Compressive Sensing (CS). However, in the current literature, the dictionary matrix has generally been assumed to be of full-rank. In this paper, we consider non-full-rank dictionaries (which are not even necessarily overcomplete), and extend the definition of SD over these dictionaries. Moreover, we present an approach which enables to use previously developed SD algorithms for this non-full-rank case. Besides this general approach, for the special case of the Smoothed l0 (SL0) algorithm, we show that a slight modification of it covers automatically non-full-rank dictionaries.", "paperid": 2146478980, "normalizedname_level1": "artificial intelligence"}
{"index": 300, "text": "The biometric systems for user verification are becoming more popular in this age. Iris recognition system is a new technology for user verification. This paper presents an iris detection and recognition method, which adopts Canny transform to extract iris texture feature and wavelet probabilistic neural network as iris biometric classifier. The method combines wavelet neural network and probabilistic neural network for a new classifier model were able to improve the biometrics recognition accuracy as well as the global system performance. A simple and fast training algorithm, AdaBoost, is also introduced for training the wavelet probabilistic neural network. When applying the algorithm on an iris images database, the experimental results show 100% correct classifications and the method have an efficiency feasibility and performance.", "paperid": 2002880630, "normalizedname_level1": "artificial intelligence"}
{"index": 301, "text": "Achieving globally coherent video inpainting results in reasonable time and in an automated manner is still an open problem. In this paper, we build on the seminal work by Wexler et al. to propose an automatic video inpainting algorithm yielding convincing results in greatly reduced computational times. We extend the PatchMatch algorithm to the spatio-temporal case in order to accelerate the search for approximate nearest neighbours in the patch space. We also provide a simple and fast solution to the well known over-smoothing problem resulting from the averaging of patches. Furthermore, we show that results similar to those of a supervised state-of-the-art method may be obtained on high resolution videos without any manual intervention. Our results indicate that globally coherent patch-based algorithms are feasible and an attractive solution to the difficult problem of video inpainting.", "paperid": 2034143235, "normalizedname_level1": "artificial intelligence"}
{"index": 302, "text": "The classical problem of constructing a multidimensional pattern classifier in the Bayesian framework is considered. Preprocessing of the learning sequence by a quasi-inverse of a space-filling curve is proposed and properties of space-filling curves which are necessary to obtain Bayes risk consistency are indicated. The learning sequence transformed into the unit interval is used to estimate the coefficients in an orthogonal expansion of the Bayes decision rule. To transform a new observation into the unit interval requires O(d) elementary operations, where d is the dimension of the observation space. Strong Bayes risk consistency of the classifiers is proved when distributions of the random pair of the observation vector and its class are absolutely continuous with respect to the Lebesgue measure. Attainable convergence rate of the Bayes risk is discussed. Details of the classification algorithm based on the Haar series and its properties are presented. Distribution-free consistency of the classifier is established. The performance of such a classifier is tested both on simulated data and on the standard benchmarks providing misclassification errors comparable to, or even better than the k nearest neighbors (k-NN) method.", "paperid": 2163470070, "normalizedname_level1": "artificial intelligence"}
{"index": 303, "text": "Handwritten character recognition has been an intensive research in artificial intelligence for last decades. A handwritten character recognition fuzzy system with an automatically generated rule base possesses the features of flexibility, efficiency and online adaptability. A major requirement of such a fuzzy system for either online or offline handwritten character recognition is, the segmentation of individual characters into meaningful segments. Then these segments can be used for the calculation of fuzzy features and then these features can be used for the recognition of characters. This paper presents an improved segmentation algorithm for individual offline handwritten character segmentation. This proposed algorithm guarantees the segmentation of individual handwritten character skeletons into meaningful segments, avoiding the problems of over-segmentation and under-segmentation", "paperid": 1485853137, "normalizedname_level1": "artificial intelligence"}
{"index": 304, "text": "This paper presents a new hierarchical clustering algorithm for crop stage classification using hyperspectral satellite image. Amongst the multiple benefits and uses of remote sensing, one of the important application is to solve the problem of crop stage classification. Modern commercial imaging satellites, owing to their large volume of satellite imagery, offer greater opportunities for automated image analysis. Hence, we propose a unsupervised algorithm namely Hierarchical Artificial Immune System (HAIS) of two steps: splitting the cluster centers and merging them. The high dimensionality of the data has been reduced with the help of Principal Component Analysis (PCA). The classification results have been compared with K-means and Artificial Immune System algorithms. From the results obtained, we conclude that the proposed hierarchical clustering algorithm is accurate.", "paperid": 2055931105, "normalizedname_level1": "artificial intelligence"}
{"index": 305, "text": "This article provides a compositional semantics for temporal nouns and temporal prepositions that are annotated as temporal prepositional phrases or noun phrases by an automatic tagging system (e.g., last Monday, on Dec. 1st, for three weeks or before Christmas). Current temporal tagging systems rely on an ad-hoc-representation for temporal date and time expressions, but the more demanding tasks of temporal question-answering and automatic text summarization require a sound logical derivation and representation of temporal expressions. Our proposal draws from two formal accounts of temporal prepositional phrases by Pratt and Francez [2001] and von Stechow [2002b], and is realized within an automatic temporal tagging system for German newspaper articles.", "paperid": 2051712349, "normalizedname_level1": "artificial intelligence"}
{"index": 306, "text": "Decision tree can be considered as one of the most widely used methods due to the acceptable accuracy and interpretable results. The main limitation of this method is uncontrolled growing of tree that leads to produce complex model and degrade comprehensibility. In this paper, we propose layered decision tree (LDT) approach based on data clustering. The proposed algorithm, initially cluster data and sort them with respect to their importance. Then, these data divided into groups that each of them used for construct one layer of tree. Finally, tree will be updated based on maximum depth of each layer. Practical results show that LDT outperform than popular methods, in the term of accuracy, processing time, and complexity.", "paperid": 2076187238, "normalizedname_level1": "artificial intelligence"}
{"index": 307, "text": "The restoration of biomedical images that have been blurred due to body movement are discussed. The observation system for these images is described using a mathematical operator and coordinate transformations. And a band-suppressed restoration filter composed of a series of such operators is proposed for improving the quality of images. In addition, redundancy is introduced into these restoration filters in order to suppress additive noise. The proposed method is applied to blurred X-ray images of a bone model of the elbow joint. The optimum position and number of markers, which are attached to the subject as a reference signal, are also discussed.", "paperid": 2158412460, "normalizedname_level1": "artificial intelligence"}
{"index": 308, "text": "Due to the advances of digital technology, digital contents are widely accepted all over the world. Since the criminal counterfeits and copyright infringement is still under study for the new digital media, there is an increasing need called digital forensics, which involves the collection and analysis of the original digital device, has become an important issue. This study offers a universal approach to extract the important statistical features from Gray Level Co-occurrence Matrix (GLCM), Discrete Wavelet Transform (DWT), Spatial filter, Wiener filter and Gabor filter to identify the printer source for Japanese characters. The average experimental results achieves high identification rate at 94.23% which is significantly superior to the existing known method of using GLCM at 77.31%. The result is also better than the technique for Chinese character identification method at 88.77%. This superior testing performance demonstrates that the proposed identification technique is very effective for Japanese character based source laser printer identification.", "paperid": 1566219896, "normalizedname_level1": "artificial intelligence"}
{"index": 309, "text": "This paper presents a fully automatic white matter lesion (WML) segmentation method, based on local features determined by combining multiple MR acquisition protocols, including T1-weighted, T2-weighted, proton density (PD)-weighted and fluid attenuation inversion recovery (FLAIR) scans. Support vector machines (SVMs) are used to integrate features from these 4 acquisition types, thereby identifying nonlinear imaging profiles that distinguish and classify WMLs from normal brain tissue. Validation on a population of 45 diabetes patients with diverse spatial and size distribution of WMLs shows the robustness and accuracy of the proposed segmentation method, compared to the manual segmentation results from two experienced neuroradiologists.", "paperid": 2109905341, "normalizedname_level1": "artificial intelligence"}
{"index": 310, "text": "Associative classification is a novel and powerful method originating from association rule mining. In the previous studies, a relatively small number of high-quality association rules were used in the prediction. We propose a new approach in which a large number of association rules are generated. Then, the rules are filtered using a new method which is equivalent to a deterministic Boosting algorithm. Through this equivalence, our approach effectively adapts to large-scale classification tasks such as text categorization. Experiments with various text collections show that our method achieves one of the best prediction performance compared with the state-of-the-arts of this field.", "paperid": 2168133915, "normalizedname_level1": "artificial intelligence"}
{"index": 311, "text": "Single-sensor digital color cameras use a process called color demosaicking to produce full color images from the data captured by a color filter array (CFA). The quality of demosaicked images is degraded due to the sensor noise introduced during the image acquisition process. The conventional solution to combating CFA sensor noise is demosaicking first, followed by a separate denoising processing. This strategy will generate many noise-caused color artifacts in the demosaicking process, which are hard to remove in the denoising process. Few denoising schemes that work directly on the CFA images have been presented because of the difficulties arisen from the red, green and blue interlaced mosaic pattern, yet a well designed ldquodenoising first and demosaicking laterrdquo scheme can have advantages such as less noise-caused color artifacts and cost-effective implementation. This paper presents a principle component analysis (PCA) based spatially-adaptive denoising algorithm, which works directly on the CFA data using a supporting window to analyze the local image statistics. By exploiting the spatial and spectral correlations existed in the CFA image, the proposed method can effectively suppress noise while preserving color edges and details. Experiments using both simulated and real CFA images indicate that the proposed scheme outperforms many existing approaches, including those sophisticated demosaicking and denoising schemes, in terms of both objective measurement and visual evaluation.", "paperid": 2155311268, "normalizedname_level1": "artificial intelligence"}
{"index": 312, "text": "A novel method combining a motion detection technique and geometric active contours for face contour detection and tracking with complex backgrounds is presented in this paper. First, Wronskian change detector is used to segment moving regions from complex background. Second, via projection histograms of moving silhouette and geometric constraints of facial parts, possible face region can be determined very quickly. Third, a modified geometric active contour model incorporating prior face shape into the Chan-Vese model is applied to further detect face contour accurately. And finally, via first-order linear motion estimation, the detected face contour is tracked correctly in the following frames. Experimental results show the efficiency of our method.", "paperid": 2119655139, "normalizedname_level1": "artificial intelligence"}
{"index": 313, "text": "This article designs an expert system with real-time inquiry of quayside crane (QC) generated by utilizing a producing system. According to the operational knowledge learned from quayside crane operators, and the adoption of the forward-direction inference mechanism based on the blackboard structure, the designer, by summing up the knowledge, designs an eight-type database with relevant regulations that manifests the data with the producing method, and finally realizes the expert system under the condition of software PB9.0. Through the established database, therefore, the detailed encoding can be transferred to the specified location within the coordinate, and build up an internalized database with the Entity-Relationship Approach.", "paperid": 2138430729, "normalizedname_level1": "artificial intelligence"}
{"index": 314, "text": "Contourlet transform is a new two-dimensional extension of the wavelet transform using multiscale and directional filter banks. In this work, we focus on optimizing the image watermarking using the genetic algorithm applied to the contourlet transform which improves the quality of the watermarked image and the robustness of watermark. We employ, genetic algorithm based embedding schemes namely surrounding mean and zerotree embedding approach. The detection process can be performed without using the original image. The experimental results demonstrate that the watermark is robust to attacks.", "paperid": 2109821064, "normalizedname_level1": "artificial intelligence"}
{"index": 315, "text": "Many recent content-based image retrieval techniques utilize relevance feedback (RF) from the user to adjust the system response to better meet user expectations. One school of RF-based methods uses a weighted Minkowski distance metric to assess similarity, and adjusts the weights to refine query response. A new method of estimating these weight vectors is presented which outperforms existing methods, particularly for the important case of limited training data. A new objective function is presented for an iterative optimization routine which more closely aligns optimization goals with true system goals. A new analysis framework is presented in the derivation of this technique which is useful for understanding the limitations of many RF methods.", "paperid": 2129789438, "normalizedname_level1": "artificial intelligence"}
{"index": 316, "text": "This paper illustrates an evolution in state-of-the-art speaker verification by highlighting the contribution from newly developed techniques. Starting from a baseline system based on Gaussian mixture models that reached state-of-the-art performances during the NIST'04 SRE, final systems with new intersession compensation techniques show a relative gain of around 50%. This work highlights that a key element in recent improvements is still the classical maximum a posteriori (MAP) adaptation, while the latest compensation methods have a crucial impact on overall performances. Nuisance attribute projection (NAP) and factor analysis (FA) are examined and shown to provide significant improvements. For FA, a new symmetrical scoring (SFA) approach is proposed. We also show further improvement with an original combination between a support vector machine and SFA. This work is undertaken through the open-source ALIZE toolkit.", "paperid": 2161471225, "normalizedname_level1": "artificial intelligence"}
{"index": 317, "text": "Visual working memory (VWM) consolidation is the process to transfer a fleeting perpetual representation into a durable WM representation that can survive the presentation of new sensory inputs. It is investigated by post-exposure of a mask shortly after offset of memory array (S1). The memory performance increases as stimulus onset asynchrony (SOA) between S1 and mask array increases and finally reaches a asymptote level not influenced by the mask. It is considered that masks interfere the memory items representation into VWM in short SOA and that causes the consolidation phenomenon. Nevertheless, the question leaves open: how do masks interfere with this consolidation process. In this study, we tested whether masks overwrote the perceptual representation of memory items or competed with them for VWM representation. Masks interfered only when they appeared in the same location as memory items. We concluded that “overwriting”, not “competing”, characterized the VWM consolidation. Using the model “boost and bounce theory of temporal attention”[1], we gave the explanation to that conclusion.", "paperid": 1972616684, "normalizedname_level1": "artificial intelligence"}
{"index": 318, "text": "This paper presents a novel and reliable tracking-by detection method for image regions that undergo affine transformations such as translation, rotation, scale, dilatation and shear deformations, which span the six degrees of freedom of motion. Our method takes advantage of the intrinsic Lie group structure of the 2D affine motion matrices and imposes this motion structure on a kernelized structured output SVM classifier that provides an appearance based prediction function to directly estimate the object transformation between frames using geodesic distances on manifolds unlike the existing methods proceeding by linearizing the motion. We demonstrate that these combined motion and appearance model structures greatly improve the tracking performance while an incorporated particle filter on the motion hypothesis space keeps the computational load feasible. Experimentally, we show that our algorithm is able to outperform state-of-the-art affine trackers in various scenarios.", "paperid": 2091075118, "normalizedname_level1": "artificial intelligence"}
{"index": 319, "text": "An algorithm dedicated to automatic segmentation of breast magnetic resonance images is presented in this paper. Our approach is based on a pipeline that includes a denoising step and statistical segmentation. The noise removal preprocessing relies on an anisotropic diffusion scheme, whereas the statistical segmentation is conducted through a Markov random field model. The continuous updating of all parameters governing the diffusion process enables automatic denoising, and the partial volume effect is also addressed during the labeling step. To assess the relevance, the Jaccard similarity coefficient was computed. Experiments were conducted on synthetic data and breast magnetic resonance images extracted from a high-risk population. The relevance of the approach for the dataset is highlighted, and we demonstrate accuracy superior to that of traditional clustering algorithms. The results emphasize the benefits of both denoising guided by input data and the inclusion of spatial dependency through a Markov random field. For example, the Jaccard coefficient for the clinical data was increased by 114%, 109%, and 140% with respect to a K-means algorithm and, respectively, for the adipose, glandular and muscle and skin components. Moreover, the agreement between the manual segmentations provided by an experienced radiologist and the automatic segmentations performed with this algorithm was good, with Jaccard coefficients equal to 0.769, 0.756, and 0.694 for the above-mentioned classes.", "paperid": 2001349183, "normalizedname_level1": "artificial intelligence"}
{"index": 320, "text": "The shape and size of retinal vessels have been prospectively associated with cardiovascular outcomes in adult life, and with cardiovascular precursors in early life, suggesting life course patterning of vascular development. However, the shape and size of arterioles and venules may show similar or opposing associations with disease precursors / outcomes. Hence accurate detection of vessel type is important when considering cardio-metabolic influences on vascular health. This paper presents an automated method of identifying arterioles and venules, based on colour features using the ensemble classifier of boot strapped decision trees. The classifier utilizes pixel based features, vessel profile based features and vessel segment based features from both RGB and HIS colour spaces. To the best of our knowledge, the decision trees based ensemble classifier has been used for the first time for arteriole/venule classification. The classification is performed across the entire image, including the optic disc. The methodology is evaluated on 3149 vessel segments from 40 colour fundus images acquired from an adult population based study in the UK (EPIC Norfolk), resulting in 83% detection rate. This methodology can be further developed into an automated system for measurement of arterio-venous ratio and quantification of arterio-venous nicking in retinal images, which may be of use in identifying those at high risk of cardiovascular events, in need of early intervention.", "paperid": 1895732026, "normalizedname_level1": "artificial intelligence"}
{"index": 321, "text": "We investigated the impact of exploratory head movements on sound localization accuracy using real and virtual 5.1 loudspeaker arrays. Head orientation data in the horizontal plane was provided either by the Microsoft Kinect face-tracking or Oculus Rift's built-in Inertial Measurement Unit (IMU) which resulted in significantly different precision and accuracy of measurements. In both cases, results suggest improvements in virtual source localization accuracy in the front and rear quadrants.", "paperid": 2207044539, "normalizedname_level1": "artificial intelligence"}
{"index": 322, "text": "We report on a longitudinal study of unconstrained handwriting recognition performance. After 250 minutes of practice, participants had a mean text entry rate of 24.1 wpm. For the first four hours of usage, entry and error rates of handwriting recognition are about the same as for a baseline QWERTY software keyboard. Our results reveal that unconstrained handwriting is faster than what was previously assumed in the text entry literature.", "paperid": 2049600821, "normalizedname_level1": "artificial intelligence"}
{"index": 323, "text": "Real-world datasets often involve multiple views of data items, e.g., a Web page can be described by both its content and anchor texts of hyperlinks leading to it; photos in Flickr could be characterized by visual features, as well as user contributed tags. Different views provide information complementary to each other. Synthesizing multi-view features can lead to a comprehensive description of the data items, which could benefit many data analytic applications. Unfortunately, the simple idea of concatenating different feature vectors ignores statistical properties of each view and usually incurs the “curse of dimensionality” problem. We propose Multi-view Concept Learning (MCL), a novel nonnegative latent representation learning algorithm for capturing conceptual factors from multi-view data. MCL exploits both multi-view information and label information. The key idea is to learn a common latent space across different views which (1) captures the semantic relationships between data items through graph embedding regularization on labeled items, and (2) allows each latent factor to be associated with a subset of views via sparseness constraints. In this way, MCL could capture flexible conceptual patterns hidden in multi-view features. Experiments on a toy problem and three real-world datasets show that MCL performs well and outperforms baseline methods.", "paperid": 1655434153, "normalizedname_level1": "artificial intelligence"}
{"index": 324, "text": "Collocation, i.e. the sequences of certain words which habitually co-occur, plays an essential part in human language. The present study is intending to identify the detailed classification and typical features of collocations in Chinese language, and explore a new computer-assistant way for extraction and representation of Chinese collocations. The investigation is based on the largest and only Singapore Chinese corpus (SCC), of which 20 million words have been analysed. The central novel idea of this research is the combination of dictionary, language rules and statistic data in automatic collocation extraction. So far, this method has not been proposed.", "paperid": 2142261369, "normalizedname_level1": "artificial intelligence"}
{"index": 325, "text": "We present a multi-mobile robot collision avoidance system based on the velocity obstacle paradigm. Current positions and velocities of surrounding robots are translated to an efficient geometric representation to determine safe motions. Each robot uses on-board localization and local communication to build the velocity obstacle representation of its surroundings. Our close and error-bounded convex approximation of the localization density distribution results in collision-free paths under uncertainty. While in many algorithms the robots are approximated by circumscribed radii, we use the convex hull to minimize the overestimation in the footprint. Results show that our approach allows for safe navigation even in densely packed environments.", "paperid": 2072337003, "normalizedname_level1": "artificial intelligence"}
{"index": 326, "text": "In this paper, we propose an automated tracking system which can robustly recognize multiple people in the video sequence. By checking the foreground coverage measure, the system first segments and localizes isolated people from multiple foreground regions. We then model each tracked person by his/her color histogram so that the system is capable of performing optimal recognition after occlusion from a statistical viewpoint. Since shadow could severely degrade color histograms, we propose a novel shadow-removal scheme to suppress shadow effects and thus help improve the reliability of people recognition. The people recognition technique is also extended to identify people that re-enter the scene of a closed-environment.", "paperid": 2120442810, "normalizedname_level1": "artificial intelligence"}
{"index": 327, "text": "We propose a spatio-temporal error concealment algorithm for video transmission in an error-prone environment. The proposed technique employs motion vector estimation, edge-preserving interpolation, and texture analysis/synthesis. It has two main advantages with respect to existing methods, namely: (i) it aims at optimizing the visual quality of the restored video, and not only PSNR; and (ii) it employs an automatic mode selection algorithm in order to decide, on a macroblock basis, whether to use the spatial restoration, the temporal one, or a combination thereof. The algorithm has been applied to H.26L video, providing satisfactory performance over a large set of operating conditions.", "paperid": 2096091887, "normalizedname_level1": "artificial intelligence"}
{"index": 328, "text": "In the real world, images always have several visual objects instead of only one, which makes it difficult for conventional object recognition methods to deal with them. In this paper, we present a topologically sorted classifier chain method for learning images with multi-label. We first provide a means of generating a topo-logically sorted label chain ordering by employing a topological sort algorithm and then apply the chain ordering to the classifier chain model proposed by [1] to classify multi-label images. Our method can capture the correlations between labels very effectively due to the sorted label chain ordering and the advantages brought by classifier chain method. We evaluate the proposed method on Corel dataset and demonstrate the micro and macro F1 measures superior to the state-of-the-art methods.", "paperid": 2006126391, "normalizedname_level1": "artificial intelligence"}
{"index": 329, "text": "We compare two methods for assessing the performance of groupwise non-rigid registration algorithms. The first approach, which has been described previously, utilizes a measure of overlap between ground-truth anatomical labels. The second, which is new, exploits the fact that, given a set of nonrigidly registered images, a generative statistical model of appearance can be constructed. We observe that the quality of this model depends on the quality of the registration, and define measures of model specificity and generalisation - based on comparing synthetic images sampled from the model, with those in the original image set - that can be used to assess model/registration quality. We show that both approaches detect the loss of registration accuracy as the alignment of a set of correctly registered MR images of the brain is progressively perturbed. We compare the sensitivities of the two approaches and show that, as well as requiring no ground truth, specificity provides the most sensitive measure of misregistration.", "paperid": 2149176104, "normalizedname_level1": "artificial intelligence"}
{"index": 330, "text": "The intelligent computer was a hot topic in the sixties and seventies. After the most optimistic prophets had lived to see that their “in twenty-five years” predictions of an intelligent computer had failed to appear, the few remaining are cleverly dating their forecasts several hundred years into the future. In addition there is also a change of content in their predictions of computer intelligence. Many have now moderated their statements, now mentioning “applications that would have needed intelligence if humans were doing the job.” But that is something quite different. The Vikings, for example, navigated by using the sun, moon, stars, and wind directions. Even bird flight and floating seaweed may have aided an intelligent or creative navigator. Today, we also use stars for navigating, but artificial stars, satellites, which send a radio beacon that is used by any GPS system to pinpoint the position on the surface of the earth with an accuracy of a few meters. It does a better navigating job than the Vikings just by computing a simple function, but no intelligence is needed.", "paperid": 2102982541, "normalizedname_level1": "artificial intelligence"}
{"index": 331, "text": "Dynamic computed tomography (CT) imaging aims at reconstructing image sequences where the dynamic nature of the living human body is of primary interest. The main applications concerned are image-guided interventional procedures, functional studies and cardiac imaging. The introduction of ultra-fast rotating gantries along with multi-row detectors and in near future area detectors allows huge progress toward the imaging of moving organs with low-contrast resolution. This paper gives an overview of the different concepts used in dynamic CT. A new reconstruction algorithm based on a voxel-specific dynamic evolution compensation is also presented. It provides four-dimensional image sequences with accurate spatio-temporal information, where each frame is reconstructed using a long-scan acquisition mode on several half-turns. In the same time, this technique permits to reduce the dose delivered per rotation while keeping the same signal to noise ratio for every frame using an adaptive motion-compensated temporal averaging. Results are illustrated on simulated data.", "paperid": 2101806109, "normalizedname_level1": "artificial intelligence"}
{"index": 332, "text": "Super-resolution image reconstruction is an important technology in many image processing areas such as image sensing, medical imaging, satellite imaging, and television signal conversion. It is also a key word of a recent consumer HDTV set that utilizes the CELL processor. Among various super-resolution methods, the learning-based method is one of the most promising solutions. The problem of the learning-based method is its enormous computational time for image searching from the large database of training images. We have proposed a new Total Variation (TV) regularization super-resolution method that utilizes a learning-based super-resolution method. We have obtained excellent results in image quality improvement. However, our proposed method needs long computational time because of the learning-based method. In this paper, we examine two methods that reduce the computational time of the learning-based method. The resulting algorithms reduce complexity significantly while maintaining comparable image quality. This enables the adoption of learning-based super-resolution to the motion pictures such as HDTV and internet movies.", "paperid": 2117045227, "normalizedname_level1": "artificial intelligence"}
{"index": 333, "text": "Electrical impedance tomography (EIT) is a relatively new medical imaging method, and has the advantages of being inexpensive, safe, non-invasive and portable. Complementary to CT and MRI, it reconstructs images of impedance changes from the EIT sensor arrays. Its principal shortcoming is poor spatial resolution. One approach to improving spatial resolution is to include prior information regarding the physical geometry of the object. In this paper, a forward model regarding the physical information of the human thorax has been developed using a commercial software tool COMSOL, and lung ventilation experiments are undertaken and illustrated. Compared to the circular model, the visualization results of lung ventilation indicate that EIT incorporating physical information is a promising technique with the potential to be applied in future clinical diagnoses and monitoring.", "paperid": 2123884309, "normalizedname_level1": "artificial intelligence"}
{"index": 334, "text": "This chapter contains sections titled:      Takagi-Sugeno Fuzzy Systems as Interpolators between Memoryless Functions       Takagi-Sugeno Fuzzy Systems as Interpolators between Continuous-Time Linear State-Space Dynamic Systems       Takagi-Sugeno Fuzzy Systems as Interpolators between Discrete-Time Linear State-Space Dynamic Systems       Takagi-Sugeno Fuzzy Systems as Interpolators between Discrete-Time Dynamic Systems described by Input-Output Difference Equations       Summary       Exercises       ]]>", "paperid": 1594985658, "normalizedname_level1": "artificial intelligence"}
{"index": 335, "text": "This paper aims at dealing with a critical issue for electromyography (EMG) recognition. The issue is related to the stability of an EMG-based prosthesis control. Traditional EMG recognition systems receive EMG patterns and send them into classifiers directly, which generally results in unstable situations if the classes of some of the input EMG patterns are not included in the training of the classifiers. The EMG patterns whose class labels are not defined in the training phase are called non-target patterns. There should be a filter and this filter should be able to reject all non-target EMG patterns. As such, only target EMG patterns are fed into classifier, thus achieving a high-accuracy EMG classification. To this end, we propose in this paper a one-class classification-based non-target EMG pattern filtering scheme. By introducing a novel one-class classifier, called support vector data description (SVDD), into the filtering scheme, the goal mentioned above can easily be achieved. SVDD is a powerful machine learning technique. It can be built on a single class and find a flexible boundary to enclose the target class by using the so-called kernel trick. In experiments, we will show that if the filtering scheme is not performed, the traditional EMG classification system suffers from unstable situations. Contrarily, the whole classification system will achieve satisfactory and stable performance no matter what the input EMG patterns are target or non-target ones, if the proposed filtering scheme is embedded.", "paperid": 2132980678, "normalizedname_level1": "artificial intelligence"}
{"index": 336, "text": "The growth of internet has given rise to the need for better Information Retrieval (IR) techniques which help in obtaining relevant information at a faster rate. Text Summarization is one such technique which aims at producing a quick and concise summary of the Text. Of late, Key word based summary has drawn wide attention of researchers in Natural Language Processing community. The algorithm we have developed extracts key words from Kannada text documents, for which we combine GSS (Galavotti, Sebastiani, Simi)[13] coefficients and IDF(Inverse Document Frequency) methods along with TF(Term Frequency) for extracting key words and later uses these for summarization. The important objective our work is to assign a weight to each word in a sentence, the weight of a sentence is the sum of weights of all words, based on the scoring of sentences; we choose top ‘m’ sentences. A document from a given category is selected from our database custom built for this purpose. The files are obtained from Kannada Webdunia. Kannada Webdunia is a Kannada Portal which offers Political News, Cinema News, Sports news, Shopping and Jokes. Depending on the number of sentences given by the user, a summary is generated. Finally we make comparison of machine generated summary with that of human summary. Yet another objective of this work is to perform feature extraction through removal of stop words. For removing stop words we have presented a novel technique which finds structurally similar words in a document.", "paperid": 2084621467, "normalizedname_level1": "artificial intelligence"}
{"index": 337, "text": "Although the Hopfield network can store and retrieve patterns, its storage capacity is limited. In this study we investigate the effect of Hamming distance of stored patterns on the success of their retrieval. The results show that by removing patterns having low Hamming distance with each other, the capacity of the network increases.", "paperid": 1587116091, "normalizedname_level1": "artificial intelligence"}
{"index": 338, "text": "Multi-agent system is a major research domain in artificial intelligence. The RoboCup (robot soccer cup) provides an ideal testing platform for MAS. In order to making an appropriate decision in a group of agents, each agent has to take the behavior of the other agents into account. In a dynamic environment, these dependencies may change rapidly as a result of the continuously changing state. First, the characteristics about RoboCup competition and the main research directions in this field was described in this paper. Second, we analyze five outstanding simulator teamspsila cooperative strategies in the RoboCup China Open 2007. The analysis would help us to find the new ideas which are different from the traditional teams and seize the research trend in this field. Finally, the importance of the multi-agent system in the related field was discussed in the last part.", "paperid": 2018387326, "normalizedname_level1": "artificial intelligence"}
{"index": 339, "text": "A Type-2 Fuzzy logic controller adapted with genetic algorithm, called type-2 genetic fuzzy logic controller (T2GFLC), is presented in this paper to handle uncertainty with dynamic optimal learning. Genetic algorithm is employed to simultaneous design of type-2 membership functions and rule sets for type-2 fuzzy logic controllers. Traditional fuzzy logic controllers (FLCs), often termed as type-1 fuzzy logic systems using type-1 fuzzy sets, cannot handle large amount of uncertainties present in many real environments. Therefore, recently type-2 FLC has been proposed. The type-2 FLC can be considered as a collection of different embedded type-1 FLCs. However, the current design process of type-2 FLC is not automatic and relies on human experts. The purpose of our study is to make the design process automatic. The evolved type-2 FLCs can deal with large amount of uncertainties and exhibit better performance for the mobile robot. Furthermore, it has outperformed their type-1 counterparts as well as the traditionally designed type-2 FLCs.", "paperid": 1994298248, "normalizedname_level1": "artificial intelligence"}
{"index": 340, "text": "We present a promising new framework for improving boosting performance with transductive inference when training an automatic text detector. The resulting detector is fast and efficient, and it exhibits high accuracy on a large test set.", "paperid": 2125369874, "normalizedname_level1": "artificial intelligence"}
{"index": 341, "text": "Search result diversification aims to maximize the coverage of different pieces of relevant information in the search results. Many diversification methods have been proposed and studied. However, the advantage and disadvantage of each method still remain unclear. In this paper, we conduct a diagnostic study over two state of the art diversification methods with the goal of identifying the weaknesses of these methods to further improve the performance. Specifically, we design a set of perturbation tests that isolate individual factors, i.e., relevance and diversity, which affect the diversification performance. The test results are expected to provide insights on how well each method deals with these factors in the diversification process. Experimental results suggest that some methods perform better in queries whose originally retrieved documents are more relevant to the query while other methods perform better when the documents are more diversified. We therefore propose methods to combine these existing methods based on the predicted factor of the query. The experimental results show that the combined methods can outperform individual methods on TREC collections.", "paperid": 2022256318, "normalizedname_level1": "artificial intelligence"}
{"index": 342, "text": "We present a new automated neuron segmentation algorithm for isotropic 3D electron microscopy data. We cast the problem into the asymmetric multiway cut framework. The latter combines boundary-based segmentation (clustering) with region-based segmentation (semantic labeling) in a single problem and objective function. This joint formulation allows us to augment local boundary evidence with higherlevel biological priors, such as membership to an axonic or dendritic neurite. Joint optimization enforces consistency between evidence and priors, leading to correct resolution of many difficult boundary configurations. We show experimentally on a FIB/SEM dataset of mouse cortex that the new approach outperforms existing hierarchical segmentation and multicut algorithms which only use boundary evidence.", "paperid": 1483512324, "normalizedname_level1": "artificial intelligence"}
{"index": 343, "text": "Recently robots working in various situations as workplace and nursing home autonomously are expected. In those situations, it is important for such robots to recognize surrounding environment and to avoid the risk to people. We propose a method to realize an autonomous robot moving smoothly and safely with low calculation cost by performing motion prediction and human detection in dynamic or static environment. In this paper we use only a single 2D Laser Range Finder (LRF) for reducing total cost. Our proposed method leads to satisfactory results of motion prediction in dynamic environment and human detection in static environment.", "paperid": 2283867284, "normalizedname_level1": "artificial intelligence"}
{"index": 344, "text": "Over the recent years learning from data streams that evolve over time has been witnessing an ever-increasing interest within research and industry communities. Typically a wide range of applications exploit data streams for different sorts of decision making, including monitoring, industrial processes, internet traffic, surveillance, etc. By their very nature, data streams are usually unlabeled given the high velocity of their generation. Collecting labelled examples become very difficult, delayed, costly and sometimes prone to errors. It is therefore very important to devise mechanisms to optimize the labeling process. Active learning offers a principled and systematic way to selectively choose candidate data examples whose labels are to be queried. The overall goal of active learning is to provide, in the worst case, the same performance as that of passive learning (i.e., relying on random sampling) while using less labeled examples. Obviously, the learner should also be able to accommodate unlabeled and labeled data in an online manner. In this talk we will cover recent work on active learning for data stream classification, which is known as stream-based selective sampling. In this latter, the learner makes immediate query decision for each data example during a single scan of the data stream. Stream-based selective sampling is in particular suitable for applications that demand on-the-fly interactive labelling. It is however difficult, because the learner lacks complete knowledge of the underlying data distribution and because such distribution may suffer dynamic change over time. We will overview active learning for stationary as well as non-stationary evolving data streams. In particular, we will discuss multi-criteria active learning and methods for dealing with data drift using online active learning. We will also highlight some of the typical applications where online active learning is relevant.", "paperid": 2548445107, "normalizedname_level1": "artificial intelligence"}
{"index": 345, "text": "As the development of digital technology is increasing, Digital cinema is getting more spread. However, content copy and attack against the digital cinema becomes a serious problem. To solve the above security problem, we propose \"Additional Watermarking\" for digital cinema delivery system. With this proposed \"Additional watermarking\" method, we protect content copyrights at encoder and user side information at decoder. It realizes the traceability of the watermark embedded at encoder. The watermark is embedded into the random-selected frames using Hash function. Using it, the embedding position is distributed by Hash Function so that third parties do not break off the watermarking algorithm. Finally, our experimental results show that proposed method is much better than the convenient watermarking techniques in terms of robustness, image quality and its simple but unbreakable algorithm. Keywords—Decoder, Digital content, JPEG2000 Frame, System-On-Chip and additional watermark", "paperid": 2131607728, "normalizedname_level1": "artificial intelligence"}
{"index": 346, "text": "Network Traffic Classification using classical techniques such as port number based and payload based is becoming very difficult because many applications use dynamic port number and encryption technique to avoid detection. To overcome the drawbacks of classical techniques various machine learning techniques were proposed. Machine learning technique faces the problem of labeled instances (in supervised learning) and time consuming manual work (in unsupervised learning). To address the above problems we proposed a semi supervised machine learning technique. The key idea of proposed technique is to build a classifier from training dataset consisting of both labeled and unlabeled instances. For experimental purpose KDD CUP 99 intrusion detection dataset and MATLAB tool is used. We evaluate and compare the performance of the classifier build with 10%, 20% and 30% labeled instances in training dataset. The result of experiments show that classifier build with 30% labeled instances in training dataset has better performance at number of clusters equals to 50.", "paperid": 2534605732, "normalizedname_level1": "artificial intelligence"}
{"index": 347, "text": "The spread-spectrum watermarking (SSW) technology has become an important technique that is not only widely used for still image and video watermarking, but is also used for audio watermarking. However, some technical problems, such as requiring watermark shaping to reduce audible noise, have greatly limited its application. We present a novel audio watermarking scheme using SSW technology by which we can embed a watermark into an audio signal imperceptibly with robustness to a wide range of unintended and intended attacks. The watermark is represented by sinusoidal patterns consisting of sinusoids with phase-modulation by elements of a pseudorandom sequence. We have confirmed, theoretically and experimentally, that the sinusoidal patterns based on pseudorandom sequences have the same correlation properties as pseudorandom sequences. Compared with the traditional methods using pseudorandom sequences directly for watermarking, the sinusoidal patterns are more easily manipulated for watermark embedding and detection and do not require time-consuming watermark shaping. Both watermark embedding and detection are computationally inexpensive and watermark extraction is completed by blind detection. The effectiveness of the proposed method both in inaudibility and robustness were verified by the STEP 2001 test.", "paperid": 2139053724, "normalizedname_level1": "artificial intelligence"}
{"index": 348, "text": "We propose a new algorithm to detect facial points in frontal and near-frontal face images. It combines a regression-based approach with a probabilistic graphical model-based face shape model that restricts the search to anthropomorphically consistent regions. While most regression-based approaches perform a sequential approximation of the target location, our algorithm detects the target location by aggregating the estimates obtained from stochastically selected local appearance information into a single robust prediction. The underlying assumption is that by aggregating the different estimates, their errors will cancel out as long as the regressor inputs are uncorrelated. Once this new perspective is adopted, the problem is reformulated as how to optimally select the test locations over which the regressors are evaluated. We propose to extend the regression-based model to provide a quality measure of each prediction, and use the shape model to restrict and correct the sampling region. Our approach combines the low computational cost typical of regression-based approaches with the robustness of exhaustive-search approaches. The proposed algorithm was tested on over 7,500 images from five databases. Results showed significant improvement over the current state of the art.", "paperid": 2148739392, "normalizedname_level1": "artificial intelligence"}
{"index": 349, "text": "As the temperature changing in the tubular furnace, the color of the reflect light from the tubular furnace will change also, so a method of temperature measurement using image based on generalized regression neural network (GRNN) is proposed. Firstly, the original color images were segmented and smoothed. Then the calculation of pixel's R, G, B value of each image divided by the pixels number of the furnace mouth and its results are set to input vectors of GRNN network. GRNN is used to approximate the nonlinear relationship between temperature and the color value of each image. Finally, GRNN network is used to forecast the temperature, and compared with the results of BP network. The experimental results show that it is high speed and accuracy to apply GRNN to the method of temperature measurement using image.", "paperid": 1996976413, "normalizedname_level1": "artificial intelligence"}
{"index": 350, "text": "The knowledge of the detailed structure of the scatterer using synthetic aperture radar (SAR) can only be possible if a complete description of the scene was available, which is impossible in practice. Therefore, it is necessary to average data to obtain estimates of useful precision. The statistical description presents a great interest since it allows predicting signal behavior. This paper deals with the statistical behavior of the phase difference between two interferometric signals. The probability density function is examined stating its dependence on the coherence and the mean of the phase difference.", "paperid": 2103293241, "normalizedname_level1": "artificial intelligence"}
{"index": 351, "text": "With the work of magnetoencephalography (MEG) classification in brain-computer interface (BCI), a feature extraction method of frequency band power and statistical characteristics was proposed. On the basis of spectrum analysis for the two subjects' experimental MEG data, frequency band powers of 0.5~6Hz for S1 and 10~25Hz for S2 were extracted as features for the two subjects, together with the statistical characteristics of mean for S1/S2 and standard deviation for S1, finally, the features were classified with linear discriminate analysis function directly and secondly, the results showed that the average classification accuracy was 54.38% which was higher than the achievement of BCI competition winner. Therefore, the frequency band power and statistical characteristics are effective features for MEG signals and the research of this paper gives MEG-based BCIs a beneficial complement.", "paperid": 2282650131, "normalizedname_level1": "artificial intelligence"}
{"index": 352, "text": "Research on language modeling for speech recognition has increasingly focused on the application of neural networks. Two competing concepts have been developed: On the one hand, feedforward neural networks representing an n-gram approach, on the other hand recurrent neural networks that may learn context dependencies spanning more than a fixed number of predecessor words. To the best of our knowledge, no comparison has been carried out between feedforward and state-of-the-art recurrent networks when applied to speech recognition. This paper analyzes this aspect in detail on a well-tuned French speech recognition task. In addition, we propose a simple and efficient method to normalize language model probabilities across different vocabularies, and we show how to speed up training of recurrent neural networks by parallelization.", "paperid": 2058695628, "normalizedname_level1": "artificial intelligence"}
{"index": 353, "text": "Protein threading programs align a probe amino acid sequence onto a library of representative folds of known protein structure to identify a structural homology. A scoring function is usually formulated in terms of the threading energy to evaluate protein sequence-structure fitness. The structure that yields the lowest total energy is considered the leading template of the probe protein. An alternative approach is to predict the probabilities of observing amino acid side-chains in structural environment without considering the energy of contacts. In this paper, a model named TES is proposed on building a new environment-specific protein sequence-structure mapping with artificial neural network. The decoy sets obtained from the web are used to test the proposed TES method on discrimination of native and decoy protein three-dimensional structure. The verified approach shows that the performance of the proposed method is comparable to those of knowledge-based potential energy function.", "paperid": 2162974158, "normalizedname_level1": "artificial intelligence"}
{"index": 354, "text": "Scale-space image processing is a basic technique used for object recognition and low-level feature extraction in computer vision. Many Gaussian filtering techniques have been proposed. Recently, the spectral decomposition method was proposed, which is an infinite version of principal components analysis. Using this method, Gaussian blurred images can be represented as polynomials with a scale parameter and a Gaussian blurred image with an arbitrary scale can be obtained from simple linear combinations of the convolved eigenimages. However, the scale is limited to a small range in this method. In this study, we propose an improvement to the spectral decomposition of a Gaussian kernel by widening the scale using a piecewise polynomial representation. We present an analysis of the continuous spectral decompositions of a Gaussian kernel and their eigensolutions. Experimental results show that the proposed method can generate accurate Gaussian blurred images with an arbitrary scale and a wide scale range.", "paperid": 1999504142, "normalizedname_level1": "artificial intelligence"}
{"index": 355, "text": "The security and protection of biometric template has been the bottleneck of its applications due to permanent appearance of biometrics. We propose “Cancelable PalmCode” based on coding approaches to protect texture features of palmprints. Cancelable PalmCode templates are generated from Gabor filters whose parameters are randomized by the user-specific tokenised pseudo-random number (PRN). The effects of seven randomization schemes are studied and compared. Three of the randomization schemes are proved feasible and practicable for cancelable PalmCode. Cancelable PalmCode is able to be revoked and reissued conveniently and the transform is non-invertible. The selected three schemes avoid performance degradation caused by PRN. The experimental results on PolyU palmprint database show the feasibility and efficiency of cancelable PalmCode and it can be easily generalized to other palmprint codes for biometric security and protection.", "paperid": 2074548247, "normalizedname_level1": "artificial intelligence"}
{"index": 356, "text": "In the interaction between humans and robots it is necessary a mechanism to specify robot's tasks. One of them, is to define the sequence of tasks to do using a language that the robot is able to interpret and perform. The AD architecture, used in this work for the control of the mobile robot, uses task sequencing in its two abstraction levels: automatic and deliberative. A sequence is defined as a specification in which actions that must performed in a concurrent or sequential way are detailed. These actions consist of deliberative or automatic skills, changes in the environment that generate execution changes, and the new actions to perform that allow sequence evolution. This paper presents a sequencer language that allows to describe a sequence of skills in order to make the robot accomplish different tasks such as navigation or to build complex skills. Each sequence is made up of a set of processes which can be executed in a sequential or concurrent way, depending on the actions associated to set of rules belonging to each process", "paperid": 2533884167, "normalizedname_level1": "artificial intelligence"}
{"index": 357, "text": "To cope with the problem of the difficulty of analyzing and detecting the data race problems in multithread programs, a method based on statistic model is presented for the analysis of time sequences in multithread programs. The random variable uncertainty is used to depict the mutual influence in different multithread in time sequences, and the probability distribution for random variable uncertainty is analyzed as the outcome of multithread programs on the condition of data race. It is proved by experiment that the model can reflect the time sequence of the multithread programs, which can be used to instruct the detecting process of multithread programs should and must provide normalized electronic documents in order for readers to search and read papers conveniently.", "paperid": 2160644180, "normalizedname_level1": "artificial intelligence"}
{"index": 358, "text": "In this paper, we present a context based multiple railway object recognition method from mobile laser scanning data. This research makes use of contextual information for classification, which is retrieved from the unlabeled neighborhood as feature vector. The interaction (object context) among object labels is also utilized to enforce local smoothness constraint. The model we use to incorporate contextual information is Conditional Random Field (CRF). By maximizing the object label agreement in the local neighborhood, CRF could improve the classification results obtained from local GMM-EM classifier. The proposed method was validated with mobile laser scanning data using cross validation.", "paperid": 1972676744, "normalizedname_level1": "artificial intelligence"}
{"index": 359, "text": "New color image edge detection is proposed in this paper. Dempster-Shafer theory, also known as the theory of belief function, is applied in the color image edge detection. The reason is that by selecting the mass function, Dempster-Shafer theory can distinguish the edge pixels from the uncertain edge pixels correctly. Firstly, the color image is transformed into R, G and B components; then in these three components, the edge gradient magnitude images are obtained by the Sobel operator respectively; thirdly, the mass functions are selected and the orthogonal sum is calculated; finally, the mass function of the edge probability is regarded as the edge image. From the experiment, the result could be accepted.", "paperid": 2146962542, "normalizedname_level1": "artificial intelligence"}
{"index": 360, "text": "A new robotic system named \"Hyper Finger\" for minimally invasive surgery in deep organs has been developed. The finger size of the latest version is 10 mm and the entire system is much smaller and lighter, and can be set up on a camera tripod. This is one of the smallest master-slave robots in medicine. Each finger has nine degrees of freedom and several unique mechanisms are employed to solve the fundamental issues of conventional wire drive manipulators. The new concept and system were verified successfully by in-vivo remote minimally invasive surgery. Further improvements of the system toward a clinical version are now underway.", "paperid": 2119386891, "normalizedname_level1": "artificial intelligence"}
{"index": 361, "text": "It is well known that many surfaces exhibit reflectance that is not well modelled by Lambert's law. This is the case not only for surfaces that are rough or shiny, but also those that are matte and composed of materials that are particle suspensions. As a result, standard Lambertian shape-from-shading methods cannot be applied directly to the analysis of rough and shiny surfaces. In order to overcome this difficulty, we consider how to reconstruct the Lambertian component for rough and shiny surfaces when the object is illuminated in the viewing direction. To do this we make use of the diffuse reflectance models described by Oren and Nayar, and by Wolff. Our experiments with synthetic and real-world data reveal the effectiveness of the correction method, leading to improved surface normal and height recovery.", "paperid": 1489497384, "normalizedname_level1": "artificial intelligence"}
{"index": 362, "text": "Adversarial pattern classification has been proposed in [1]. In adversarial pattern classification, an adversary wants to change the attributes of an instance to let the classifier make a wrong classification to gain utility. But to disguise an instance an adversary has to pay a cost. The adversary will never do this if the cost is higher than the utility. Adversarial classification systems include examples such as biometric personal authentication, intrusion detection in computer networks and spam filtering. Several methods have been proposed to tackle adversarial pattern classification problem using multiple classifiers[6] and randomization[1] methodology. In this paper, we apply the adversarial pattern classification model to KNN classifier. We assume the existence of an adversary in the KNN classifier and add randomization into the KNN classifier. Experiments to simulate the two-player game between classifier and adversary were perform. Experimental results show that adding randomization could make the adversary harder to attack the classifier.", "paperid": 1998510843, "normalizedname_level1": "artificial intelligence"}
{"index": 363, "text": "This paper proposes a fuzzy min-max neural network classifier with compensatory neurons (FMCNs). FMCN uses hyperbox fuzzy sets to represent the pattern classes. It is a supervised classification technique with new compensatory neuron architecture. The concept of compensatory neuron is inspired from the reflex system of human brain which takes over the control in hazardous conditions. Compensatory neurons (CNs) imitate this behavior by getting activated whenever a test sample falls in the overlapped regions amongst different classes. These neurons are capable to handle the hyperbox overlap and containment more efficiently. Simpson used contraction process based on the principle of minimal disturbance, to solve the problem of hyperbox overlaps. FMCN eliminates use of this process since it is found to be erroneous. FMCN is capable to learn the data online in a single pass through with reduced classification and gradation errors. One of the good features of FMCN is that its performance is less dependent on the initialization of expansion coefficient, i.e., maximum hyperbox size. The paper demonstrates the performance of FMCN by comparing it with fuzzy min-max neural network (FMNN) classifier and general fuzzy min-max neural network (GFMN) classifier, using several examples", "paperid": 2016883203, "normalizedname_level1": "artificial intelligence"}
{"index": 364, "text": "Long-term electroencephalographic (EEG) recordings are important in the presurgical evaluation of refractory partial epilepsy for the delineation of the ictal onset zones. In this paper we introduce a new concept for an automatic, fast and objective localisation of the ictal onset zone in ictal EEG recordings and show with a simulation study that canonical decomposition of ictal scalp EEG allows a robust and accurate localisation of the ictal onset zone.", "paperid": 2059827039, "normalizedname_level1": "artificial intelligence"}
{"index": 365, "text": "The problem of sampling and recovering bandlimited signals in the presence of noise is studied. A new signal acquisition technique, which collects multiple sets of thresholded signal's samples acquired at the Nyquist sampling rate, is proposed. The exact formula for mean integrated squared error of the proposed scheme is established. The obtained results show that the proposed technique gives better reconstruction accuracy than the common method using oversampling and post-filtering. Moreover, in most cases, the proposed scheme reduces the amount of data required for representation and reconstruction of signal. As a result, it provides the reduced data storage.", "paperid": 1588008030, "normalizedname_level1": "artificial intelligence"}
{"index": 366, "text": "The paper presents the implementation of a real-time face tracker to study the integration of support vector machines (SVM) classifiers into a visual real-time tracking architecture. Face tracking has a large number of applications, especially in the fields of surveillance and human-computer interaction, which requires real-time performance. Even though SVM have previously been applied to face detection, their use in real-time applications is a challenge due to the computational cost implied in the SVM's evaluation stage. We address this problem by reducing the number of support vectors with almost no loss in accuracy of the classifier. Experiments showed that classification performed by the original SVM without reducing the number of support vectors took 42% of the total computation time of the face tracker and less than 2% after the reduction was performed.", "paperid": 2142189319, "normalizedname_level1": "artificial intelligence"}
{"index": 367, "text": "Keywords and key-phrases that concisely represent text documents are integral to many knowledge management and text information retrieval systems, as well as digital libraries in general. Not all text documents, however, are annotated with good keywords; and the quality of these keywords is often dependent on a tedious, sometimes manual, extraction and tagging process. To automatically extract high quality keywords without the need for a semantic analysis of the document, it is shown that artificial neural networks (ANN) can be trained to only consider in-document word features such as word frequency, word distribution in document, use of word in special parts of the document, and use of word formatting features (i.e. bold-faced, italicized, large-font size). Results show that purely local features are adequate in determining whether a word in a document is a keyword or not. Classification performance yields a G mean of a least 0.83, and weighted f-measure of 0.96 for both keywords and non-keywords. Precision for keywords alone, however, is not as high. To understand the basis for classifying keywords, C4.5 is used to extract rules from the ANN. The extracted rules from C4.5, in the form of a decision tree, show the relative importance of the different document features that were extracted.", "paperid": 2051588247, "normalizedname_level1": "artificial intelligence"}
{"index": 368, "text": "we would like to clarify the basic factors necessary for a symbiosis between creatures and robots through the interaction experiments between rats and robots. In our previous studies, we developed a robot involving two levers and conditioned the rat to push these levers to obtain food by experimenter’s teaching. In this paper, we try to condition the rats to push the levers on the robot to obtain food by the robot autonomously showing its functions without experimenter’s teaching. On animal psychology, “shaping” is used to condition animals to perform difficult or complex behavior that would rarely occur spontaneously. By importing the concept of “shaping,” we developed an operational pattern generation algorithm for the robot that enabled it to autonomously operate to show the rats its function. We then conducted an interaction experiment for evaluating this algorithm and confirmed the effectiveness of it. Thus, we proposed to import the concept of “shaping” into robot’s behavior generation to enable it to autonomously show creatures its functions.", "paperid": 2168330220, "normalizedname_level1": "artificial intelligence"}
{"index": 369, "text": "Abstract   In a measurement system, new representation methods are necessary to maintain the uncertainty andto supply more powerful ability for reasoning and transformation between numerical system and symbolic system. A grey measurement system is discussed from the point of view of intelligent sensors and incomplete informationprocessing compared with a numerical and symbolized measurement system. The methods of grey representationand information processing are proposed for data collection and reasoning. As a case study, multi-ultrasonic sensorsystems are demonstrated to verify the effectiveness of the proposed methods.", "paperid": 2402380884, "normalizedname_level1": "artificial intelligence"}
{"index": 370, "text": "In this paper, a vehicle detection scheme is suggested using Hough transformation where some moving vehicles are discriminated from significant clutter by using linear trajectory detection in the Hough space. In general, the received range profile for a wideband radar pulse includes many echoes from various obstacles such as guardrail. Observing the profiles with high PRF during a short duration, each echoes trajectory can be estimated employing Hough transform. For example, the trajectory is regarded as linear when the speed of echo is constant and the moving vehicles are spatially discriminated with clutter. The Doppler can also be estimated from the time-range coordinate. As a result, some significant clutters would be eliminated in the Hough space and the vehicles are expected to be detected and tracked with high range and Doppler resolution. The field measurement at 24GHz was conducted for moving vehicles and the usefulness is discussed for various scenarios.", "paperid": 2144628860, "normalizedname_level1": "artificial intelligence"}
{"index": 371, "text": "The blind can only get perceptive and audio information, which bring them huge barriers to action. Mobile devices could provide the blind great help. However, because of limited computation capacity of mobile device, many existing visual augmentation scheme can't be migrated to embedded system. In this paper, we propose a kind of simple blind symbol and a visual aid system (VAS) for the visually impaired people. VAS is based on fast symbol recognition algorithm and RFID technology, which achieves good performance in embedded devices. VAS perceives all blind symbols in current scene and reports location information to blind by text-to-speech technology. We develop a prototype system, which consist of a mobile phone with camera and RFID reader. Test result in this paper shows that VAS gets good real-time effect under embedded platform, even though symbol database contains large number of symbol sample.", "paperid": 2087955781, "normalizedname_level1": "artificial intelligence"}
{"index": 372, "text": "This paper considers the problem of registering two observations of the same object, where the observations differ due to a combined effect of an affine geometric transformation and nonuniform illumination changes. The problem of deriving new representations of the observations that are both invariant to geometric transformations and linear in the illumination model is analyzed. In this framework, we present a novel method for linear estimation of illumination changes in an affine invariant manner, thus, decoupling the original problem into two simpler ones. The computational complexity of the method is low as it requires no more than solving a linear set of equations. The prior step of illumination estimation is shown to improve the accuracy of state-of-the-art registration techniques by a factor of two.", "paperid": 2119722178, "normalizedname_level1": "artificial intelligence"}
{"index": 373, "text": "In this paper, a novel algorithm based on Hough transform is presented for automatic detection hyperbolas in images using a modified artificial bee colony (ABC) algorithm. Hough technique is the most common solution for detecting hyperbolas in images. This method was first introduced by Richard O. Duda for detecting lines in images [1]. The disadvantage of Hough algorithm lies in the fact that it requires large memory size and long computation time. Therefore, optimization method has been used to deal with this problem in this paper. Since optimization is used to find the best solution, the output of the algorithm will only detect one hyperbola in the image if classical optimization methods are used. In this paper, a modified ABC algorithm is presented in order to detect multiple hyperbolas in one time implementation of the algorithm. The classical algorithm is modified to a multimodal optimization algorithm. Therefore, the objective function is based on Hough method. Experiments conducted on the images made by computer (unrealistic data) showed that algorithms could detect multiple hyperbolas in one time implementation of the algorithm. Moreover, the results obtained from the conducted experiments on Noisy images that the algorithm can efficiently meet the criteria and resolve the problem.", "paperid": 2033328929, "normalizedname_level1": "artificial intelligence"}
{"index": 374, "text": "Recent studies on ECG signals proved that they can be employed as biometric traits able to obtain sufficient accuracy in a wide set of applicative scenarios. Most of the systems in the literature, however, are based on templates consisting in vectors of integer or floating point numbers. While any numerical representation is inherently binary, in here we consider as binary templates only those codings in which similarity or distance metrics can be directly applied to the for performing identity comparisons. With respect to templates composed by integer or floating point values, the use of binary templates presents important advantages, such as smaller memory space, and faster and simpler matching functions. Binary templates could therefore be adopted in a wider range of applications with respect to traditional ECG templates, like wearable devices and body area networks. Moreover, binary templates are suitable for most of the biometric template protection methods in the literature. This paper presents a novel approach for computing and processing binary ECG templates (HeartCode). Experimental results proved that the proposed approach is effective and obtains performance comparable to more mature biometric methods for ECG recognition, obtaining Equal Error Rate (EER) of 8.58% on a significantly large database of 8400 samples extracted from Holter acquisitions performed in uncontrolled conditions.", "paperid": 2083274228, "normalizedname_level1": "artificial intelligence"}
{"index": 375, "text": "This paper described an improved AdaBoost. AdaBoost is a face detection framework that is able to process images fast and accurately. Our improved AdaBoost is based on the original framework, but has higher detection rate without extra time cost. There are four main parts of this paper. The first part is an introduction of face detection and challenges to implement it. The second part introduced the original AdaBoost algorithm. The third part presented our improved AdaBoost in detail. The last partcompared and analyzed the improved AdaBoost with the original one, and proved the improvement through tests.", "paperid": 2553599239, "normalizedname_level1": "artificial intelligence"}
{"index": 376, "text": "The advances in satellite technologies, image analysis techniques and computational power make possible processing huge amounts of high resolution images in real time. Acquiring high resolution images has a drawback, as the pixel resolution increases the surveyed area decreases. Multispectral scene is an image stack including numerous spectral bands from the electromagnetic wave spectrum, leading to richer spectral resolution. On the other hand, higher spatial resolution is included in the Panchromatic image. In order to have an image with higher spectral and spatial resolution, the applied merging process is called fusion. In this paper, fourteen different image fusion techniques were implemented. Serial implementations of all these approaches have longer execution time disadvantage compared to parallel approaches. To decrease execution time, the methods were modified with parallel computing approaches. This paper presents a comparison regarding speed performance of all fourteen methods' serial and parallel implementations to increase pixel resolution and keep spatial resolution high by combining spectral and spatial information of high and low resolution images of the same co-registered region. Additionally, spectral quality assessments of methods are presented.", "paperid": 1967700516, "normalizedname_level1": "artificial intelligence"}
{"index": 377, "text": "Because the damage of pipeline is controlled by many factors, such as fault movement, pipe-soil interaction, buried depth, etc., the relationship between pipeline damage and influencing factors is complicated. In order to predict the pipeline damage, predictive model is constructed on the basis of artificial neural network (ANN), in which the damage of pipeline becomes a nonlinear function of influence factors. According to eight groups sample data, MATLAB is applied to analyze the design of predictive model; influences of model structure, concealed layer number, neuron number of concealed layer, and training function, on the predictive results are analyzed. Model parameters and preferences are optimized, and predictive model of pipeline damage is determined based on results of numerical simulation. Finally, optimum model structure is worked out and some advice for modeling and protection of pipeline is proposed.", "paperid": 2119488654, "normalizedname_level1": "artificial intelligence"}
{"index": 378, "text": "In this paper, we present some results obtained with the Shadows system, the new synthetic aperture sonar, by IXSEA. First sections are a slight presentation where we expose the performances and specifications of Shadows. We present the results obtained on the lateral sonar by using first the algorithm of DPCA, second the simulation of DPCA parameters by using the navigation data. Hence we present some results obtained by comparing the two methods. The data were recorded by the Shadows system in La Ciotat Bay during summer 2006.", "paperid": 2053403714, "normalizedname_level1": "artificial intelligence"}
{"index": 379, "text": "This study reports a novel application of the Adaptive Neuro Fuzzy Inference Systems (ANFIS) to a second language listening test, and compares it with path modeling of observed variables. Seven variables were defined and hypothesized to influence the primary dependent variable, test item difficulty. Next, a matrix of these eight variables was developed and subjected to ANFIS and path modeling. ANFIS analysis found stronger effects for several of the seven explanatory variables. Path modeling captured some of the same effects through a mediating variable, test section, which captures aggregate differences across different subsections of the test. In general, neurofuzzy models (NFMs) appear to be a promising tool in language and educational assessment.", "paperid": 2164243765, "normalizedname_level1": "artificial intelligence"}
{"index": 380, "text": "The present work aims to present the method of factor analysis applied to pre-processing of data mining process. The factor analysis is applied to two pattern recognition and classification's databases of power engineering's problems. This pre-processing analyzes the input variables, thus having a better understanding of the importance of each variable as a input for pattern recognition and classification's methods. The pattern recognition and classification's methods were applied to the data with and without the application of factor analysis, thus comparing the real rate of right classification.", "paperid": 2090150141, "normalizedname_level1": "artificial intelligence"}
{"index": 381, "text": "In this paper, the problem of combined target model estimation and position-based visual servoing is addressed. The target object is assumed to be stationary with at least 3 distinguishable feature points. The midpoint triangulation method and a rough estimation method are developed for initial estimation of the target model and relative pose of target in current end-effector frame. A novel decoupled EKF-based online estimation algorithm is proposed to improve the target model and relative pose estimation simultaneously under continuous robot dynamic motion. This new method is robust to large initial estimation errors and provides consistent and accurate target model estimation for optimal pose estimation as required in position-based visual servoing. Experimental results are given to demonstrate the performance of the proposed method.", "paperid": 1534826902, "normalizedname_level1": "artificial intelligence"}
{"index": 382, "text": "We propose a novel statistical analysis method for functional magnetic resonance imaging (fMRI) to overcome the drawbacks of conventional data-driven methods such as the independent component analysis (ICA). Although ICA has been broadly applied to fMRI due to its capacity to separate spatially or temporally independent components, the assumption of independence has been challenged by recent studies showing that ICA does not guarantee independence of simultaneously occurring distinct activity patterns in the brain. Instead, sparsity of the signal has been shown to be more promising. This coincides with biological findings such as sparse coding in V1 simple cells, electrophysiological experiment results in the human medial temporal lobe, etc. The main contribution of this paper is, therefore, a new data driven fMRI analysis that is derived solely based upon the sparsity of the signals. A compressed sensing based data-driven sparse generalized linear model is proposed that enables estimation of spatially adaptive design matrix as well as sparse signal components that represent synchronous, functionally organized and integrated neural hemodynamics. Furthermore, a minimum description length (MDL)-based model order selection rule is shown to be essential in selecting unknown sparsity level for sparse dictionary learning. Using simulation and real fMRI experiments, we show that the proposed method can adapt individual variation better compared to the conventional ICA methods.", "paperid": 2163722029, "normalizedname_level1": "artificial intelligence"}
{"index": 383, "text": "In Fisher Linear Discriminant (FLD), the within-class scatter matrix is always singular. To overcome the above problem and preserve discriminatory information, a new method for palm-dorsa vein feature extraction based on Two-Dimensional FLD (2DFLD) is presented in this paper. We applied PCA, PCA+FLD and 2DFLD to extract the palm-dorsa vein feature subspace. The images to be recognized were projected onto the low-dimensional subspace. A classifier to vein matching based on cosine distance was used. Experimental results suggested that the recognition rate of PCA+FLD is about 4.84% higher than that of PCA. Compared with PCA+FLD, 2DFLD is able to yield recognition rate as high as 98.44%, with accuracy enhanced by 7.51%, while the feature extraction time is only 0.4 s. It was demonstrated that the algorithm is effective and quick.", "paperid": 2084489067, "normalizedname_level1": "artificial intelligence"}
{"index": 384, "text": "We present a novel computer vision driven display system enabling 3D scene visualization using standard 2D display hardware and a pair of calibrated web cameras. Our system requires no worn or other special hardware. Rather than producing a depth illusion through disparity, we deliver a full volumetric 3D visualization — enabling users to interactively explore 3D scenes by varying their viewing position and angle according to the tracked 3D position of their face and eyes. The resulting system operates at real-time speeds with low latency, delivering a compelling natural user interface and immersive experience for 3D viewing.", "paperid": 2093818820, "normalizedname_level1": "artificial intelligence"}
{"index": 385, "text": "Gait classification is a developing research area, particularly with regards to biometrics. It aims to use the distinctive spatial and temporal characteristics of human motion to classify differing activities. As a biometric, this extends to recognising different people by the heterogeneous aspects of their gait. This research aims to use a modified deformable model, the temporal PDM, to distinguish the movements of a walking and running person. The movement of 2D points on the moving form is used to provide input into the model and classify the type of gait present.", "paperid": 2124377899, "normalizedname_level1": "artificial intelligence"}
{"index": 386, "text": "Cable-suspended robots are structurally similar to parallel-actuated robots, but with the fundamental difference that cables can only pull the end-effector, but not push it. These input constraints make feedback control of cable-suspended robots a lot more challenging than their counterpart parallel-actuated robots. In this paper, we present a computationally efficient control design procedure for a cable robot with six cables, which is kinematically determined as long as all cables are in tension. The control strategy is based on dynamic aspects of statically feasible workspace. The basic idea suggested in this paper is to represent the reachable domain in terms of achievable set points under a specified control law that respects the input constraints. This computational framework is recursively used to find a set of reachable domains, using which, we are able to expand the region of feasibility by connecting adjacent domains through common points. The salient feature of the technique is that it is computationally efficient, or online implementable, for the control of a cable robot with positive input constraints. However, due to the complexity of the dynamics of general motion of a cable robot, we consider only translations. No cable interference is considered in this paper. Finally, the effectiveness of the proposed method is illustrated by numerical simulations and laboratory experiments on a six-degree-of-freedom cable-suspended robot.", "paperid": 2119775627, "normalizedname_level1": "artificial intelligence"}
{"index": 387, "text": "Applying advanced video technology to understand activity and intent is becoming increasingly important for intelligent video surveillance. We present a general model of a d-level dynamic Bayesian network to perform complex event recognition. The levels of the network are constrained to enforce state hierarchy while the dth level models the duration of simplest event. Moreover, in this paper we propose to use the deterministic annealing clustering method to automatically discover the states for the observable levels. We used real world data sets to show the effectiveness of our proposed method.", "paperid": 2157182763, "normalizedname_level1": "artificial intelligence"}
{"index": 388, "text": "In this paper,a novel technique for embedding watermarks into a host image on the frequency domain is proposed. Due to good randomness and easy reproducibility of chaos, the watermark is permuted by the chaotic sequence, then a number of reference points are selected in the low frequency bands of the DCT domain based on the chaotic sequences. Moreover, the watermark is self-extractable and the algorithm is simple. Experimental results show that the algorithm proposed in the paper is effective and robust to common signal processing operations and some geometric distortions,such as cropping.Especially,it receives high robustness under JPEG lossy compression and signal enhancement operations,such as sharpening.", "paperid": 2081240409, "normalizedname_level1": "artificial intelligence"}
{"index": 389, "text": "The objective of this paper is to investigate alignment and tracking of facial features with component based active appearance models and optical flow. Face tracking becomes essential issue in the area of human-computer interactions. With accurate tracking and analysis of facial features, computers or robots can response properly to user's facial emotional movements and facial expressions. We apply Active Appearance Model (AAM) and other techniques in real-time on cameras. Good AAM alignment results depend on proper selections of initial positions. However, it does take a lot of time when it applies image pyramid to get accurate results. In this paper, we introduce a brand new method to apply AAM fitting and effectively solve above problems. In our fitting scheme, we apply partial AAM separately on eyes and mouth. Thus, we can make facial features alignment much more efficient, and it becomes able to perform real-time alignment and tracking to real-world video. To make partial AAM more stable, we determine initial positions of facial feature models by multi-level optical flow. By the use of our developed algorithm, it is relative easier to get accurate positions of facial features and extract detail information of faces for further application in real world environments.", "paperid": 2145727501, "normalizedname_level1": "artificial intelligence"}
{"index": 390, "text": "Machine learning techniques have been used extensively to build models for the analysis and retrieval of multimedia data. The explosion of multimedia data on the Web poses a great challenge to such techniques not simply because of the sheer data volume, but also because of the heterogeneity of the data. With data from a wide variety of domains, models trained from one domain do not generalize well to other domains, while at the same time it is prohibitively expensive to build new models for each and every domain due to the high cost for labeling training examples. In this paper, we tackle the heterogeneity challenge in large-scale multimedia data using cross-domain model adaptation for better performance and reduced human cost. Specifically, we investigate the problem of adapting supervised classifiers trained from one or more source domains to a new classifier for a target domain that has only limited labeled examples. The foundation of our work is a general framework for function-level classifier adaptation based on the regularized loss minimization principle, which adapts a classifier by directly modifying its decision function. Under this framework, one can derive concrete adaptation algorithms by plugging in any loss and regularization functions, among which we elaborate on adaptive support vector machines (a-SVM). We further extend this framework for multiclassifier adaptation, namely adapting multiple existing classifiers into a classifier for the target domain, in a way that the contributions of these existing classifiers are automatically determined. We evaluate the proposed approaches in cross-domain semantic concept detection based on TRECVID corpora. The results show that our approaches outperform existing (adaptation and nonadaptation) methods in terms of accuracy and/or efficiency, and adaptation from multiple classifiers offers further benefits.", "paperid": 2069481966, "normalizedname_level1": "artificial intelligence"}
{"index": 391, "text": "A common problem in video surveys in very shallow waters is the presence of strong light fluctuations, due to sun light refraction. Refracted sunlight casts fast moving patterns, which can significantly degrade the quality of the acquired data. Motivated by the growing need to improve the quality of shallow water imagery, we propose a method to remove sunlight patterns in video sequences. The method exploits the fact that video sequences allow several observations of the same area of the sea floor, over time. It is based on computing the image difference between a given reference frame and the temporal median of a registered set of neighboring images. A key observation is that this difference will have two components with separable spectral content. One is related to the illumination field (lower spatial frequencies) and the other to the registration error (higher frequencies). The illumination field, recovered by lowpass filtering, is used to correct the reference image. In addition to removing the sunflickering patterns, an important advantage of the approach is the ability to preserve the sharpness in corrected image, even in the presence of registration inaccuracies. The effectiveness of the method is illustrated in image sets acquired under strong camera motion containing non-rigid benthic structures. The results testify the good performance and generality of the approach.", "paperid": 2166983607, "normalizedname_level1": "artificial intelligence"}
{"index": 392, "text": "We consider the problem of learning a similarity function from a set of positive equivalence constraints, i.e. 'similar' point pairs. We define the similarity in information theoretic terms, as the gain in coding length when shifting from independent encoding of the pair to joint encoding. Under simple Gaussian assumptions, this formulation leads to a non-Mahalanobis similarity function which is efficient and simple to learn. This function can be viewed as a likelihood ratio test, and we show that the optimal similarity-preserving projection of the data is a variant of Fisher Linear Discriminant. We also show that under some naturally occurring sampling conditions of equivalence constraints, this function converges to a known Mahalanobis distance (RCA). The suggested similarity function exhibits superior performance over alternative Mahalanobis distances learnt from the same data. Its superiority is demonstrated in the context of image retrieval and graph based clustering, using a large number of data sets.", "paperid": 2144729466, "normalizedname_level1": "artificial intelligence"}
{"index": 393, "text": "Clustering is an important technique for mining the intrinsic community structures in networks. The density-based network clustering method is able to not only detect communities of arbitrary size ...", "paperid": 3006537632, "normalizedname_level1": "artificial intelligence"}
{"index": 394, "text": "This paper describes a new algorithm for the efficient mass-matrix inversion of serial manipulators. Whereas several well-known O(n) algorithms already exist, our presentation is an alternative and completely different formulation that builds on Fixman’s theorem from the polymer physics literature. The main contributions here are therefore adding a new perspective to the manipulator dynamics literature and providing an alternative to existing algorithms. The essence of this theory is to consider explicitly the band-diagonal structure of the inverted mass matrix of a manipulator with no constraints on link length, offsets or twist angles, and then build in constraints by appropriate partitioning of the inverse of the unconstrained mass matrix. We present the theory of the partitioned mass matrix and inverse of the mass matrix for serial revolute manipulators. The planar N-link manipulator with revolute joints is used to illustrate the procedure. Numerical results verify the O(n) complexity of the algorithm. Exposure of the robotics community to this approach may lead to new ways of thinking about manipulator dynamics and control.", "paperid": 2164646558, "normalizedname_level1": "artificial intelligence"}
{"index": 395, "text": "The challenging task of medical diagnosis based on machine learning techniques requires an inherent bias, i.e., the diagnosis should favor the \"ill\" class over the \"healthy\" class, since misdiagnosing a patient as a healthy person may delay the therapy and aggravate the illness. Therefore,the objective in this task is not to improve the overall accuracy of the classification,but to focus on improving the sensitivity (the accuracy of the \"ill\" class) while maintaining an acceptable specificity (the accuracy of the \"healthy\" class). Some current methods adopt roundabout ways to impose a certain bias toward the important class, i.e., they try to utilize some intermediate factors to influence the classification. However, it remains uncertain whether these methods can improve the classification performance systematically. In this paper, by engaging a novel learning tool, the biased minimax probability machine(BMPM), we deal with the issue in a more elegant way and directly achieve the objective of appropriate medical diagnosis. More specifically, the BMPM directly controls the worst case accuracies to incorporate a bias toward the \"ill\" class. Moreover, in a distribution-free way, the BMPM derives the decision rule in such a way as to maximize the worst case sensitivity while maintaining an acceptable worst case specificity. By directly controlling the accuracies,the BMPM provides a more rigorous way to handle medical diagnosis; by deriving a distribution-free decision rule, the BMPM distinguishes itself from a large family of classifiers, namely, the generative classifiers, where an assumption on the data distribution is necessary. We evaluate the performance of the model and compare it with three traditional classifiers: the k-nearest neighbor, the naive Bayesian, and the C4.5. The test results on two medical datasets, the breast-cancer dataset and the heart disease dataset, show that the BMPM outperforms the other three models", "paperid": 2152813753, "normalizedname_level1": "artificial intelligence"}
{"index": 396, "text": "Visual analysis of human motion in video sequences has attached more and more attention to computer visions in recent years. In order to indicate pedestrian movement in Intelligent Monitoring System, a Euclidean distance based on centroid method is proposed. And then according to the movement of body a set of standard images contour are made. All matrixes which represent human silhouette are normalized using affine transformation, which cuts computational cost. The difference between two matrixes is regard as fuzzy function. Fuzzy neural network is proposed to infer abnormal behavior of the walker. First of all, a four layer fuzzy neural network is presented. And then Fuzzy C-means clustering algorithm is used to calculate the number of hidden layer nodes. Finally the degree of the anomaly is resulted from the fuzzy membership of the two matrixes difference. Fuzzy discriminant can detect irregularities and implements initiative analysis to body behavior. The results show that the new algorithm has better performance.", "paperid": 1971907514, "normalizedname_level1": "artificial intelligence"}
{"index": 397, "text": "This paper presents a texture descriptor based on the fine detail coefficients at three resolution levels of a traslation invariant undecimated wavelet transform. First, we consider vertical and horizontal wavelet detail coefficients at the same position as the components of a bivariate random vector, and the magnitude and angle of these vectors are computed. The magnitudes are modeled by a Generalized Gamma distribution. Their parameters, together with the circular histograms of angles, are used to characterize each texture image of the database. The Kullback-Leibler divergence is used as the similarity measurement. Retrieval experiments, in which we compare two wavelet transforms, are carried out on the Brodatz texture collection. Results reveal the good performance of this wavelet-based texture descriptor obtained via the Generalized Gamma distribution.", "paperid": 2045740656, "normalizedname_level1": "artificial intelligence"}
{"index": 398, "text": "The field of environmental sciences is abundant with various interfaces and it is the right place for application of new fundamental approaches leading towards better understanding of environmental phenomena. Generalizing the notion of physical and biophysical interactions between different media (e.g. between natural or artificially built surfaces and atmosphere or between intercellular space and cell's surrounding environment), we obtain the potentially fruitful concept of environmental interface as a higher-dimensional complex system. In this paper, we report the results of numerical investigation on the coupled system of two logistic maps, representing (i) energy exchange on environmental interfaces, with different logistic parameters such that one map lies in a period one stable attractor or a bifurcation point and the other in chaotic region when decoupled and (ii) substance exchange between two cells in their surrounding environment having the form of coupled logistic equation and power map expressing the nonlinearity of the processes.", "paperid": 2139110013, "normalizedname_level1": "artificial intelligence"}
{"index": 399, "text": "Estimating the output luminance for the whole range of gray-levels is quite needed for high-quality image enhancement algorithms. In this paper, a new modeling method of light emission is proposed to estimate a more exact output luminance for the whole gray-levels by consideration of phosphor saturation and its recovery effect. Based on high accuracy of the estimated output luminance, the proposed modeling is helpful in removing the inversion problem of grayscale and improving the grayscale linearity of PDP-TV considerably.", "paperid": 2131002339, "normalizedname_level1": "artificial intelligence"}
{"index": 400, "text": "This paper presents an efficient method for building a topological map for robots in urban environments based on a geographic information system (GIS) such as satellite maps. In urban space, mobile robots need a special map, such as a topological map, to generate a path toward their goal. Unlike a car map, a topological map for mobile robot navigation should include semantic data, e.g., the width and type of road. This paper divides the GIS-based topological map building process into two steps. The first step is defining the topological map model to fit the mobile robot navigation in an urban environment. The second step is generating a topological map from existing GIS data to reduce the cost and improve the accuracy in building the map.", "paperid": 1964006841, "normalizedname_level1": "artificial intelligence"}
{"index": 401, "text": "The electrocardiogram (ECG) is a powerful non-invasive tool which allows for diagnosis of a wide range of heart conditions. Today, portable ECG recording devices, equipped with a transmitter, can be used to provide health related information and to trigger alarms in case of life threatening situations. However, these devices suffer from motion induced artifacts. While much research has been conducted to remove time invariant noise, the removal of motion induced artifacts remains an unsolved problem. We therefore introduce a new method which removes these artifacts. This is done by obtaining an estimate of the artifacts using the stationary wavelet transformation. An automatic multi-resolution thresholding scheme which uses a robustified QRS detection is proposed. Real data examples as well as simulations are given which illustrate the performance of the method.", "paperid": 1546062102, "normalizedname_level1": "artificial intelligence"}
{"index": 402, "text": "Even for small programs, the input space is huge - often unbounded. Partition testing divides the input space into disjoint equivalence classes and combinatorial testing selects a subset of all possible input class combinations, according to criteria such as pairwise coverage. The down side of this approach is that the partitioning of the input space into equivalence classes (input classification) is done manually. It is expensive and requires deep domain and implementation understanding. In this paper, we propose a novel approach to classify test inputs and their dependencies automatically. Firstly, random (or automatically generated) input vectors are sent to the system under test (SUT). For each input vector, an observed \"hit vector\" is produced by monitoring the execution of the SUT. Secondly, hit vectors are grouped into clusters using machine learning. Each cluster contains similar hit vectors, i.e., similar behaviors, and from them we obtain corresponding clusters of input vectors. Input classes are then extracted for each input parameter straightforwardly. Our experiments with a number of subjects show good results as the automatically generated classifications are the same or very close to the expected ones.", "paperid": 2037682493, "normalizedname_level1": "artificial intelligence"}
{"index": 403, "text": "As a new biometric for person authentication, hand-dorsa vein has attracted increasing attention in recent years. This paper proposes a novel approach for hand-dorsa vein recognition, which makes use of multi-level keypoint detection and SIFT feature based local matching. In order to overcome the difficulty in finding local features on NIR images of hand dorsa, a multi-level keypoint detection approach, composed by Harris-Laplace and Hessian-Laplace detectors, is designed to localize enough keypoints so that more discriminative information can be highlighted. Then SIFT based local matching efficiently associates these keypoints between hand dorsa of the same individual. The experimental results achieved on the NCUT database clearly indicate the effectiveness of the proposed method for hand-dorsa vein recognition.", "paperid": 1504428828, "normalizedname_level1": "artificial intelligence"}
{"index": 404, "text": "We propose a map building system for generation of 3D city models using a vehicle equipped with LIDAR (Light Detection And Ranging) technology, which is capable of obtaining more than one million points per second. This approach is based in two methods: plane segmentation and local map construction. The First method introduces a fast method to extract the ground plane. The rest of the points are used to extract planes using a general plane extraction. For the general plane extraction process, is used a method that choose a random point and its neighbor in order to compute a plane; this plane is evaluated with respect to the inliers number, and the best plane is the one containing the biggest number of points. Finally, using the planes, we construct an environment map.", "paperid": 2066712380, "normalizedname_level1": "artificial intelligence"}
{"index": 405, "text": "This paper addresses the problem of improving accuracy in the machine-learning task of classification from microarray data. One of the known issues specifically related to microarray data is the large number of inputs (genes) versus the small number of available samples (conditions). A promising direction of research to decrease the generalization error of classification algorithms is to perform gene selection so as to identify those genes which are potentially most relevant for the classification. Classical feature selection methods are based on direct statistical methods. We present a reduction algorithm based on the notion of prototypegene. Each prototype represents a set of similar gene according to a given clustering method. We present experimental evidence of the usefulness of combining prototype-based feature selection with statistical gene selection methods for the task of classifying adenocarcinoma from gene expressions.", "paperid": 1964599014, "normalizedname_level1": "artificial intelligence"}
{"index": 406, "text": "Most robots are designed to operate in environments that are either highly constrained (as is the case in an assembly line) or extremely hazardous (such as the surface of Mars). Machine learning has been an effective tool in both of these environments by augmenting the flexibility and reliability of robotic systems, but this is often a very difficult problem because the complexity of learning in the real world introduces very high dimensional state spaces and applies severe penalties for mistakes. Human children are raised in environments that are just as complex (or even more so) than those typically studied in robot learning scenarios. However, the presence of parents and other caregivers radically changes the type of learning that is possible. Consciously and unconsciously, adults tailor their action and the environment to the child. They draw attention to important aspects of a task, help in identifying the cause of errors and generally tailor the task to the child's capabilities. Our research group builds robots that learn in the same type of supportive environment that human children have and develop skills incrementally through their interactions. Our robots interact socially with human adults using the same natural conventions that a human child would use. Our work sits at the intersection of the fields of social robotics (Fong et al., 2003; Breazeal and Scawellan, 2002) and autonomous mental development (Weng et al., 2000). Together, these two fields offer the vision of a machine that can learn incrementally, directly from humans, in the same ways that humans learn from each other. In this article, we introduce some of the challenges, goals, and applications of this research", "paperid": 1561221222, "normalizedname_level1": "artificial intelligence"}
{"index": 407, "text": "The quality of corpus-based text-to-speech systems depends on the accuracy of the unit selection process, which in turn relies on the cost function definition. This function should map the user perceptual preference when selecting synthesis units, which is a very difficult task. This paper continues our previous work on fusing the human judgements with the cost function by means of interactive weight tuning. The application of active interactive genetics algorithms mitigates user fatigue by improving user consistency. As a result, the obtained weights generate more natural synthetic speech when compared to previous objective and subjective proposals.", "paperid": 2098490645, "normalizedname_level1": "artificial intelligence"}
{"index": 408, "text": "Autonomous driving in off-road environments requires an exceptionally capable sensor system, particularly given that the unstructured environment does not provide many of the cues available in on-road environments. This paper presents a complex vision system, which is able to provide the two basic sensorial capabilities needed by autonomous vehicle navigation in extreme environments: obstacle detection and path detection. A variable-width-baseline (up to 1.5 m) single-frame stereo system is used for pitch estimation and obstacle detection, whereas a decision-network approach is used to detect the drivable path by a monocular vision system. The system has been field tested on the TerraMax vehicle, which is one of the only five vehicles to complete the 2005 Defense Advanced Research Projects Agency (DARPA) Grand Challenge course.", "paperid": 2155520527, "normalizedname_level1": "artificial intelligence"}
{"index": 409, "text": "In the cognitive radio networks, the cognitive user must scan the multi-channel to acquire an idle channel before its cognitive transmission. If the sensing capability is limited, the user must sense the multi-channel one by one. So the sensing order is very crucial to the realization of maximum throughput. In this paper, we aim to find the optimal wideband sensing order in polynomial time which is much faster than brute-force search and dynamic programming approach. First, we present two greed solutions, with the trait: fast but not optimal. Also, we analyze and improve the potential function in the optimality principle of sequencing. Then, we propose the decision-making tree method with the proper branching rule and the lopping rule, to search the optimal wideband spectrum sensing order. In the end, through the performance analysis, we validate the low computational complexity and the optimality.", "paperid": 1990463445, "normalizedname_level1": "artificial intelligence"}
